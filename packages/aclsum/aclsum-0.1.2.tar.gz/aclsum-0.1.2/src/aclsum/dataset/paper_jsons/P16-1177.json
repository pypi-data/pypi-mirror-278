{
    "paper_id": "P16-1177",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:16:25.239161Z"
    },
    "title": "Learning Text Pair Similarity with Context-sensitive Autoencoders",
    "authors": [
        {
            "first": "Hadi",
            "middle": [],
            "last": "Amiri",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Maryland",
                "location": {
                    "settlement": "College Park",
                    "region": "MD"
                }
            },
            "email": ""
        },
        {
            "first": "Philip",
            "middle": [],
            "last": "Resnik",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Maryland",
                "location": {
                    "settlement": "College Park",
                    "region": "MD"
                }
            },
            "email": "resnik@umd.edu"
        },
        {
            "first": "Jordan",
            "middle": [],
            "last": "Boyd-Graber",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Colorado",
                "location": {
                    "settlement": "Boulder",
                    "region": "CO"
                }
            },
            "email": "jordan.boyd.graber@colorado.edu"
        },
        {
            "first": "Hal",
            "middle": [],
            "last": "Daum\u00e9",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Maryland",
                "location": {
                    "settlement": "College Park",
                    "region": "MD"
                }
            },
            "email": ""
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "We present a pairwise context-sensitive Autoencoder for computing text pair similarity. Our model encodes input text into context-sensitive representations and uses them to compute similarity between text pairs. Our model outperforms the state-of-the-art models in two semantic retrieval tasks and a contextual word similarity task. For retrieval, our unsupervised approach that merely ranks inputs with respect to the cosine similarity between their hidden representations shows comparable performance with the state-of-the-art supervised models and in some cases outperforms them.",
    "pdf_parse": {
        "paper_id": "P16-1177",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "We present a pairwise context-sensitive Autoencoder for computing text pair similarity. Our model encodes input text into context-sensitive representations and uses them to compute similarity between text pairs. Our model outperforms the state-of-the-art models in two semantic retrieval tasks and a contextual word similarity task. For retrieval, our unsupervised approach that merely ranks inputs with respect to the cosine similarity between their hidden representations shows comparable performance with the state-of-the-art supervised models and in some cases outperforms them.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Representation learning algorithms learn representations that reveal intrinsic low-dimensional structure in data (Bengio et al., 2013) . Such representations can be used to induce similarity between textual contents by computing similarity between their respective vectors (Huang et al., 2012; Silberer and Lapata, 2014) .",
                "cite_spans": [
                    {
                        "start": 113,
                        "end": 134,
                        "text": "(Bengio et al., 2013)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 273,
                        "end": 293,
                        "text": "(Huang et al., 2012;",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 294,
                        "end": 320,
                        "text": "Silberer and Lapata, 2014)",
                        "ref_id": "BIBREF28"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Recent research has made substantial progress on semantic similarity using neural networks (Rothe and Sch\u00fctze, 2015; Dos Santos et al., 2015; Severyn and Moschitti, 2015) . In this work, we focus our attention on deep autoencoders and extend these models to integrate sentential or document context information about their inputs. We represent context information as low dimensional vectors that will be injected to deep autoencoders. To the best of our knowledge, this is the first work that enables integrating context into autoencoders.",
                "cite_spans": [
                    {
                        "start": 91,
                        "end": 116,
                        "text": "(Rothe and Sch\u00fctze, 2015;",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 117,
                        "end": 141,
                        "text": "Dos Santos et al., 2015;",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 142,
                        "end": 170,
                        "text": "Severyn and Moschitti, 2015)",
                        "ref_id": "BIBREF27"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In representation learning, context may appear in various forms. For example, the context of a current sentence in a document could be either its neighboring sentences (Lin et al., 2015; Wang and Cho, 2015) , topics associated with the sentence (Mikolov and Zweig, 2012; Le and Mikolov, 2014) , the document that contains the sentence (Huang et al., 2012) , as well as their combinations (Ji et al., 2016) . It is important to integrate context into neural networks because these models are often trained with only local information about their individual inputs. For example, recurrent and recursive neural networks only use local information about previously seen words in a sentence to predict the next word or composition. 1On the other hand, context information (such as topical information) often capture global information that can guide neural networks to generate more accurate representations.",
                "cite_spans": [
                    {
                        "start": 168,
                        "end": 186,
                        "text": "(Lin et al., 2015;",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 187,
                        "end": 206,
                        "text": "Wang and Cho, 2015)",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 245,
                        "end": 270,
                        "text": "(Mikolov and Zweig, 2012;",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 271,
                        "end": 292,
                        "text": "Le and Mikolov, 2014)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 335,
                        "end": 355,
                        "text": "(Huang et al., 2012)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 388,
                        "end": 405,
                        "text": "(Ji et al., 2016)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We investigate the utility of context information in three semantic similarity tasks: contextual word sense similarity in which we aim to predict semantic similarity between given word pairs in their sentential context (Huang et al., 2012; Rothe and Sch\u00fctze, 2015) , question ranking in which we aim to retrieve semantically equivalent questions with respect to a given test question (Dos Santos et al., 2015) , and answer ranking in which we aim to rank single-sentence answers with respect to a given question (Severyn and Moschitti, 2015) .",
                "cite_spans": [
                    {
                        "start": 219,
                        "end": 239,
                        "text": "(Huang et al., 2012;",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 240,
                        "end": 264,
                        "text": "Rothe and Sch\u00fctze, 2015)",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 384,
                        "end": 409,
                        "text": "(Dos Santos et al., 2015)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 512,
                        "end": 541,
                        "text": "(Severyn and Moschitti, 2015)",
                        "ref_id": "BIBREF27"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The contributions of this paper are as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "(1) integrating context information into deep autoencoders and (2) showing that such integration improves the representation performance of deep autoencoders across several different semantic similarity tasks.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Our model outperforms the state-of-the-art su-pervised baselines in three semantic similarity tasks. Furthermore, the unsupervised version of our autoencoder show comparable performance with the supervised baseline models and in some cases outperforms them.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "2 Context-sensitive Autoencoders",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We first provide a brief description of basic autoencoders and extend them to context-sensitive ones in the next Section. Autoencoders are trained using a local unsupervised criterion (Vincent et al., 2010; Hinton and Salakhutdinov, 2006; Vincent et al., 2008) . Specifically, the basic autoencoder in Figure 1 (a) locally optimizes the hidden representation h of its input x such that h can be used to accurately reconstruct x,",
                "cite_spans": [
                    {
                        "start": 184,
                        "end": 206,
                        "text": "(Vincent et al., 2010;",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 207,
                        "end": 238,
                        "text": "Hinton and Salakhutdinov, 2006;",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 239,
                        "end": 260,
                        "text": "Vincent et al., 2008)",
                        "ref_id": "BIBREF31"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 309,
                        "end": 310,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Basic Autoencoders",
                "sec_num": "2.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "h = g(Wx + b h ) (1) x = g(W h + b x), (",
                        "eq_num": "2"
                    }
                ],
                "section": "Basic Autoencoders",
                "sec_num": "2.1"
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Basic Autoencoders",
                "sec_num": "2.1"
            },
            {
                "text": "where x is the reconstruction of x, the learning parameters W \u2208 R Training a single-layer autoencoder corresponds to optimizing the learning parameters to minimize the overall loss between inputs and their reconstructions. For real-valued x, squared loss is often used, l(x) = ||x -x|| 2 , (Vincent et al., 2010) :",
                "cite_spans": [
                    {
                        "start": 290,
                        "end": 312,
                        "text": "(Vincent et al., 2010)",
                        "ref_id": "BIBREF32"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Basic Autoencoders",
                "sec_num": "2.1"
            },
            {
                "text": "min \u0398 n i=1 l(x (i) ) \u0398 = {W, W , b h , b x}.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Basic Autoencoders",
                "sec_num": "2.1"
            },
            {
                "text": "(3) This can be achieved using mini-batch stochastic gradient descent (Zeiler, 2012).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Basic Autoencoders",
                "sec_num": "2.1"
            },
            {
                "text": "We extend the above basic autoencoder to integrate context information about inputs. We assume that-for each training example x \u2208 R dwe have a context vector c x \u2208 R k that contains contextual information about the input. 3 The na-2 If the squared loss is used for optimization, as in Equation (3), nonlinearity is often not used in Equation (2) (Vincent et al., 2010) . 3 We slightly abuse the notation throughout this paper by referring to cx or hi as vectors, not elements of vectors. ture of this context vector depends on the input and target task. For example, neighboring words can be considered as the context of a target word in contextual word similarity task.",
                "cite_spans": [
                    {
                        "start": 346,
                        "end": 368,
                        "text": "(Vincent et al., 2010)",
                        "ref_id": "BIBREF32"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Integrating Context into Autoencoders",
                "sec_num": "2.2"
            },
            {
                "text": "We first learn the hidden representation h c \u2208 R d for the given context vector c x . For this, we use the same process as discussed above for the basic autoencoder where we use c x as the input in Equations ( 1) and ( 2) to obtain h c . We then use h c to develop our context-sensitive autoencoder as depicted in Figure 1 (b) . This autoencoder maps its inputs x and h c into a context-sensitive representation h as follows:",
                "cite_spans": [
                    {
                        "start": 323,
                        "end": 326,
                        "text": "(b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 321,
                        "end": 322,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Integrating Context into Autoencoders",
                "sec_num": "2.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "h = g(Wx + Vh c + b h ) (4) x = g(W h + b x) (5) \u0125c = g(V h + b \u0125c ). (",
                        "eq_num": "6"
                    }
                ],
                "section": "Integrating Context into Autoencoders",
                "sec_num": "2.2"
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Integrating Context into Autoencoders",
                "sec_num": "2.2"
            },
            {
                "text": "Our intuition is that if h leads to a good reconstruction of its inputs, it has retained information available in the input. Therefore, it is a contextsensitive representation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Integrating Context into Autoencoders",
                "sec_num": "2.2"
            },
            {
                "text": "The loss function must then compute the loss between the input pair (x, h c ) and its reconstruction (x, \u0125c ). For optimization, we can still use squared loss with a different set of parameters to minimize the overall loss on the training examples: where \u03bb \u2208 [0, 1] is a weight parameter that controls the effect of context information in the reconstruction process.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Integrating Context into Autoencoders",
                "sec_num": "2.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "l(x, h c ) = ||x -x|| 2 + \u03bb||h c -\u0125c || 2 min \u0398 n i=1 l(x (i) , h (i) c ) \u0398 = {W, W , V, V , b h , b x, b \u0125c },",
                        "eq_num": "(7)"
                    }
                ],
                "section": "Integrating Context into Autoencoders",
                "sec_num": "2.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\ud835\udc84 \ud835\udc65 \ud835\udc7d 0 h 1 h c \ud835\udc99 \ud835\udc7e 1 \ud835\udc7d 1 h i-1 h i \ud835\udc7e \ud835\udc56 h c \ud835\udc7d \ud835\udc56 DAE-i DAE-1 h c DAE-0",
                        "eq_num": "("
                    }
                ],
                "section": "Integrating Context into Autoencoders",
                "sec_num": "2.2"
            },
            {
                "text": "Denoising autoencoders (DAEs) reconstruct an input from a corrupted version of it for more effective learning (Vincent et al., 2010) . The corrupted input is then mapped to a hidden representation from which we obtain the reconstruction. However, the reconstruction loss is still computed with respect to the uncorrupted version of the input as before. Denoising autoencoders effectively learn representations by reversing the effect of the corruption process. We use masking noise to corrupt the inputs where a fraction \u03b7 of input units are randomly selected and set to zero (Vincent et al., 2008) .",
                "cite_spans": [
                    {
                        "start": 110,
                        "end": 132,
                        "text": "(Vincent et al., 2010)",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 576,
                        "end": 598,
                        "text": "(Vincent et al., 2008)",
                        "ref_id": "BIBREF31"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Denoising",
                "sec_num": "2.2.1"
            },
            {
                "text": "Autoencoders can be stacked to create deep networks. A deep autoencoder is composed of multiple hidden layers that are stacked together. The initial weights in such networks need to be properly initialized through a greedy layer-wise training approach. Random initialization does not work because deep autoencoders converge to poor local minima with large initial weights and result in tiny gradients in the early layers with small initial weights (Hinton and Salakhutdinov, 2006) .",
                "cite_spans": [
                    {
                        "start": 448,
                        "end": 480,
                        "text": "(Hinton and Salakhutdinov, 2006)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Deep Context-Sensitive Autoencoders",
                "sec_num": "2.2.2"
            },
            {
                "text": "Our deep context-sensitive autoencoder is composed of a stacked set of DAEs. As discussed above, we first need to properly initialize the learn-ing parameters (weights and biases) associated to each DAE. As shown in Figure 2 (a), we first train DAE-0, which initializes parameters associated to the context layer. The training procedure is exactly the same as training a basic autoencoder (Section 2.1 and Figure 1(a) ). 4 We then treat h c and x as \"inputs\" for DAE-1 and use the same approach as in training a context-sensitive autoencoder to initialize the parameters of DAE-1 (Section 2.2 and Figure 1(b) ). Similarly, the i th DAE is built on the output of the (i -1) th DAE and so on until the desired number of layers (e.g. n layers) are initialized. For denoising, the corruption is only applied on \"inputs\" of individual autoencoders. For example, when we are training DAE-i, h i-1 and h c are first obtained from the original inputs of the network (x and c x ) through a single forward pass and then their corrupted versions are computed to train DAE-i.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 223,
                        "end": 224,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    },
                    {
                        "start": 413,
                        "end": 417,
                        "text": "1(a)",
                        "ref_id": "FIGREF0"
                    },
                    {
                        "start": 604,
                        "end": 608,
                        "text": "1(b)",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Deep Context-Sensitive Autoencoders",
                "sec_num": "2.2.2"
            },
            {
                "text": "Figure 2 (b) shows that the n properly initialized DAEs can be stacked to form a deep contextsensitive autoencoder. We unroll this network to fully optimize its weights through gradient descent and backpropagation (Vincent et al., 2010; Hinton and Salakhutdinov, 2006) .",
                "cite_spans": [
                    {
                        "start": 214,
                        "end": 236,
                        "text": "(Vincent et al., 2010;",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 237,
                        "end": 268,
                        "text": "Hinton and Salakhutdinov, 2006)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Deep Context-Sensitive Autoencoders",
                "sec_num": "2.2.2"
            },
            {
                "text": "We optimize the learning parameters of our initialized context-sensitive deep autoencoder by unfolding its n layers and making a 2n -1 layer net-work whose lower layers form an \"encoder\" network and whose upper layers form a \"decoder\" network (Figure 2 (c)). A global fine-tuning stage backpropagates through the entire network to finetune the weights for optimal reconstruction. In this stage, we update the network parameters again by training the network to minimize the loss between original inputs and their actual reconstruction. We backpropagate the error derivatives first through the decoder network and then through the encoder network. Each decoder layer tries to recover the input of its corresponding encoder layer. As such, the weights are initially symmetric and the decoder weights do need to be learned.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 251,
                        "end": 252,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Unrolling and Fine-tuning",
                "sec_num": "2.2.3"
            },
            {
                "text": "After the training is complete, the hidden layer h n contains a context-sensitive representation of the inputs x and c x .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Unrolling and Fine-tuning",
                "sec_num": "2.2.3"
            },
            {
                "text": "Context is task and data dependent. For example, a sentence or document that contains a target word forms the word's context.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Context Information",
                "sec_num": "2.3"
            },
            {
                "text": "When context information is not readily available, we use topic models to determine such context for individual inputs (Blei et al., 2003; Stevens et al., 2012) . In particular, we use Non-Negative Matrix Factorization (NMF) (Lin, 2007) : Given a training set with n instances, i.e., X \u2208 R v\u00d7n , where v is the size of a global vocabulary and the scalar k is the number of topics in the dataset, we learn the topic matrix D \u2208 R v\u00d7k and context matrix C \u2208 R k\u00d7n using the following sparse coding algorithm:",
                "cite_spans": [
                    {
                        "start": 119,
                        "end": 138,
                        "text": "(Blei et al., 2003;",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 139,
                        "end": 160,
                        "text": "Stevens et al., 2012)",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 225,
                        "end": 236,
                        "text": "(Lin, 2007)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Context Information",
                "sec_num": "2.3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "min D,C X -DC 2 F + \u00b5 C 1 ,",
                        "eq_num": "(8)"
                    }
                ],
                "section": "Context Information",
                "sec_num": "2.3"
            },
            {
                "text": "s.t. D \u2265 0, C \u2265 0,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Context Information",
                "sec_num": "2.3"
            },
            {
                "text": "where each column in C is a sparse representation of an input over all topics and will be used as global context information in our model. We obtain context vectors for test instances by transforming them according to the fitted NMF model on training data. We also note that advanced topic modeling approaches, such as syntactic topic models (Boyd-Graber and Blei, 2009) , can be more effective here as they generate linguistically rich context information.",
                "cite_spans": [
                    {
                        "start": 359,
                        "end": 370,
                        "text": "Blei, 2009)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Context Information",
                "sec_num": "2.3"
            },
            {
                "text": "We present unsupervised and supervised approaches for predicting semantic similarity scores for input texts (e.g., a pair of words) each with its corresponding context information. These scores will then be used to rank \"documents\" against \"queries\" (in retrieval tasks) or evaluate how predictions of a model correlate with human judgments (in contextual word sense similarity task).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Text Pair Similarity",
                "sec_num": "3"
            },
            {
                "text": "In unsupervised settings, given a pair of input texts with their corresponding context vectors, (x 1 ,c x 1 ) and (x 2 ,c x 2 ), we determine their semantic similarity score by computing the cosine similarity between their hidden representations h 1 n and h 2 n respectively.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Text Pair Similarity",
                "sec_num": "3"
            },
            {
                "text": "In supervised settings, we use a copy of our context-sensitive autoencoder to make a pairwise architecture as depicted in Figure 3 . Given (x 1 ,c x 1 ), (x 2 ,c x 2 ), and their binary relevance score, we use h 1 n and h 2 n as well as additional features (see below) to train our pairwise network (i.e. further fine-tune the weights) to predict a similarity score for the input pair as follows:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 129,
                        "end": 130,
                        "text": "3",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Text Pair Similarity",
                "sec_num": "3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "rel(x 1 , x 2 ) = sof tmax(M 0 a+M 1 h 1 n +M 2 h 2 n +b)",
                        "eq_num": "(9"
                    }
                ],
                "section": "Text Pair Similarity",
                "sec_num": "3"
            },
            {
                "text": ") where a carries additional features, Ms are weight matrices, and b is the bias. We use the difference and similarity between the context-sensitive representations of inputs, h 1 n and h 2 n , as additional features:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Text Pair Similarity",
                "sec_num": "3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "h sub = |h 1 n -h 2 n | h dot = h 1 n h 2 n ,",
                        "eq_num": "(10)"
                    }
                ],
                "section": "Text Pair Similarity",
                "sec_num": "3"
            },
            {
                "text": "where h sub and h dot capture the element-wise difference and similarity (in terms of the sign of elements in each dimension) between h 1 n and h 2 n , respectively. We expect elements in h sub to be small for semantically similar and relevant inputs and large otherwise. Similarly, we expect elements in h dot to be positive for relevant inputs and negative otherwise.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Text Pair Similarity",
                "sec_num": "3"
            },
            {
                "text": "We can use any task-specific feature as additional features. This includes features from the minimal edit sequences between parse trees of the input pairs (Heilman and Smith, 2010; Yao et al., 2013) , lexical semantic features extracted from resources such as WordNet (Yih et al., 2013) , or other features such as word overlap features (Severyn and Moschitti, 2015; Severyn and Moschitti, 2013) . We can also use additional features (Equation 10), computed for BOW representations of the inputs x 1 and x 2 . Such additional features improve the performance of our and baseline models.",
                "cite_spans": [
                    {
                        "start": 155,
                        "end": 180,
                        "text": "(Heilman and Smith, 2010;",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 181,
                        "end": 198,
                        "text": "Yao et al., 2013)",
                        "ref_id": "BIBREF35"
                    },
                    {
                        "start": 268,
                        "end": 286,
                        "text": "(Yih et al., 2013)",
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 337,
                        "end": 366,
                        "text": "(Severyn and Moschitti, 2015;",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 367,
                        "end": 395,
                        "text": "Severyn and Moschitti, 2013)",
                        "ref_id": "BIBREF26"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Text Pair Similarity",
                "sec_num": "3"
            },
            {
                "text": "In this Section, we use t-test for significant testing and asterisk mark (*) to indicate significance at \u03b1 = 0.05.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "4"
            },
            {
                "text": "We use three datasets: \"SCWS\" a word similarity dataset with ground-truth labels on similarity of pairs of target words in sentential context from Huang et al. (2012) ; \"qAns\" a TREC QA dataset with ground-truth labels for semantically relevant questions and (single-sentence) answers from Wang et al. (2007) ; and \"qSim\" a community QA dataset crawled from Stack Exchange with ground-truth labels for semantically equivalent questions from Dos Santos et al. (2015) . Table 1 shows statistics of these datasets. To enable direct comparison with previous work, we use the same training, development, and test data provided by Dos Santos et al. (2015) and Wang et al. (2007) for qSim and qAns respectively and the entire data of SCWS (in unsupervised setting).",
                "cite_spans": [
                    {
                        "start": 147,
                        "end": 166,
                        "text": "Huang et al. (2012)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 290,
                        "end": 308,
                        "text": "Wang et al. (2007)",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 459,
                        "end": 465,
                        "text": "(2015)",
                        "ref_id": null
                    },
                    {
                        "start": 629,
                        "end": 649,
                        "text": "Santos et al. (2015)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 654,
                        "end": 672,
                        "text": "Wang et al. (2007)",
                        "ref_id": "BIBREF34"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data and Context Information",
                "sec_num": "4.1"
            },
            {
                "text": "We consider local and global context for target words in SCWS. The local context of a target word is its ten neighboring words (five before and five after) (Huang et al., 2012) , and its global context is a short paragraph that contains the target word (surrounding sentences). We compute average word embeddings to create context vectors for target words.",
                "cite_spans": [
                    {
                        "start": 156,
                        "end": 176,
                        "text": "(Huang et al., 2012)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data and Context Information",
                "sec_num": "4.1"
            },
            {
                "text": "Also, we consider question title and body and answer text as input in qSim and qAns and use NMF to create global context vectors for questions and answers (Section 2.3).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data and Context Information",
                "sec_num": "4.1"
            },
            {
                "text": "We use pre-trained word vectors from GloVe (Pennington et al., 2014) . However, because qSim questions are about specific technical topics, we only use GloVe as initialization. For the unsupervised SCWS task, following Huang et al. (2012) , we use 100-dimensional word embeddings, d = 100, with hidden layers and context vectors of the same size, d = 100, k = 100. In this unsupervised setting, we set the weight parameter \u03bb = .5, masking noise \u03b7 = 0, depth of our model n = 3. Tuning these parameters will further improve the performance of our model.",
                "cite_spans": [
                    {
                        "start": 43,
                        "end": 68,
                        "text": "(Pennington et al., 2014)",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 219,
                        "end": 238,
                        "text": "Huang et al. (2012)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Parameter Setting",
                "sec_num": "4.2"
            },
            {
                "text": "For qSim and qAns, we use 300-dimensional word embeddings, d = 300, with hidden layers of size d = 200. We set the size of context vectors k (number of topics) using the reconstruction error of NMF on training data for different values of k. This leads to k = 200 for qAns and k = 300 for qSim. We tune the other hyper-parameters (\u03b7, n, and \u03bb) using development data.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Parameter Setting",
                "sec_num": "4.2"
            },
            {
                "text": "We set each input x (target words in SCWS, question titles and bodies in qSim, and question titles and single-sentence answers in qAns) to the average of word embeddings in the input. Input vectors could be initialized through more accurate approaches (Mikolov et al., 2013b; Li and Hovy, 2014) ; however, averaging leads to reasonable representations and is often used to initialize neural networks (Clinchant and Perronnin, 2013; Iyyer et al., 2015) .",
                "cite_spans": [
                    {
                        "start": 252,
                        "end": 275,
                        "text": "(Mikolov et al., 2013b;",
                        "ref_id": null
                    },
                    {
                        "start": 276,
                        "end": 294,
                        "text": "Li and Hovy, 2014)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 400,
                        "end": 431,
                        "text": "(Clinchant and Perronnin, 2013;",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 432,
                        "end": 451,
                        "text": "Iyyer et al., 2015)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Parameter Setting",
                "sec_num": "4.2"
            },
            {
                "text": "We first consider the contextual word similarity task in which a model should predict the semantic similarity between words in their sentential context. For this evaluation, we compute Spearman's \u03c1 correlation (Kokoska and Zwillinger, 2000) between the \"relevance scores\" predicted by different models and human judgments (Section 3).",
                "cite_spans": [
                    {
                        "start": 210,
                        "end": 240,
                        "text": "(Kokoska and Zwillinger, 2000)",
                        "ref_id": "BIBREF12"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Contextual Word Similarity",
                "sec_num": "4.3"
            },
            {
                "text": "The state-of-the-art model for this task is a semi-supervised approach (Rothe and Sch\u00fctze, 2015) . This model use resources like WordNet to compute embeddings for different senses of words. Given a pair of target words and their context (neighboring words and sentences), this model represents each target word as the average of its sense embeddings weighted by cosine similarity to the context. The cosine similarity between the representations of words in a pair is then used to determine their semantic similarity. Also, the Skip-gram model (Mikolov et al., 2013a ) is extended in (Neelakantan et al., 2014; Chen et al., 2014) to learn contextual word pair similarity in an unsupervised way.",
                "cite_spans": [
                    {
                        "start": 71,
                        "end": 96,
                        "text": "(Rothe and Sch\u00fctze, 2015)",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 544,
                        "end": 566,
                        "text": "(Mikolov et al., 2013a",
                        "ref_id": null
                    },
                    {
                        "start": 584,
                        "end": 610,
                        "text": "(Neelakantan et al., 2014;",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 611,
                        "end": 629,
                        "text": "Chen et al., 2014)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Contextual Word Similarity",
                "sec_num": "4.3"
            },
            {
                "text": "Table 2 shows the performance of different models on the SCWS dataset. SAE, CSAE-LC, CSAE-LGC show the performance of our pairwise autoencoders without context, with local context, and with local and global context, respectively. In case of CSAE-LGC, we concatenate local and global context to create context vectors. CSAE-LGC performs significantly better than the baselines, including the semi-supervised approach in Rothe and Sch\u00fctze (2015) . It is also interesting that SAE (without any context information) outperforms the pre-trained word embeddings (Pretrained embeds.).",
                "cite_spans": [
                    {
                        "start": 419,
                        "end": 443,
                        "text": "Rothe and Sch\u00fctze (2015)",
                        "ref_id": "BIBREF25"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Contextual Word Similarity",
                "sec_num": "4.3"
            },
            {
                "text": "Comparing the performance of CSAE-LC and CSAE-LGC indicates that global context is useful for accurate prediction of semantic similarity between word pairs. We further investigate these models to understand why global context is useful. Table 3 shows an example in which global context (words in neighboring sentences) effectively help to judge the semantic similarity between \"Airport\" and \"Airfield.\" This is while local context (ten neighboring words) are less effective in helping the models to relate the two words.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 243,
                        "end": 244,
                        "text": "3",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Contextual Word Similarity",
                "sec_num": "4.3"
            },
            {
                "text": "Furthermore, we study the effect of global context in different POS tag categories. As Figure 4 shows global context has greater impact on A-A and N-N categories. We expect high improvement in the N-N category as noun senses are fairly self-contained and often refer to concrete things. Thus broader (not only local) context is needed to judge their semantic similarity. However, we don't know the reason for improvement on the A-A category as, in context, adjective interpretation is often affected by local context (e.g., the nouns that adjectives modify). One reason for improvement could be because adjectives are often interchangeable and this characteristic makes their meaning to be less sensitive to local context. ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 94,
                        "end": 95,
                        "text": "4",
                        "ref_id": "FIGREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Contextual Word Similarity",
                "sec_num": "4.3"
            },
            {
                "text": "We evaluate the performance of our model in the answer ranking task in which a model should retrieve correct answers from a set of candidates for test questions. For this evaluation, we rank answers with respect to each test question according to the \"relevance score\" between question and each answer (Section 3).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Answer Ranking Performance",
                "sec_num": "4.4"
            },
            {
                "text": "The state-of-the-art model for answer ranking on qAns is a pairwise convolutional neural network (PCNN) presented in (Severyn and Moschitti, 2015) . PCNN is a supervised model that first maps input question-answer pairs to hidden representations through a standard convolutional neural network (CNN) and then utilizes these representations in a pairwise CNN to compute a relevance score for each pair. This model also utilizes external word overlap features for each questionanswer pair.5 PCNN outperforms other competing CNN models (Yu et al., 2014) and models that use syntax and semantic features (Heilman and Smith, 2010; Yao et al., 2013) .",
                "cite_spans": [
                    {
                        "start": 117,
                        "end": 146,
                        "text": "(Severyn and Moschitti, 2015)",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 533,
                        "end": 550,
                        "text": "(Yu et al., 2014)",
                        "ref_id": "BIBREF37"
                    },
                    {
                        "start": 600,
                        "end": 625,
                        "text": "(Heilman and Smith, 2010;",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 626,
                        "end": 643,
                        "text": "Yao et al., 2013)",
                        "ref_id": "BIBREF35"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Answer Ranking Performance",
                "sec_num": "4.4"
            },
            {
                "text": "N-N V-V A-A N-V 0.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Answer Ranking Performance",
                "sec_num": "4.4"
            },
            {
                "text": "Tables 4 and 5 show the performance of different models in terms of Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR) in supervised and unsupervised settings. PCNN-WO and PCNN show the baseline performance with and without word overlap features. SAE and CSAE show the performance of our pairwise autoencoders without and with context information respectively. Their \"X-DST\" versions show their performance when additional features (Equation 10) are used. These features are computed for the hidden and BOW representations of questionanswer pairs. We also include word overlap features as additional features.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "4",
                        "ref_id": "TABREF4"
                    },
                    {
                        "start": 13,
                        "end": 14,
                        "text": "5",
                        "ref_id": "TABREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Answer Ranking Performance",
                "sec_num": "4.4"
            },
            {
                "text": "Table 4 shows that SAE and CSAE consistently outperform PCNN, and SAE-DST and CSAE-DST outperform PCNN-WO when the models are trained on the larger training dataset, \"Train-All.\" But PCNN shows slightly better performance than our model on \"Train,\" the smaller training dataset. We conjecture this is because PCNN's convolution filter is wider (n-grams, n > 2) (Severyn and Moschitti, 2015) .",
                "cite_spans": [
                    {
                        "start": 361,
                        "end": 390,
                        "text": "(Severyn and Moschitti, 2015)",
                        "ref_id": "BIBREF27"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "4",
                        "ref_id": "TABREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Answer Ranking Performance",
                "sec_num": "4.4"
            },
            {
                "text": "Table 5 shows that the performance of unsupervised SAE and CSAE are comparable and in some cases better than the performance of the supervised PCNN model. We attribute the high performance of our models to context information that leads to richer representations of inputs.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "5",
                        "ref_id": "TABREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Answer Ranking Performance",
                "sec_num": "4.4"
            },
            {
                "text": "Furthermore, comparing the performance of CSAE and SAE in both supervised and unsupervised settings in Tables 4 and 5 shows that context information consistently improves the MAP and MRR performance at all settings except for MRR on \"Train\" (supervised setting) that leads to a com- parable performance. Context-sensitive representations significantly improve the performance of our model and often lead to higher MAP than the models that ignore context information.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 110,
                        "end": 111,
                        "text": "4",
                        "ref_id": "TABREF4"
                    },
                    {
                        "start": 116,
                        "end": 117,
                        "text": "5",
                        "ref_id": "TABREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Answer Ranking Performance",
                "sec_num": "4.4"
            },
            {
                "text": "In the question ranking task, given a test question, a model should retrieve top-K questions that are semantically equivalent to the test question for K = {1, 5, 10}. We use qSim for this evaluation. We compare our autoencoders against PCNN and PBOW-PCNN models presented in Dos Santos et al. (2015) . PCNN is a pairwise convolutional neural network and PBOW-PCNN is a joint model that combines vector representations obtained from a pairwise bag-of-words (PBOW) network and a pairwise convolutional neural network (PCNN). Both models are supervised as they require similarity scores to train the network.",
                "cite_spans": [
                    {
                        "start": 279,
                        "end": 299,
                        "text": "Santos et al. (2015)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Question Ranking Performance",
                "sec_num": "4.5"
            },
            {
                "text": "Table 6 shows the performance of different models in terms of Precision at Rank K, P@K. CSAE is more precise than the baseline; CSAE and CSAE-DST models consistently outperform the baselines on P@1, an important metric in search applications (CSAE also outperforms PCNN on P@5). Although context-sensitive models are more precise than the baselines at higher ranks, the PCNN and PBOW-PCNN models remain the best model for P@10.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "6",
                        "ref_id": "TABREF6"
                    }
                ],
                "eq_spans": [],
                "section": "Question Ranking Performance",
                "sec_num": "4.5"
            },
            {
                "text": "Tables 6 and 7 show that context information consistently improves the results at all ranks in both supervised and unsupervised settings. The performance of the unsupervised SAE and CSAE models are comparable with the supervised PCNN model in higher ranks. ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "6",
                        "ref_id": "TABREF6"
                    },
                    {
                        "start": 13,
                        "end": 14,
                        "text": "7",
                        "ref_id": "TABREF7"
                    }
                ],
                "eq_spans": [],
                "section": "Question Ranking Performance",
                "sec_num": "4.5"
            },
            {
                "text": "We investigate the effect of context information in reconstructing inputs and try to understand reasons for improvement in reconstruction error. We compute the average reconstruction error of SAE and CSAE (Equations ( 3) and ( 7)). For these experiments, we set \u03bb = 0 in Equation ( 7) so that we can directly compare the resulting loss of the two models. CSAE will still use context information with \u03bb = 0 but it does not backpropagate the reconstruction loss of context information.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Performance Analysis and Discussion",
                "sec_num": "5"
            },
            {
                "text": "Figures 5(a ) and 5(b) show the average reconstruction error of SAE and CSAE on qSim and qAns datasets. Context information conistently improves reconstruction. The improvement is greater on qSim which contains smaller number of words per question as compared to qAns. Also, both models generate smaller reconstruction errors than NMF (Section 2.3). The lower performance of NMF is because it reconstructs inputs merely using global topics identified in datasets, while our models utilize both local and global information to reconstruct inputs.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 8,
                        "end": 11,
                        "text": "5(a",
                        "ref_id": "FIGREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Performance Analysis and Discussion",
                "sec_num": "5"
            },
            {
                "text": "The improvement in reconstruction error mainly stems from areas in data where \"topic density\" is lower. We define topic density for a topic as the number of documents that are assigned to the topic by our topic model. We compute the average improvement in reconstruction error for each topic T j using the loss functions for the basic and contextsensitive autoencoders:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Analysis of Context information",
                "sec_num": "5.1"
            },
            {
                "text": "\u2206 j = 1 |T j | x\u2208T j l(x) -l(x, h x )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Analysis of Context information",
                "sec_num": "5.1"
            },
            {
                "text": "where we set \u03bb = 0. Figure 5 (c) shows improvement of reconstruction error versus topic density on qSim. Lower topic densities have greater improvement. This is because they have insufficient training data to train the networks. However, injecting context information improves the reconstruction power of our model by providing more information. The improvements in denser areas are smaller because neural networks can train effectively in these areas.6 ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 27,
                        "end": 28,
                        "text": "5",
                        "ref_id": "FIGREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Analysis of Context information",
                "sec_num": "5.1"
            },
            {
                "text": "The intuition behind deep autoencoders (and, generally, deep neural networks) is that each layer learns a more abstract representation of the input than the previous one (Hinton and Salakhutdinov, 2006; Bengio et al., 2013) . We investigate if adding depth to our context-sensitive autoencoder will improve its performance in the contextual word similarity task.",
                "cite_spans": [
                    {
                        "start": 170,
                        "end": 202,
                        "text": "(Hinton and Salakhutdinov, 2006;",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 203,
                        "end": 223,
                        "text": "Bengio et al., 2013)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Effect of Depth",
                "sec_num": "5.2"
            },
            {
                "text": "Figure 6 shows that as we increase the depth of our autoencoders, their performances initially improve. The CSAE-LGC model that uses both local and global context benefits more from greater number of hidden layers than CSAE-LC that only uses local context. We attribute this to the use of global context in CSAE-LGC that leads to more accurate representations of words in their context. We also note that with just a single hidden layer, CSAE-LGC largely improves the performance as compared to CSAE-LC.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "6",
                        "ref_id": "FIGREF6"
                    }
                ],
                "eq_spans": [],
                "section": "Effect of Depth",
                "sec_num": "5.2"
            },
            {
                "text": "Representation learning models have been effective in many tasks such as language modeling (Bengio et al., 2003; Mikolov et al., 2013b ), topic modeling (Nguyen et al., 2015) , paraphrase detection (Socher et al., 2011) , and ranking tasks (Yih et al., 2013) . We briefly review works that use context information for text representation. Huang et al. (2012) presented an RNN model that uses document-level context information to construct more accurate word representations. In particular, given a sequence of words, the approach uses other words in the document as external (global) knowledge to predict the next word in the sequence. Other approaches have also modeled context at the document level (Lin et al., 2015; Wang and Cho, 2015; Ji et al., 2016) . Ji et al. (2016) presented a context-sensitive RNN-based language model that integrates representations of previous sentences into the language model of the current sentence. They showed that this approach outperforms several RNN language models on a text coherence task. Liu et al. (2015) proposed a context-sensitive RNN model that uses Latent Dirichlet Allocation (Blei et al., 2003) to extract topic-specific word embeddings. Their best-performing model regards each topic that is associated to a word in a sentence as a pseudo word, learns topic and word embeddings, and then concatenates the embeddings to obtain topic-specific word embeddings. Mikolov and Zweig (2012) extended a basic RNN language model (Mikolov et al., 2010) by an additional feature layer to integrate external information (such as topic information) about inputs into the model. They showed that such information improves the perplexity of language models.",
                "cite_spans": [
                    {
                        "start": 91,
                        "end": 112,
                        "text": "(Bengio et al., 2003;",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 113,
                        "end": 134,
                        "text": "Mikolov et al., 2013b",
                        "ref_id": null
                    },
                    {
                        "start": 135,
                        "end": 174,
                        "text": "), topic modeling (Nguyen et al., 2015)",
                        "ref_id": null
                    },
                    {
                        "start": 198,
                        "end": 219,
                        "text": "(Socher et al., 2011)",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 240,
                        "end": 258,
                        "text": "(Yih et al., 2013)",
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 339,
                        "end": 358,
                        "text": "Huang et al. (2012)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 702,
                        "end": 720,
                        "text": "(Lin et al., 2015;",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 721,
                        "end": 740,
                        "text": "Wang and Cho, 2015;",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 741,
                        "end": 757,
                        "text": "Ji et al., 2016)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 760,
                        "end": 776,
                        "text": "Ji et al. (2016)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 1032,
                        "end": 1049,
                        "text": "Liu et al. (2015)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 1127,
                        "end": 1146,
                        "text": "(Blei et al., 2003)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 1411,
                        "end": 1435,
                        "text": "Mikolov and Zweig (2012)",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 1472,
                        "end": 1494,
                        "text": "(Mikolov et al., 2010)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "6"
            },
            {
                "text": "In contrast to previous research, we integrate context into deep autoencoders. To the best of our knowledge, this is the first work to do so. Also, in this paper, we depart from most previous approaches by demonstrating the value of context information in sentence-level semantic similarity and ranking tasks such as QA ranking tasks. Our approach to the ranking problems, both for Answer Ranking and Question Ranking, is different from previous approaches in the sense that we judge the relevance between inputs based on their context information. We showed that adding sentential or document context information about questions (or answers) leads to better rankings.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "6"
            },
            {
                "text": "We introduce an effective approach to integrate sentential or document context into deep autoencoders and show that such integration is important in semantic similarity tasks. In the future, we aim to investigate other types of linguistic context (such as POS tag and word dependency information, word sense, and discourse relations) and develop a unified representation learning framework that integrates such linguistic context with representation learning models.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion and Future Work",
                "sec_num": "7"
            },
            {
                "text": "For example, RNNs can predict the word \"sky\" given the sentence \"clouds are in the ,\" but they are less accurate when longer history or global context is required, e.g. predicting the word \"french\" given the paragraph \"I grew up in France. . . . I speak fluent.\"",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Figure 2(a) shows compact schematic diagrams of autoencoders used in Figures 1(a) and 1(b)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Word overlap and IDF-weighted word overlap computed for (a): all words, and (b): only non-stop words for each question-answer pair(Severyn and Moschitti, 2015).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We observed the same pattern in qAns.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "We thank anonymous reviewers for their thoughtful comments. This paper is based upon work supported, in whole or in part, with funding from the United States Government. Boyd-Graber is supported by NSF grants IIS/1320538, IIS/1409287, and NCSE/1422492. Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the view of the sponsors.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "A neural probabilistic language model",
                "authors": [
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    },
                    {
                        "first": "R\u00e9jean",
                        "middle": [],
                        "last": "Ducharme",
                        "suffix": ""
                    },
                    {
                        "first": "Pascal",
                        "middle": [],
                        "last": "Vincent",
                        "suffix": ""
                    },
                    {
                        "first": "Christian",
                        "middle": [],
                        "last": "Janvin",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "J. Mach. Learn. Res",
                "volume": "3",
                "issue": "",
                "pages": "1137--1155",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yoshua Bengio, R\u00e9jean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic lan- guage model. J. Mach. Learn. Res., 3:1137-1155, March.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Representation learning: A review and new perspectives. Pattern Analysis and Machine Intelligence",
                "authors": [
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    },
                    {
                        "first": "Aaron",
                        "middle": [],
                        "last": "Courville",
                        "suffix": ""
                    },
                    {
                        "first": "Pierre",
                        "middle": [],
                        "last": "Vincent",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "IEEE Transactions on",
                "volume": "",
                "issue": "8",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yoshua Bengio, Aaron Courville, and Pierre Vincent. 2013. Representation learning: A review and new perspectives. Pattern Analysis and Machine Intelli- gence, IEEE Transactions on, 35(8).",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Latent dirichlet allocation",
                "authors": [
                    {
                        "first": "Andrew",
                        "middle": [
                            "Y"
                        ],
                        "last": "David M Blei",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [
                            "I"
                        ],
                        "last": "Ng",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Jordan",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "the Journal of machine Learning research",
                "volume": "3",
                "issue": "",
                "pages": "993--1022",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. the Journal of ma- chine Learning research, 3:993-1022.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Syntactic topic models",
                "authors": [
                    {
                        "first": "Jordan L Boyd-Graber",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [
                            "M"
                        ],
                        "last": "Blei",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of NIPS",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jordan L Boyd-Graber and David M Blei. 2009. Syn- tactic topic models. In Proceedings of NIPS.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "A unified model for word sense representation and disambiguation",
                "authors": [
                    {
                        "first": "Xinxiong",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Zhiyuan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Maosong",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xinxiong Chen, Zhiyuan Liu, and Maosong Sun. 2014. A unified model for word sense representation and disambiguation. In Proceedings of EMNLP.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Aggregating continuous word embeddings for information retrieval. the Workshop on Continuous Vector Space Models and their Compositionality, ACL",
                "authors": [
                    {
                        "first": "St\u00e9phane",
                        "middle": [],
                        "last": "Clinchant",
                        "suffix": ""
                    },
                    {
                        "first": "Florent",
                        "middle": [],
                        "last": "Perronnin",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "St\u00e9phane Clinchant and Florent Perronnin. 2013. Ag- gregating continuous word embeddings for informa- tion retrieval. the Workshop on Continuous Vector Space Models and their Compositionality, ACL.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Learning hybrid representations to retrieve semantically equivalent questions",
                "authors": [
                    {
                        "first": "Santos",
                        "middle": [],
                        "last": "Cicero Dos",
                        "suffix": ""
                    },
                    {
                        "first": "Luciano",
                        "middle": [],
                        "last": "Barbosa",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of ACL-IJCNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Cicero Dos Santos, Luciano Barbosa, Dasha Bog- danova, and Bianca Zadrozny. 2015. Learning hy- brid representations to retrieve semantically equiva- lent questions. In Proceedings of ACL-IJCNLP.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Tree edit models for recognizing textual entailments, paraphrases, and answers to questions",
                "authors": [
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Heilman",
                        "suffix": ""
                    },
                    {
                        "first": "Noah",
                        "middle": [
                            "A"
                        ],
                        "last": "Smith",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of NAACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Michael Heilman and Noah A Smith. 2010. Tree edit models for recognizing textual entailments, para- phrases, and answers to questions. In Proceedings of NAACL.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Reducing the dimensionality of data with neural networks",
                "authors": [
                    {
                        "first": "Geoffrey",
                        "middle": [
                            "E"
                        ],
                        "last": "Hinton",
                        "suffix": ""
                    },
                    {
                        "first": "Ruslan",
                        "middle": [
                            "R"
                        ],
                        "last": "Salakhutdinov",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Science",
                "volume": "313",
                "issue": "5786",
                "pages": "504--507",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Geoffrey E Hinton and Ruslan R Salakhutdinov. 2006. Reducing the dimensionality of data with neural net- works. Science, 313(5786):504-507.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Improving word representations via global context and multiple word prototypes",
                "authors": [
                    {
                        "first": "Eric",
                        "middle": [
                            "H"
                        ],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [
                            "Y"
                        ],
                        "last": "Ng",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Proceedings of ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Eric H Huang, Richard Socher, Christopher D Man- ning, and Andrew Y Ng. 2012. Improving word representations via global context and multiple word prototypes. In Proceedings of ACL.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Deep unordered composition rivals syntactic methods for text classification",
                "authors": [
                    {
                        "first": "Mohit",
                        "middle": [],
                        "last": "Iyyer",
                        "suffix": ""
                    },
                    {
                        "first": "Varun",
                        "middle": [],
                        "last": "Manjunatha",
                        "suffix": ""
                    },
                    {
                        "first": "Jordan",
                        "middle": [],
                        "last": "Boyd-Graber",
                        "suffix": ""
                    },
                    {
                        "first": "Hal",
                        "middle": [],
                        "last": "Daum\u00e9",
                        "suffix": ""
                    },
                    {
                        "first": "Iii",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of ACL-IJCNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber, and Hal Daum\u00e9 III. 2015. Deep unordered compo- sition rivals syntactic methods for text classification. In Proceedings of ACL-IJCNLP.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Document context language models",
                "authors": [
                    {
                        "first": "Yangfeng",
                        "middle": [],
                        "last": "Ji",
                        "suffix": ""
                    },
                    {
                        "first": "Trevor",
                        "middle": [],
                        "last": "Cohn",
                        "suffix": ""
                    },
                    {
                        "first": "Lingpeng",
                        "middle": [],
                        "last": "Kong",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Dyer",
                        "suffix": ""
                    },
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Eisenstein",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "ICLR",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yangfeng Ji, Trevor Cohn, Lingpeng Kong, Chris Dyer, and Jacob Eisenstein. 2016. Document context lan- guage models. ICLR (Workshop track).",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "CRC Standard Probability and Statistics Tables and Formulae",
                "authors": [
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Kokoska",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Zwillinger",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "S. Kokoska and D. Zwillinger. 2000. CRC Standard Probability and Statistics Tables and Formulae, Stu- dent Edition. Taylor & Francis.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Distributed representations of sentences and documents",
                "authors": [
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Quoc",
                        "suffix": ""
                    },
                    {
                        "first": "Tomas",
                        "middle": [],
                        "last": "Le",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Mikolov",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of ICML",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Quoc V. Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. In Pro- ceedings of ICML.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "A model of coherence based on distributed sentence representation",
                "authors": [
                    {
                        "first": "Jiwei",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Eduard",
                        "middle": [],
                        "last": "Hovy",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jiwei Li and Eduard Hovy. 2014. A model of coher- ence based on distributed sentence representation. In Proceedings of EMNLP.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Hierarchical recurrent neural network for document modeling",
                "authors": [
                    {
                        "first": "Rui",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Shujie",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Muyun",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Mu",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Sheng",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Rui Lin, Shujie Liu, Muyun Yang, Mu Li, Ming Zhou, and Sheng Li. 2015. Hierarchical recurrent neural network for document modeling. In Proceedings of EMNLP.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Projected gradient methods for nonnegative matrix factorization",
                "authors": [
                    {
                        "first": "Chuan-Bi",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Neural computation",
                "volume": "19",
                "issue": "10",
                "pages": "2756--2779",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chuan-bi Lin. 2007. Projected gradient methods for nonnegative matrix factorization. Neural computa- tion, 19(10):2756-2779.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Tat-Seng Chua, and Maosong Sun",
                "authors": [
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Zhiyuan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of AAAI",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yang Liu, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun. 2015. Topical word embeddings. In Proceed- ings of AAAI.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Context dependent recurrent neural network language model",
                "authors": [
                    {
                        "first": "Tomas",
                        "middle": [],
                        "last": "Mikolov",
                        "suffix": ""
                    },
                    {
                        "first": "Geoffrey",
                        "middle": [],
                        "last": "Zweig",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Spoken Language Technologies",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tomas Mikolov and Geoffrey Zweig. 2012. Context dependent recurrent neural network language model. In Spoken Language Technologies. IEEE.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Recurrent neural network based language model",
                "authors": [
                    {
                        "first": "Tomas",
                        "middle": [],
                        "last": "Mikolov",
                        "suffix": ""
                    },
                    {
                        "first": "Martin",
                        "middle": [],
                        "last": "Karafiat",
                        "suffix": ""
                    },
                    {
                        "first": "Lukas",
                        "middle": [],
                        "last": "Burget",
                        "suffix": ""
                    },
                    {
                        "first": "Jan",
                        "middle": [],
                        "last": "Cernocky",
                        "suffix": ""
                    },
                    {
                        "first": "Sanjeev",
                        "middle": [],
                        "last": "Khudanpur",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of INTERSPEECH",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tomas Mikolov, Martin Karafiat, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur. 2010. Recur- rent neural network based language model. In Pro- ceedings of INTERSPEECH.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Efficient estimation of word representations in vector space",
                "authors": [
                    {
                        "first": "Tomas",
                        "middle": [],
                        "last": "Mikolov",
                        "suffix": ""
                    },
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Greg",
                        "middle": [],
                        "last": "Corrado",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Dean",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1301.3781"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word represen- tations in vector space. arXiv:1301.3781.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Distributed representations of words and phrases and their compositionality",
                "authors": [
                    {
                        "first": "Tomas",
                        "middle": [],
                        "last": "Mikolov",
                        "suffix": ""
                    },
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    },
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Greg",
                        "middle": [
                            "S"
                        ],
                        "last": "Corrado",
                        "suffix": ""
                    },
                    {
                        "first": "Jeff",
                        "middle": [],
                        "last": "Dean",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of NIPS",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor- rado, and Jeff Dean. 2013b. Distributed representa- tions of words and phrases and their compositional- ity. In Proceedings of NIPS.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Efficient nonparametric estimation of multiple embeddings per word in vector space",
                "authors": [
                    {
                        "first": "Arvind",
                        "middle": [],
                        "last": "Neelakantan",
                        "suffix": ""
                    },
                    {
                        "first": "Jeevan",
                        "middle": [],
                        "last": "Shankar",
                        "suffix": ""
                    },
                    {
                        "first": "Alexandre",
                        "middle": [],
                        "last": "Passos",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Mccallum",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Arvind Neelakantan, Jeevan Shankar, Alexandre Pas- sos, and Andrew McCallum. 2014. Efficient non- parametric estimation of multiple embeddings per word in vector space. In Proceedings of the EMNLP.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Improving topic models with latent feature word representations",
                "authors": [
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Dat Quoc Nguyen",
                        "suffix": ""
                    },
                    {
                        "first": "Lan",
                        "middle": [],
                        "last": "Billingsley",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Du",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Johnson",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "TACL",
                "volume": "3",
                "issue": "",
                "pages": "299--313",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dat Quoc Nguyen, Richard Billingsley, Lan Du, and Mark Johnson. 2015. Improving topic models with latent feature word representations. TACL, 3:299- 313.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Glove: Global vectors for word representation",
                "authors": [
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Pennington",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In Proceedings of EMNLP.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Autoextend: Extending word embeddings to embeddings for synsets and lexemes",
                "authors": [
                    {
                        "first": "Sascha",
                        "middle": [],
                        "last": "Rothe",
                        "suffix": ""
                    },
                    {
                        "first": "Hinrich",
                        "middle": [],
                        "last": "Sch\u00fctze",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of ACL-IJNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sascha Rothe and Hinrich Sch\u00fctze. 2015. Autoex- tend: Extending word embeddings to embeddings for synsets and lexemes. In Proceedings of ACL- IJNLP.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Automatic feature engineering for answer selection and extraction",
                "authors": [
                    {
                        "first": "Aliaksei",
                        "middle": [],
                        "last": "Severyn",
                        "suffix": ""
                    },
                    {
                        "first": "Alessandro",
                        "middle": [],
                        "last": "Moschitti",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Aliaksei Severyn and Alessandro Moschitti. 2013. Au- tomatic feature engineering for answer selection and extraction. In Proceedings of EMNLP.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Learning to rank short text pairs with convolutional deep neural networks",
                "authors": [
                    {
                        "first": "Aliaksei",
                        "middle": [],
                        "last": "Severyn",
                        "suffix": ""
                    },
                    {
                        "first": "Alessandro",
                        "middle": [],
                        "last": "Moschitti",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of SIGIR",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Aliaksei Severyn and Alessandro Moschitti. 2015. Learning to rank short text pairs with convolutional deep neural networks. In Proceedings of SIGIR.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Learning grounded meaning representations with autoencoders",
                "authors": [
                    {
                        "first": "Carina",
                        "middle": [],
                        "last": "Silberer",
                        "suffix": ""
                    },
                    {
                        "first": "Mirella",
                        "middle": [],
                        "last": "Lapata",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Carina Silberer and Mirella Lapata. 2014. Learn- ing grounded meaning representations with autoen- coders. In Proceedings of ACL.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection",
                "authors": [
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Eric",
                        "middle": [
                            "H"
                        ],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Pennington",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [
                            "Y"
                        ],
                        "last": "Ng",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of NIPS",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Richard Socher, Eric H. Huang, Jeffrey Pennington, Andrew Y. Ng, and Christopher D. Manning. 2011. Dynamic pooling and unfolding recursive autoen- coders for paraphrase detection. In Proceedings of NIPS.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Exploring topic coherence over many models and many topics",
                "authors": [
                    {
                        "first": "Keith",
                        "middle": [],
                        "last": "Stevens",
                        "suffix": ""
                    },
                    {
                        "first": "Philip",
                        "middle": [],
                        "last": "Kegelmeyer",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Andrzejewski",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Buttler",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Proceedings of EMNLP-CNNL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Keith Stevens, Philip Kegelmeyer, David Andrzejew- ski, and David Buttler. 2012. Exploring topic co- herence over many models and many topics. In Pro- ceedings of EMNLP-CNNL.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Extracting and composing robust features with denoising autoencoders",
                "authors": [
                    {
                        "first": "Pascal",
                        "middle": [],
                        "last": "Vincent",
                        "suffix": ""
                    },
                    {
                        "first": "Hugo",
                        "middle": [],
                        "last": "Larochelle",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    },
                    {
                        "first": "Pierre-Antoine",
                        "middle": [],
                        "last": "Manzagol",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proceedings ICML",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. 2008. Extracting and composing robust features with denoising autoen- coders. In Proceedings ICML.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion",
                "authors": [
                    {
                        "first": "Pascal",
                        "middle": [],
                        "last": "Vincent",
                        "suffix": ""
                    },
                    {
                        "first": "Hugo",
                        "middle": [],
                        "last": "Larochelle",
                        "suffix": ""
                    },
                    {
                        "first": "Isabelle",
                        "middle": [],
                        "last": "Lajoie",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    },
                    {
                        "first": "Pierre-Antoine",
                        "middle": [],
                        "last": "Manzagol",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "The Journal of Machine Learning Research",
                "volume": "11",
                "issue": "",
                "pages": "3371--3408",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. 2010. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. The Journal of Machine Learn- ing Research, 11:3371-3408.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "Larger-context language modelling",
                "authors": [
                    {
                        "first": "Tian",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Kyunghyun",
                        "middle": [],
                        "last": "Cho",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tian Wang and Kyunghyun Cho. 2015. Larger-context language modelling. CoRR, abs/1511.03729.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "What is the Jeopardy model? a quasisynchronous grammar for QA",
                "authors": [
                    {
                        "first": "Mengqiu",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Noah",
                        "middle": [
                            "A"
                        ],
                        "last": "Smith",
                        "suffix": ""
                    },
                    {
                        "first": "Teruko",
                        "middle": [],
                        "last": "Mitamura",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of EMNLP-CoNLL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mengqiu Wang, Noah A. Smith, and Teruko Mita- mura. 2007. What is the Jeopardy model? a quasi- synchronous grammar for QA. In Proceedings of EMNLP-CoNLL.",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "Answer extraction as sequence tagging with tree edit distance",
                "authors": [
                    {
                        "first": "Xuchen",
                        "middle": [],
                        "last": "Yao",
                        "suffix": ""
                    },
                    {
                        "first": "Benjamin",
                        "middle": [],
                        "last": "Van Durme",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Callisonburch",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of NAACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xuchen Yao, Benjamin Van Durme, Chris Callison- burch, and Peter Clark. 2013. Answer extraction as sequence tagging with tree edit distance. In Pro- ceedings of NAACL.",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "b36",
                "title": "Question answering using enhanced lexical semantic models",
                "authors": [
                    {
                        "first": "Wen-Tau",
                        "middle": [],
                        "last": "Yih",
                        "suffix": ""
                    },
                    {
                        "first": "Ming-Wei",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Meek",
                        "suffix": ""
                    },
                    {
                        "first": "Andrzej",
                        "middle": [],
                        "last": "Pastusiak",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wen-tau Yih, Ming-Wei Chang, Christopher Meek, and Andrzej Pastusiak. 2013. Question answering using enhanced lexical semantic models. In Proceedings of ACL.",
                "links": null
            },
            "BIBREF37": {
                "ref_id": "b37",
                "title": "Deep learning for answer sentence selection",
                "authors": [
                    {
                        "first": "Lei",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Karl",
                        "middle": [
                            "Moritz"
                        ],
                        "last": "Hermann",
                        "suffix": ""
                    },
                    {
                        "first": "Phil",
                        "middle": [],
                        "last": "Blunsom",
                        "suffix": ""
                    },
                    {
                        "first": "Stephen",
                        "middle": [],
                        "last": "Pulman",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "NIPS, Deep Learning Workshop",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Lei Yu, Karl Moritz Hermann, Phil Blunsom, and Stephen Pulman. 2014. Deep learning for answer sentence selection. In NIPS, Deep Learning Work- shop.",
                "links": null
            },
            "BIBREF38": {
                "ref_id": "b38",
                "title": "ADADELTA: an adaptive learning rate method",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Matthew",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Zeiler",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Matthew D. Zeiler. 2012. ADADELTA: an adaptive learning rate method. CoRR, abs/1212.5701.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: Schematic representation of basic and context-sensitive autoencoders: (a) Basic autoencoder maps its input x into the representation h such that it can reconstruct x with minimum loss, and (b) Context-sensitive autoencoder maps its inputs x and h c into a context-sensitive representation h (h c is the representation of the context information associated to x).",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 2: Proposed framework for integrating context into deep autoencoders. Context layer (c x and h c ) and context-sensitive representation of input (h n ) are shown in light red and gray respectively. (a) Pretraining properly initializes a stack of context-sensitive denoising autoencoders (DAE), (b) A contextsensitive deep autoencoder is created from properly initialized DAEs, (c) The network in (b) is unrolled and its parameters are fine-tuned for optimal reconstruction.",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Figure 3: Pairwise context-sensitive autoencoder for computing text pair similarity.",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "FIGREF3": {
                "num": null,
                "text": "Figure 4: Effect of global context on contextual word similarity in different parts of speech (N: noun, V: verb, A: adjective). We only consider frequent categories.",
                "uris": null,
                "fig_num": "4",
                "type_str": "figure"
            },
            "FIGREF5": {
                "num": null,
                "text": "Figure 5: Reconstruction Error and Improvement: (a) and (b) reconstruction error on qSim and qAns respectively. err N M F shows the reconstruction error of NMF. Smaller error is better, (c) improvement in reconstruction error vs. topic density: greater improvement is obtained in topics with lower density.",
                "uris": null,
                "fig_num": "5",
                "type_str": "figure"
            },
            "FIGREF6": {
                "num": null,
                "text": "Figure6: Effect of depth in contextual word similarity. Three hidden layers is optimal for this task.",
                "uris": null,
                "fig_num": "6",
                "type_str": "figure"
            },
            "TABREF2": {
                "content": "<table><tr><td>Model</td><td>Context</td><td>\u03c1\u00d7100</td></tr><tr><td>Huang et al. (2012)</td><td>LGC</td><td>65.7</td></tr><tr><td>Chen et al. (2014)</td><td>LGC</td><td>65.4</td></tr><tr><td>Neelakantan et al. (2014)</td><td>LGC</td><td>69.3</td></tr><tr><td>Rothe and Sch\u00fctze (2015)</td><td>LGC</td><td>69.8</td></tr><tr><td>Pre-trained embeds. (GloVe)</td><td>-</td><td>60.2</td></tr><tr><td>SAE</td><td>-</td><td>61.1</td></tr><tr><td>CSAE</td><td>LC</td><td>66.4</td></tr><tr><td>CSAE</td><td>LGC</td><td>70.9*</td></tr><tr><td colspan=\"3\">Table 2: Spearman's \u03c1 correlation between model</td></tr><tr><td colspan=\"3\">predictions and human judgments in contextual</td></tr><tr><td colspan=\"3\">word similarity. (LC: local context only, LGC: lo-</td></tr><tr><td>cal and global context.)</td><td/><td/></tr></table>",
                "type_str": "table",
                "text": ". . No cases in Gibraltar were reported. The airport is built on the isthmus which the Spanish Government claim not to have been ceded in the Treaty of Utrecht. Thus the integration of Gibraltar Airport in the Single European Sky system has been blocked by Spain. The 1987 agreement for joint control of the airport with. . . . . . called \"Tazi\" by the German pilots. On 23 Dec 1942, the Soviet 24th Tank Corps reached nearby Skassirskaya and on 24 Dec, the tanks reached Tatsinskaya. Without any soldiers to defend the airfield it was abandoned under heavy fire. In a little under an hour, 108 Ju-52s and 16 Ju-86s took off for Novocherkassk -leaving 72 Ju-52s and many other aircraft burning on the ground. A new base was established. . .",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table/>",
                "type_str": "table",
                "text": "",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table><tr><td>Model</td><td/><td>Train</td><td colspan=\"2\">Train-All</td></tr><tr><td/><td>MAP</td><td>MRR</td><td>MAP</td><td>MRR</td></tr><tr><td>PCNN</td><td>62.58</td><td>65.91</td><td>67.09</td><td>72.80</td></tr><tr><td>SAE</td><td colspan=\"2\">65.69* 71.70*</td><td colspan=\"2\">69.54* 75.47*</td></tr><tr><td>CSAE</td><td colspan=\"2\">67.02* 70.99*</td><td colspan=\"2\">72.29* 77.29*</td></tr><tr><td>PCNN-WO</td><td>73.29</td><td>79.62</td><td>74.59</td><td>80.78</td></tr><tr><td>SAE-DST</td><td>72.53</td><td>76.97</td><td colspan=\"2\">76.38* 82.11*</td></tr><tr><td>CSAE-DST</td><td>71.26</td><td>76.88</td><td colspan=\"2\">76.75* 82.90*</td></tr><tr><td>Model</td><td/><td>Train</td><td colspan=\"2\">Train-All</td></tr><tr><td/><td>MAP</td><td>MRR</td><td>MAP</td><td>MRR</td></tr><tr><td>SAE</td><td>63.81</td><td>69.30</td><td>66.37</td><td>71.71</td></tr><tr><td>CSAE</td><td colspan=\"2\">64.86* 69.93*</td><td colspan=\"2\">66.76* 73.79*</td></tr></table>",
                "type_str": "table",
                "text": "Answer ranking in supervised setting",
                "html": null,
                "num": null
            },
            "TABREF5": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Answer ranking in unsupervised setting.",
                "html": null,
                "num": null
            },
            "TABREF6": {
                "content": "<table><tr><td>Model</td><td>P@1</td><td>P@5</td><td>P@10</td></tr><tr><td>PCNN</td><td>20.0</td><td>33.8</td><td>40.4</td></tr><tr><td>SAE</td><td>16.8</td><td>29.4</td><td>32.8</td></tr><tr><td>CSAE</td><td>21.4</td><td>34.9</td><td>37.2</td></tr><tr><td>PBOW-PCNN</td><td>22.3</td><td>39.7</td><td>46.4</td></tr><tr><td>SAE-DST</td><td>22.2</td><td>35.9</td><td>42.0</td></tr><tr><td>CSAE-DST</td><td>24.6</td><td>37.9</td><td>38.9</td></tr><tr><td>Model</td><td>P@1</td><td>P@5</td><td>P@10</td></tr><tr><td>SAE</td><td>17.3</td><td>32.4</td><td>32.8</td></tr><tr><td>CSAE</td><td>18.6</td><td>33.2</td><td>34.1</td></tr></table>",
                "type_str": "table",
                "text": "Question ranking in supervised setting",
                "html": null,
                "num": null
            },
            "TABREF7": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Question ranking in unsupervised setting",
                "html": null,
                "num": null
            }
        }
    }
}