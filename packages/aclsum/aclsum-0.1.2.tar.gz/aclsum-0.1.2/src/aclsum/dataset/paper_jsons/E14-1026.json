{
    "paper_id": "E14-1026",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T11:57:32.192257Z"
    },
    "title": "Source-side Preordering for Translation using Logistic Regression and Depth-first Branch-and-Bound Search *",
    "authors": [
        {
            "first": "Laura",
            "middle": [],
            "last": "Jehl",
            "suffix": "",
            "affiliation": {},
            "email": "jehl@cl.uni-heidelberg.de"
        },
        {
            "first": "Adri\u00e0",
            "middle": [],
            "last": "De Gispert",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "SDL Research. East Road",
                "location": {
                    "postCode": "CB1 1BH",
                    "settlement": "Cambridge",
                    "country": "U.K"
                }
            },
            "email": "agispert@sdl.com"
        },
        {
            "first": "Mark",
            "middle": [],
            "last": "Hopkins",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "SDL Research. East Road",
                "location": {
                    "postCode": "CB1 1BH",
                    "settlement": "Cambridge",
                    "country": "U.K"
                }
            },
            "email": "mhopkins@sdl.com"
        },
        {
            "first": "William",
            "middle": [],
            "last": "Byrne",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "SDL Research. East Road",
                "location": {
                    "postCode": "CB1 1BH",
                    "settlement": "Cambridge",
                    "country": "U.K"
                }
            },
            "email": "bbyrne@sdl.com"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "We present a simple preordering approach for machine translation based on a featurerich logistic regression model to predict whether two children of the same node in the source-side parse tree should be swapped or not. Given the pair-wise children regression scores we conduct an efficient depth-first branch-and-bound search through the space of possible children permutations, avoiding using a cascade of classifiers or limiting the list of possible ordering outcomes. We report experiments in translating English to Japanese and Korean, demonstrating superior performance as (a) the number of crossing links drops by more than 10% absolute with respect to other state-of-the-art preordering approaches, (b) BLEU scores improve on 2.2 points over the baseline with lexicalised reordering model, and (c) decoding can be carried out 80 times faster.",
    "pdf_parse": {
        "paper_id": "E14-1026",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "We present a simple preordering approach for machine translation based on a featurerich logistic regression model to predict whether two children of the same node in the source-side parse tree should be swapped or not. Given the pair-wise children regression scores we conduct an efficient depth-first branch-and-bound search through the space of possible children permutations, avoiding using a cascade of classifiers or limiting the list of possible ordering outcomes. We report experiments in translating English to Japanese and Korean, demonstrating superior performance as (a) the number of crossing links drops by more than 10% absolute with respect to other state-of-the-art preordering approaches, (b) BLEU scores improve on 2.2 points over the baseline with lexicalised reordering model, and (c) decoding can be carried out 80 times faster.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Source-side preordering for translation is the task of rearranging the order of a given source sentence so that it best resembles the order of the target sentence. It is a divide-and-conquer strategy aiming to decouple long-range word movement from the core translation task. The main advantage is that translation becomes computationally cheaper as less word movement needs to be considered, which results in faster and better translations, if preordering is done well and efficiently. Preordering also can facilitate better estimation of alignment and translation models as the parallel data becomes more monotonically-aligned, and translation gains can be obtained for various system architectures, e.g. phrase-based, hierarchical phrase-based, etc.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "For these reasons, preordering has a clear research and commercial interest, as reflected by the extensive previous work on the subject (see Section 2). From these approaches, we are particularly interested in those that (i) involve little or no human intervention, (ii) require limited computational resources at runtime, and (iii) make use of available linguistic analysis tools.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this paper we propose a novel preordering approach based on a logistic regression model trained to predict whether to swap nodes in the source-side dependency tree. For each pair of sibling nodes in the tree, the model uses a feature-rich representation that includes lexical cues to make relative reordering predictions between them. Given these predictions, we conduct a depth-first branch-and-bound search through the space of possible permutations of all sibling nodes, using the regression scores to guide the search. This approach has multiple advantages. First, the search for permutations is efficient and does not require specific heuristics or hard limits for nodes with many children. Second, the inclusion of the regression prediction directly into the search allows for finer-grained global decisions as the predictions that the model is more confident about are preferred. Finally, the use of a single regression model to handle any number of child nodes avoids incurring sparsity issues, while allowing the integration of a vast number of features into the preordering model.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We empirically contrast our proposed method against another preordering approach based on automatically-extracted rules when translating English into Japanese and Korean. We demonstrate a significant reduction in number of crossing links of more than 10% absolute, as well as translation gains of over 2.2 BLEU points over the baseline.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We also show it outperforms a multi-class classification approach and analyse why this is the case.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "One useful way to organize previous preordering techniques is by how they incorporate linguistic knowledge.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "2"
            },
            {
                "text": "On one end of the spectrum we find those approaches that rely on syntactic parsers and human knowledge, typically encoded via a set of hand-crafted rules for parse tree rewriting or transformation. Examples of these can be found for French-English (Xia and McCord, 2004) , German-English (Collins et al., 2005) , Chinese-English (Wang et al., 2007) , English-Arabic (Badr et al., 2009) , English-Hindi (Ramanathan et al., 2009) , English-Korean (Hong et al., 2009) , and English-Japanese (Lee et al., 2010; Isozaki et al., 2010) . A generic set of rules for transforming SVO to SOV languages has also been described (Xu et al., 2009) . The main advantage of these approaches is that a relatively small set of good rules can yield significant improvements in translation. The common criticism they receive is that they are language-specific.",
                "cite_spans": [
                    {
                        "start": 248,
                        "end": 270,
                        "text": "(Xia and McCord, 2004)",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 288,
                        "end": 310,
                        "text": "(Collins et al., 2005)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 329,
                        "end": 348,
                        "text": "(Wang et al., 2007)",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 366,
                        "end": 385,
                        "text": "(Badr et al., 2009)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 402,
                        "end": 427,
                        "text": "(Ramanathan et al., 2009)",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 445,
                        "end": 464,
                        "text": "(Hong et al., 2009)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 488,
                        "end": 506,
                        "text": "(Lee et al., 2010;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 507,
                        "end": 528,
                        "text": "Isozaki et al., 2010)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 616,
                        "end": 633,
                        "text": "(Xu et al., 2009)",
                        "ref_id": "BIBREF29"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "2"
            },
            {
                "text": "On the other end of the spectrum, there are preordering models that rely neither on human knowledge nor on syntactic analysis, but only on word alignments. One such approach is to form a cascade of two translation systems, where the first one translates the source to its preordered version (Costa-juss\u00e0 and Fonollosa, 2006) . Alternatively, one can define models that assign a cost to the relative position of each pair of words in the sentence, and search for the sequence that optimizes the global score as a linear ordering problem (Tromble and Eisner, 2009) or as a traveling salesman problem (Visweswariah et al., 2011 ). Yet another line of work attempts to automatically induce a parse tree and a preordering model from word alignments (DeNero and Uszkoreit, 2011; Neubig et al., 2012) . These approaches are attractive due to their minimal reliance on linguistic knowledge. However, their findings reveal that the best performance is obtained when using humanaligned data which is expensive to create.",
                "cite_spans": [
                    {
                        "start": 291,
                        "end": 324,
                        "text": "(Costa-juss\u00e0 and Fonollosa, 2006)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 536,
                        "end": 562,
                        "text": "(Tromble and Eisner, 2009)",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 598,
                        "end": 624,
                        "text": "(Visweswariah et al., 2011",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 744,
                        "end": 772,
                        "text": "(DeNero and Uszkoreit, 2011;",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 773,
                        "end": 793,
                        "text": "Neubig et al., 2012)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "2"
            },
            {
                "text": "Somewhere in the middle of the spectrum are works that rely on automatic source-language syntactic parses, but no direct human intervention. Preordering rules can be automatically extracted from word alignments and constituent trees (Li et al., 2007; Habash, 2007; Visweswariah et al., 2010) , dependency trees (Genzel, 2010) or predicate-argument structures (Wu et al., 2011) , or simply part-of-speech sequences (Crego and Mari\u00f1o, 2006; Rottmann and Vogel, 2007) . Rules are assigned a cost based on Maximum Entropy (Li et al., 2007) or Maximum Likelihood estimation (Visweswariah et al., 2010) , or directly on their ability to make the training corpus more monotonic (Genzel, 2010) . The latter performs very well in practice but comes at the cost of a brute-force extraction heuristic that cannot incorporate lexical information. Recently, other approaches treat ordering the children of a node as a learning to rank (Yang et al., 2012) or discriminative multi-classification task (Lerner and Petrov, 2013) . These are appealing for their use of finergrained lexical information, but they struggle to adequately handle nodes with multiple children.",
                "cite_spans": [
                    {
                        "start": 233,
                        "end": 250,
                        "text": "(Li et al., 2007;",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 251,
                        "end": 264,
                        "text": "Habash, 2007;",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 265,
                        "end": 291,
                        "text": "Visweswariah et al., 2010)",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 311,
                        "end": 325,
                        "text": "(Genzel, 2010)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 359,
                        "end": 376,
                        "text": "(Wu et al., 2011)",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 414,
                        "end": 438,
                        "text": "(Crego and Mari\u00f1o, 2006;",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 439,
                        "end": 464,
                        "text": "Rottmann and Vogel, 2007)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 518,
                        "end": 535,
                        "text": "(Li et al., 2007)",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 569,
                        "end": 596,
                        "text": "(Visweswariah et al., 2010)",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 671,
                        "end": 685,
                        "text": "(Genzel, 2010)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 922,
                        "end": 941,
                        "text": "(Yang et al., 2012)",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 986,
                        "end": 1011,
                        "text": "(Lerner and Petrov, 2013)",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "2"
            },
            {
                "text": "Our approach is closely related to this latter work, as we are interested in feature-rich discriminative approaches that automatically learn preordering rules from source-side dependency trees. Similarly to Yang et al. (2012) we train a large discriminative linear model, but rather than model each child's position in an ordered list of children, we model a more natural pair-wise swap / no-swap preference (like Tromble and Eisner (2009) did at the word level). We then incorporate this model into a global, efficient branch-and-bound search through the space of permutations. In this way, we avoid an error-prone cascade of classifiers or any limit on the possible ordering outcomes (Lerner and Petrov, 2013) .",
                "cite_spans": [
                    {
                        "start": 207,
                        "end": 225,
                        "text": "Yang et al. (2012)",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 414,
                        "end": 439,
                        "text": "Tromble and Eisner (2009)",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 686,
                        "end": 711,
                        "text": "(Lerner and Petrov, 2013)",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "2"
            },
            {
                "text": "Like Genzel (2010), our method starts with dependency parses of source sentences (which we convert to shallow constituent trees; see Figure 1 for an example), and reorders the source text by permuting sibling nodes in the parse tree. For each non-terminal node, we first apply a logistic regression model which predicts, for each pair of child nodes, the probability that they should be swapped or kept in their original order. We then apply a depth-first branch-and-bound search to find the global optimal reordering of children. ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 140,
                        "end": 141,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Preordering using logistic regression and branch-and-bound search",
                "sec_num": "3"
            },
            {
                "text": "We build a regression model that assigns a probability of swapping any two sibling nodes, a and b, in the source-side dependency tree. The probability of swapping them is denoted p(a, b) and the probability of keeping them in their original order is 1 -p(a, b). We use LIBLINEAR (Fan et al., 2008) for training an L1-regularised logistic regression model based on positively and negatively labelled samples.",
                "cite_spans": [
                    {
                        "start": 279,
                        "end": 297,
                        "text": "(Fan et al., 2008)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Logistic regression",
                "sec_num": "3.1"
            },
            {
                "text": "We generate training examples for the logistic regression from word-aligned parallel data which is annotated with source-side dependency trees. For each non-terminal node, we extract all possible pairs of child nodes. For each pair, we obtain a binary label y \u2208 {-1, 1} by calculating whether swapping the two nodes would reduce the number of crossing alignment links. The crossing score of having two nodes a and b in the given order is",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training data",
                "sec_num": "3.1.1"
            },
            {
                "text": "cs(a, b) := |{(i, j) \u2208 A a \u00d7 A b : i > j}|",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training data",
                "sec_num": "3.1.1"
            },
            {
                "text": "where A a and A b are the target-side positions to which the words spanned by a and b are aligned.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training data",
                "sec_num": "3.1.1"
            },
            {
                "text": "The label is then given as",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training data",
                "sec_num": "3.1.1"
            },
            {
                "text": "y(a, b) = 1 , cs(a, b) > cs(b, a) -1 , cs(b, a) > cs(a, b)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training data",
                "sec_num": "3.1.1"
            },
            {
                "text": "Instances for which cs(a, b) = cs(b, a) are not included in the training data. This usually happens if either A a or A b is empty, and in this case the alignments provide no indication of which order is better. We also discard any samples from nodes that have more than 16 children, as these are rare cases that often result from parsing errors. For the root node of the tree in Figure 1 , the permutation corresponding to this path (1,4,3,2) would produce \"he the smell stand could\".",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 386,
                        "end": 387,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Training data",
                "sec_num": "3.1.1"
            },
            {
                "text": "Using a machine learning setup allows us to incorporate fine-grained information in the form of features. We use the following features to characterise pairs of nodes:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Features",
                "sec_num": "3.1.2"
            },
            {
                "text": "l",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Features",
                "sec_num": "3.1.2"
            },
            {
                "text": "The dependency labels of each node t",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Features",
                "sec_num": "3.1.2"
            },
            {
                "text": "The part-of-speech tags of each node. hw",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Features",
                "sec_num": "3.1.2"
            },
            {
                "text": "The head words and classes of each node. lm, rm The left-most and right-most words and classes of a node. dst",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Features",
                "sec_num": "3.1.2"
            },
            {
                "text": "The distances between each node and the head.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Features",
                "sec_num": "3.1.2"
            },
            {
                "text": "gap",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Features",
                "sec_num": "3.1.2"
            },
            {
                "text": "If there is a gap between nodes, the left-most and right-most words and classes in the gap.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Features",
                "sec_num": "3.1.2"
            },
            {
                "text": "In order to keep the size of our feature space manageable, we only consider features which occur at least 5 times1 . For the lexical features, we use the top 100 vocabulary items from our training data, and 51 clusters generated by mkcls (Och, 1999) . Similarly to previous work (Genzel, 2010; Yang et al., 2012) , we also explore feature conjunctions. For the tag and label classes, we generate all possible combinations up to a given size. For the lexical and distance features, we explicitly specify conjunctions with the tag and label features. Results for various feature configurations are discussed in Section 4.3.1.",
                "cite_spans": [
                    {
                        "start": 238,
                        "end": 249,
                        "text": "(Och, 1999)",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 279,
                        "end": 293,
                        "text": "(Genzel, 2010;",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 294,
                        "end": 312,
                        "text": "Yang et al., 2012)",
                        "ref_id": "BIBREF30"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Features",
                "sec_num": "3.1.2"
            },
            {
                "text": "For each non-terminal node in the source-side dependency tree, we search for the best possible permutation of its children. We define the score of a permutation \u03c0 as the product of the probabilities of its node pair orientations (swapped or unswapped):",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Search",
                "sec_num": "3.2"
            },
            {
                "text": "score(\u03c0) = 1\u2264i<j\u2264k|\u03c0[i]>\u03c0[j] p(i, j) \u2022 1\u2264i<j\u2264k|\u03c0[i]<\u03c0[j] 1 -p(i, j)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Search",
                "sec_num": "3.2"
            },
            {
                "text": "Here, we represent a permutation \u03c0 of k nodes as a k-length sequence containing each integer in {1, ..., k} exactly once. Define a partial permutation of k nodes as a k < k length sequence containing each integer in {1, ..., k} at most once. We can construct a search space over partial permutations in the natural way (see Figure 2 ). The root node represents the empty sequence and has score 1. Then, given a search node representing a k -length partial permutation \u03c0 , its successor nodes are obtained by extending it by one element:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 331,
                        "end": 332,
                        "text": "2",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Search",
                "sec_num": "3.2"
            },
            {
                "text": "score(\u03c0 \u2022 i ) = score(\u03c0 ) \u2022 j\u2208V |i>j p(i, j) \u2022 j\u2208V |i<j 1 -p(i, j)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Search",
                "sec_num": "3.2"
            },
            {
                "text": "where V = {1, ..., k}\\(\u03c0 \u2022 i ) is the set of source child positions that have not yet been visited. Observe that the nodes at search depth k correspond exactly to the set of complete permutations. To search this space, we employ depth-first branchand-bound (Balas and Toth, 1983) as our search algorithm. The idea of branch-and-bound is to remember the best scoring goal node found thus far, abandoning any partial paths that cannot lead to a better scoring goal node. Algorithm 1 gives pseudocode for the algorithm2 . If the initial bound (bound 0 ) is set to 0, the search is guaranteed to find the optimal solution. By raising the bound, which acts as an under-estimate of the best scoring permutation, search can be faster but possibly fail to find any solution. All our experiments were done with bound 0 = 0, i.e. exact search, but we discuss search time in detail and pruning alternatives in Section 4.3.2. Since we use a logistic regression model and incorporate its predictions directly as swap probabilities, our search prefers those permutations with swaps which the model is more confident about. We report translation results in English-to-Japanese/Korean. Our corpora are comprised of generic parallel data extracted from the web, with some documents extracted manually and some automatically crawled. Both have about 6M sentence pairs and roughly 100M words per language.",
                "cite_spans": [
                    {
                        "start": 257,
                        "end": 279,
                        "text": "(Balas and Toth, 1983)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Search",
                "sec_num": "3.2"
            },
            {
                "text": "The dev and test sets are also generic. Source sentences were extracted from the web and one target reference was produced by a bilingual speaker. These sentences were chosen to evenly represent 10 domains, including world news, chat/SMS, health, sport, science, business, and others. The dev/test sets contain 602/903 sentences and 14K/20K words each. We do English part-of-speech tagging using SVMTool (Gim\u00e9nez and M\u00e0rquez, 2004) and dependency parsing using MaltParser (Nivre et al., 2007) .",
                "cite_spans": [
                    {
                        "start": 404,
                        "end": 431,
                        "text": "(Gim\u00e9nez and M\u00e0rquez, 2004)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 472,
                        "end": 492,
                        "text": "(Nivre et al., 2007)",
                        "ref_id": "BIBREF17"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Search",
                "sec_num": "3.2"
            },
            {
                "text": "For translation experiments, we use a phrasebased decoder that incorporates a set of standard features and a hierarchical reordering model (Galley and Manning, 2008) with weights tuned using MERT to optimize the character-based BLEU score on the dev set. The Japanese and Korean language models are 5-grams estimated on > 350M words of generic web text.",
                "cite_spans": [
                    {
                        "start": 139,
                        "end": 165,
                        "text": "(Galley and Manning, 2008)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Search",
                "sec_num": "3.2"
            },
            {
                "text": "For training the logistic regression model, we automatically align the parallel training data and intersect the source-to-target and target-to-source alignments. We reserve a random 5K-sentence approach EJ cs (%) EK cs (%) rule-based (Genzel, 2010) 61.9 64.2 multi-class 65.2 df-bnb 51.4 51.8",
                "cite_spans": [
                    {
                        "start": 234,
                        "end": 248,
                        "text": "(Genzel, 2010)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Search",
                "sec_num": "3.2"
            },
            {
                "text": "Table 1 : Percentage of the original crossing score on the heldout set, obtained after applying each preordering approach in English-Japanese (EJ, left) and Korean (EK, right). Lower is better.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Search",
                "sec_num": "3.2"
            },
            {
                "text": "subset for intrinsic evaluation of preordering, and use the remainder for model parameter estimation. We evaluate our preordering approach with logistic regression and depth-first branch-and-bound search (in short, 'df-bnb') both in terms of reordering via crossing score reduction on the heldout set, and in terms of translation quality as measured by character-based BLEU on the test set.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Search",
                "sec_num": "3.2"
            },
            {
                "text": "We contrast our work against two data-driven preordering approaches. First, we implemented the rule-based approach of Genzel (2010) and optimised its multiple parameters for our task. We report only the best results achieved, which correspond to using \u223c100K training sentences for rule extraction, applying a sliding window width of 3 children, and creating rule sequences of \u223c60 rules. This approach cannot incorporate lexical features as that would make the brute-force rule extraction algorithm unmanageable.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Preordering baselines",
                "sec_num": "4.2"
            },
            {
                "text": "We also implemented a multi-class classification setup where we directly predict complete permutations of children nodes using multi-class classification (Lerner and Petrov, 2013) . While this is straightforward for small numbers of children, it leads to a very large number of possible permutations for larger sets of children nodes, making classification too difficult. While Lerner and Petrov (2013) use a cascade of classifiers and impose a hard limit on the possible reordering outcomes to solve this, we follow Genzel's heuristic: rather than looking at the complete set of children, we apply a sliding window of size 3 starting from the left, and make classification/reordering decisions for each window separately. Since the windows overlap, decisions made for the first window affect the order of nodes in the second window, etc. We address this by soliciting decisions from the classifier on the fly as we preorder. One lim- itation of this approach is that it is able to move children only within the window. We try to remedy this by applying the method iteratively, each time re-training the classifier on the preordered data from the previous run.",
                "cite_spans": [
                    {
                        "start": 154,
                        "end": 179,
                        "text": "(Lerner and Petrov, 2013)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 378,
                        "end": 402,
                        "text": "Lerner and Petrov (2013)",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Preordering baselines",
                "sec_num": "4.2"
            },
            {
                "text": "We now report contrastive results in the intrinsic preordering task, as measured by the number of crossing links (Genzel, 2010; Yang et al., 2012) on the 5K held-out set. Without preordering, there is an average of 22.2 crossing links in English-Japanese and 20.2 in English-Korean. Table 1 shows what percentage of these links remain after applying each preordering approach to the data. We find that the 'df-bnb' method outperforms the other approaches in both language pairs, achieving more than 10 additional percentage points reduction over the rule-based approach. Interestingly, the multi-class approach is not able to match the rule-based approach despite using additional lexical cues. We hypothesise that this is due to the sliding window heuristic, which causes a mismatch in train-test conditions: while samples are not independent of each other at test time due to window overlaps, they are considered to be so when training the classifier.",
                "cite_spans": [
                    {
                        "start": 113,
                        "end": 127,
                        "text": "(Genzel, 2010;",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 128,
                        "end": 146,
                        "text": "Yang et al., 2012)",
                        "ref_id": "BIBREF30"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 289,
                        "end": 290,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Crossing score",
                "sec_num": "4.3"
            },
            {
                "text": "We now report the effects of feature configuration and training data size for the English-Japanese case. We assess our 'df-bnb' approach in terms of the classification accuracy of the trained logistic features used acc (%) cs (%) regression model (using it to predict \u00b11 labels in the held-out set) and by the percentage of crossing alignment links reduced by preordering.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Impact of training size and feature configuration",
                "sec_num": "4.3.1"
            },
            {
                "text": "Figure 3 shows the performance of the logistic regression model over different training set sizes, extracted from the training corpus as described in Section 3. We observe a constant increase in prediction accuracy, mirrored by a steady decrease in crossing score. However, gains are less for more than 8M training examples. Note that a small variation in accuracy can produce a large variation in crossing score if two nodes are swapped which have a large number of crossing alignments.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "3",
                        "ref_id": "FIGREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Impact of training size and feature configuration",
                "sec_num": "4.3.1"
            },
            {
                "text": "Table 2 shows an ablation test for various feature configurations. We start with all features, including head word and class (hw), left-most and right-most word in each node's span (lm, rm), each node's distance to the head (dst), and left-most and right-most word of the gap between nodes (gap). We then proceed by removing features to end with only label and tag features (l,t), as in Genzel (2010) . For each configuration, we generated all tag-and label-combinations of size 2. We then specified combinations between tag and label and all other features. For the lexical features we always used conjunctions of the word itself, and its class. Class information is included for all words, not just those in the top 100 vocabulary. Table 2 shows that lexical and distance feature groups contribute to prediction accuracy and crossing score, except for the gap features, which we omit from further experiments.",
                "cite_spans": [
                    {
                        "start": 387,
                        "end": 400,
                        "text": "Genzel (2010)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "2",
                        "ref_id": "TABREF1"
                    },
                    {
                        "start": 740,
                        "end": 741,
                        "text": "2",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Impact of training size and feature configuration",
                "sec_num": "4.3.1"
            },
            {
                "text": "We now demonstrate the efficiency of branch-andbound search for the problem of finding the optimum permutation of n children at runtime. Even though in the worst case the search could explore all n! permutations, making it prohibitive for nodes with many children, in practice this does not happen. Many low-scoring paths are discarded early by branch-and-bound search so that the optimal solution can be found quickly. The top curve in Figure 4 shows the average number of nodes explored in searches run on our validation set (5K sentences) as a function of the number of children. All instances are far from the worst case3 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 444,
                        "end": 445,
                        "text": "4",
                        "ref_id": "FIGREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Run time",
                "sec_num": "4.3.2"
            },
            {
                "text": "In our experiments, the time needed to conduct exact search (bound 0 = 0) was not a problem except for a few bad cases (nodes with more than 16 children), which we simply chose not to preorder; in our data, 90% of the nodes have less than 6 children, while only 0.9% have 10 children or more, so this omission does not affect performance noticeably. We verified this on our held-out set, by carrying out exhaustive searches. We found that not preordering nodes with 16 children did not worsen the crossing score. In fact, setting a harsher limit of 10 nodes would still produce a crossing score of 51.9%, compared to the best score of 51.4%.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Run time",
                "sec_num": "4.3.2"
            },
            {
                "text": "There are various ways to speed up the search, if needed. First, one could impose a hard limit on the number of explored nodes4 . As shown in Figure 4 , a limit of 4K would still allow exact search on average for permutations of up to 11 children, while stopping search early for more children. We tested this for limits of 1K/4K nodes and obtained crossing scores of 51.9/51.5%. Alternatively, one could define a higher initial bound; since the score of a path is a product of probabilities, one would select a threshold probability . Examples of this would be the lower curves of Figure 4 . The curve labels show the crossing score produced with each threshold, and in parenthesis the percentage of searches that fail to find a solution with a better score than bound 0 , in which case children are left in their original order. As shown, this strategy proves less effective than simply limiting the number of explored nodes, because the more frequent cases with less children remain unaffected.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 149,
                        "end": 150,
                        "text": "4",
                        "ref_id": "FIGREF4"
                    },
                    {
                        "start": 589,
                        "end": 590,
                        "text": "4",
                        "ref_id": "FIGREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Run time",
                "sec_num": "4.3.2"
            },
            {
                "text": "Table 3 reports English-Japanese translation results for two different values of the distortion limit d, i.e. the maximum number of source words that the decoder is allowed to jump during search. We draw the following conclusions. Firstly, all the preordering approaches outperform the baseline and the BLEU score gain they provide increases as the distortion limit decreases. This is further analysed in Figure 5 , where we report BLEU as a function of the distortion limit in decoding for both English-Japanese and English-Korean. This reveals the power of preordering as a targeted strategy to obtain high performance at fast decoding times, since d can be drastically reduced without performance degradation which leads to huge decoding speed-ups; this is consistent with the observations in (Xu et al., 2009; Genzel, 2010; Visweswariah et al., 2011) . We also find that with preordering it is possible to apply harsher pruning conditions in decoding while still maintaining the Figure 5 : BLEU scores as a function of distortion limit in decoder (+LRM case). Top: English-Japanese. Bottom: English-Korean. exact same performance, achieving further speedups. With preordering, our system is able to decode 80 times faster while producing translation output of the same quality.",
                "cite_spans": [
                    {
                        "start": 796,
                        "end": 813,
                        "text": "(Xu et al., 2009;",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 814,
                        "end": 827,
                        "text": "Genzel, 2010;",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 828,
                        "end": 854,
                        "text": "Visweswariah et al., 2011)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "3",
                        "ref_id": "TABREF2"
                    },
                    {
                        "start": 412,
                        "end": 413,
                        "text": "5",
                        "ref_id": null
                    },
                    {
                        "start": 990,
                        "end": 991,
                        "text": "5",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Translation performance",
                "sec_num": "4.4"
            },
            {
                "text": "Secondly, we observe that the preordering gains, which are correlated with the crossing score reductions of Table 1 , are largely orthogonal to the gains obtained when incorporating a lexicalised reordering model (LRM). In fact, preordering gains are slightly larger with LRM, suggesting that this reordering model can be better estimated with preordered text. This echoes the notion that reordering models are particularly sensitive to alignment noise (DeNero and Uszkoreit, 2011; Neubig et al., 2012; Visweswariah et al., 2013) , and that a 'more monotonic' training corpus leads to better translation models.",
                "cite_spans": [
                    {
                        "start": 482,
                        "end": 502,
                        "text": "Neubig et al., 2012;",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 503,
                        "end": 529,
                        "text": "Visweswariah et al., 2013)",
                        "ref_id": "BIBREF25"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 114,
                        "end": 115,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Translation performance",
                "sec_num": "4.4"
            },
            {
                "text": "Finally, 'df-bnb' outperforms all other preordering approaches, and achieves an extra 0.5-0.8 BLEU over the rule-based one even at zero distortion limit. This is consistent with the substantial crossing score reductions reported in Section 4.3.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Translation performance",
                "sec_num": "4.4"
            },
            {
                "text": "We argue that these improvements are due to the usage of lexical features to facilitate finergrained ordering decisions, and to our better search through the children permutation space which is not restricted by sliding windows, does ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Translation performance",
                "sec_num": "4.4"
            },
            {
                "text": "[1\u79c1\u305f\u3061\u306f]we\u3001[2\u3059\u3063\u304b\u308a]quite [3\u897f\u5b89\u304c]Xi'an [4\u597d\u304d]like [5\u306b]to [6\u306a\u308a\u307e\u3057\u305f]come have \u3002 source [1we] [6have come] [5to] [2quite] [4like] [1xi'an] . rule-based [1we] [2quite] [4like] [3xi'an] [5to] [6come have] . df-bnb [1we] have [2quite] [3xi'an] [4like] [5to] [6come] . baseline \u79c1\u305f\u3061\u306f\u3092\u304b\u306a\u308a\u897f\u5b89\u3068\u540c\u69d8\u3067\u3059 \u3002 rule-based \u79c1\u305f\u3061\u306f\u304b\u306a\u308a\u306e\u3088\u3046\u306b\u897f\u5b89\u306b\u6765\u307e\u3059\u3002 df-bnb \u79c1\u305f\u3061\u306f\u304b\u306a\u308a\u897f\u5b89\u304c\u597d\u304d\u306b\u306a\u308b\u3002",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Translation performance",
                "sec_num": "4.4"
            },
            {
                "text": "Table 4 : Examples from our test data illustrating the differences between the preordering approaches.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "4",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Translation performance",
                "sec_num": "4.4"
            },
            {
                "text": "not depend heavily on getting the right decision in a multi-class scenario, and which incorporates regression to carry out a score-driven search.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Translation performance",
                "sec_num": "4.4"
            },
            {
                "text": "Table 4 gives three English-Japanese examples to illustrate the different preordering approaches. The first, very short, example is preordered correctly by the rule-based and the df-bnb approach, as the order of the brackets matches the order of the Japanese reference.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "4",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Analysis",
                "sec_num": "4.5"
            },
            {
                "text": "For longer sentences we see more differences between approaches, as illustrated by Example 2. In this case, both approaches succeed at moving prepositions to the back of the phrase (\"my experience in\", \"the bus of\"). However, while the dfbnb approach correctly moves the predicate of the second clause (\"was just tired\") to the back, the rule-based approach incorrectly moves the subject (\"a black woman named Rosa Parks\") to this position -possibly because of the verb \"named\" which occurs in the phrase. This could be an indication that the df-bnb is better suited for more complicated constructions. With the exception of phrases 4 and 8, all other phrases are in the correct order in the df-bnb reordering. None of the approaches manage to reorder \"a black woman named Rosa Parks\" to the correct order.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Analysis",
                "sec_num": "4.5"
            },
            {
                "text": "Example 3 shows that the translations into Japanese also reflect preordering quality. The original source results in \"like\" being translated as the main verb (which is incorrectly interpreted as \"to be like, to be equal to\"). The rule-based version correctly moves \"have come\" to the end, but fails to swap \"xi'an\" and \"like\", resulting in \"come\" being interpreted as a full verb, rather than an auxiliary. Only the df-bnb version achieves almost perfect reordering, resulting in the correct word choice of \u306a \u308b (to get to, to become) for \"have come to\".5 ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Analysis",
                "sec_num": "4.5"
            },
            {
                "text": "We have presented a novel preordering approach that estimates a preference for swapping or not swapping pairs of children nodes in the sourceside dependency tree by training a feature-rich logistic regression model. Given the pair-wise scores, we efficiently search through the space of possible children permutations using depth-first branch-and-bound search. The approach is able to incorporate large numbers of features including lexical cues, is efficient at runtime even with a large number of children, and proves superior to other state-of-the-art preordering approaches both in terms of crossing score and translation performance.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "5"
            },
            {
                "text": "Additional feature selection is achieved through L1regularisation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "See(Poole and Mackworth, 2010) for more details and a worked example.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Note that 12!\u2248479M nodes, whereas our search finds the optimal permutation path after exploring <10K nodes.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "As long as the limit exceeds the permutation length, a solution will always be found as search is depth-first.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "This translation is still not perfect, since it uses the wrong level of politeness, an important distinction in Japanese.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Syntactic Phrase Reordering for English-to-Arabic Statistical Machine Translation",
                "authors": [
                    {
                        "first": "Ibrahim",
                        "middle": [],
                        "last": "Badr",
                        "suffix": ""
                    },
                    {
                        "first": "Rabih",
                        "middle": [],
                        "last": "Zbib",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [],
                        "last": "Glass",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of EACL",
                "volume": "",
                "issue": "",
                "pages": "86--93",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ibrahim Badr, Rabih Zbib, and James Glass. 2009. Syntactic Phrase Reordering for English-to-Arabic Statistical Machine Translation. In Proceedings of EACL, pages 86-93, Athens, Greece.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Branch and Bound Methods for the Traveling Salesman Problem",
                "authors": [
                    {
                        "first": "Egon",
                        "middle": [],
                        "last": "Balas",
                        "suffix": ""
                    },
                    {
                        "first": "Paolo",
                        "middle": [],
                        "last": "Toth",
                        "suffix": ""
                    }
                ],
                "year": 1983,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Egon Balas and Paolo Toth. 1983. Branch and Bound Methods for the Traveling Salesman Prob- lem. Carnegie-Mellon Univ. Pittsburgh PA Manage- ment Sciences Research Group.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Clause Restructuring for Statistical Machine Translation",
                "authors": [
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Collins",
                        "suffix": ""
                    },
                    {
                        "first": "Philipp",
                        "middle": [],
                        "last": "Koehn",
                        "suffix": ""
                    },
                    {
                        "first": "Ivona",
                        "middle": [],
                        "last": "Kucerova",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of ACL",
                "volume": "",
                "issue": "",
                "pages": "531--540",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005. Clause Restructuring for Statistical Machine Translation. In Proceedings of ACL, pages 531-540, Ann Arbor, Michigan.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Statistical Machine Reordering",
                "authors": [
                    {
                        "first": "Marta",
                        "middle": [
                            "R"
                        ],
                        "last": "Costa-Juss\u00e0",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [
                            "R"
                        ],
                        "last": "Jos\u00e9",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Fonollosa",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proceedings of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "70--76",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Marta R. Costa-juss\u00e0 and Jos\u00e9 A. R. Fonollosa. 2006. Statistical Machine Reordering. In Proceedings of EMNLP, pages 70-76, Sydney, Australia.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Integration of POStag-based Source Reordering into SMT Decoding by an Extended Search Graph",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Josep",
                        "suffix": ""
                    },
                    {
                        "first": "Jos\u00e9",
                        "middle": [
                            "B"
                        ],
                        "last": "Crego",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Mari\u00f1o",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proceedings of AMTA",
                "volume": "",
                "issue": "",
                "pages": "29--36",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Josep M. Crego and Jos\u00e9 B. Mari\u00f1o. 2006. Integra- tion of POStag-based Source Reordering into SMT Decoding by an Extended Search Graph. In Pro- ceedings of AMTA, pages 29-36, Cambridge, Mas- sachusetts.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Inducing Sentence Structure from Parallel Corpora for Reordering",
                "authors": [
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Denero",
                        "suffix": ""
                    },
                    {
                        "first": "Jakob",
                        "middle": [],
                        "last": "Uszkoreit",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "193--203",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "John DeNero and Jakob Uszkoreit. 2011. Inducing Sentence Structure from Parallel Corpora for Re- ordering. In Proceedings of EMNLP, pages 193- 203, Edinburgh, Scotland, UK.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "LIBLINEAR: A Library for Large Linear Classification",
                "authors": [
                    {
                        "first": "Rong-En",
                        "middle": [],
                        "last": "Fan",
                        "suffix": ""
                    },
                    {
                        "first": "Kai-Wei",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Cho-Jui",
                        "middle": [],
                        "last": "Hsieh",
                        "suffix": ""
                    },
                    {
                        "first": "Xiang-Rui",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Chih-Jen",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Journal of Machine Learning Research",
                "volume": "9",
                "issue": "",
                "pages": "1871--1874",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang- Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A Library for Large Linear Classification. Journal of Machine Learning Research, 9:1871-1874.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "A Simple and Effective Hierarchical Phrase Reordering Model",
                "authors": [
                    {
                        "first": "Michel",
                        "middle": [],
                        "last": "Galley",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proceedings of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "847--855",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Michel Galley and Christopher D. Manning. 2008. A Simple and Effective Hierarchical Phrase Reorder- ing Model. In Proceedings of EMNLP, pages 847- 855, Honolulu, Hawaii.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Automatically learning sourceside reordering rules for large scale machine translation",
                "authors": [
                    {
                        "first": "Dmitriy",
                        "middle": [],
                        "last": "Genzel",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of COLING",
                "volume": "",
                "issue": "",
                "pages": "376--384",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dmitriy Genzel. 2010. Automatically learning source- side reordering rules for large scale machine trans- lation. In Proceedings of COLING, pages 376-384, Beijing, China.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "SVMTool: A general POS tagger generator based on Support Vector Machines",
                "authors": [
                    {
                        "first": "Jes\u00fas",
                        "middle": [],
                        "last": "Gim\u00e9nez",
                        "suffix": ""
                    },
                    {
                        "first": "Llu\u00eds",
                        "middle": [],
                        "last": "M\u00e0rquez",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of LREC",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jes\u00fas Gim\u00e9nez and Llu\u00eds M\u00e0rquez. 2004. SVMTool: A general POS tagger generator based on Support Vector Machines. In Proceedings of LREC, Lisbon, Portugal.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Syntactic Preprocessing for Statistical Machine Translation",
                "authors": [
                    {
                        "first": "Nizar",
                        "middle": [],
                        "last": "Habash",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of MT-Summit",
                "volume": "",
                "issue": "",
                "pages": "215--222",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Nizar Habash. 2007. Syntactic Preprocessing for Sta- tistical Machine Translation. In Proceedings of MT- Summit, pages 215-222, Copenhagen, Denmark.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Bridging Morpho-Syntactic Gap between Source and Target Sentences for English-Korean Statistical Machine Translation",
                "authors": [
                    {
                        "first": "Gumwon",
                        "middle": [],
                        "last": "Hong",
                        "suffix": ""
                    },
                    {
                        "first": "Seung-Wook",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Hae-Chang",
                        "middle": [],
                        "last": "Rim",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of ACL-IJCNLP",
                "volume": "",
                "issue": "",
                "pages": "233--236",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Gumwon Hong, Seung-Wook Lee, and Hae-Chang Rim. 2009. Bridging Morpho-Syntactic Gap be- tween Source and Target Sentences for English- Korean Statistical Machine Translation. In Proceed- ings of ACL-IJCNLP, pages 233-236, Suntec, Sin- gapore.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Head Finalization: A Simple Reordering Rule for SOV Languages",
                "authors": [
                    {
                        "first": "Hideki",
                        "middle": [],
                        "last": "Isozaki",
                        "suffix": ""
                    },
                    {
                        "first": "Katsuhito",
                        "middle": [],
                        "last": "Sudoh",
                        "suffix": ""
                    },
                    {
                        "first": "Hajime",
                        "middle": [],
                        "last": "Tsukada",
                        "suffix": ""
                    },
                    {
                        "first": "Kevin",
                        "middle": [],
                        "last": "Duh",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR",
                "volume": "",
                "issue": "",
                "pages": "244--251",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and Kevin Duh. 2010. Head Finalization: A Simple Re- ordering Rule for SOV Languages. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 244-251, Up- psala, Sweden.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Constituent Reordering and Syntax Models for English-to-Japanese Statistical Machine Translation",
                "authors": [
                    {
                        "first": "Young-Suk",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Bing",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaoqian",
                        "middle": [],
                        "last": "Luo",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of COLING",
                "volume": "",
                "issue": "",
                "pages": "626--634",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Young-Suk Lee, Bing Zhao, and Xiaoqian Luo. 2010. Constituent Reordering and Syntax Models for English-to-Japanese Statistical Machine Trans- lation. In Proceedings of COLING, pages 626-634, Beijing, China.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Source-Side Classifier Preordering for Machine Translation",
                "authors": [
                    {
                        "first": "Uri",
                        "middle": [],
                        "last": "Lerner",
                        "suffix": ""
                    },
                    {
                        "first": "Slav",
                        "middle": [],
                        "last": "Petrov",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Uri Lerner and Slav Petrov. 2013. Source-Side Clas- sifier Preordering for Machine Translation. In Pro- ceedings of EMNLP, Seattle, USA.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "A Probabilistic Approach to Syntax-based Reordering for Statistical Machine Translation",
                "authors": [
                    {
                        "first": "Chi-Ho",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Minghui",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Dongdong",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Mu",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Yi",
                        "middle": [],
                        "last": "Guan",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of ACL",
                "volume": "",
                "issue": "",
                "pages": "720--727",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li, Ming Zhou, and Yi Guan. 2007. A Probabilistic Approach to Syntax-based Reordering for Statistical Machine Translation. In Proceedings of ACL, pages 720-727, Prague, Czech Republic.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Inducing a Discriminative Parser to Optimize Machine Translation Reordering",
                "authors": [
                    {
                        "first": "Graham",
                        "middle": [],
                        "last": "Neubig",
                        "suffix": ""
                    },
                    {
                        "first": "Taro",
                        "middle": [],
                        "last": "Watanabe",
                        "suffix": ""
                    },
                    {
                        "first": "Shinsuke",
                        "middle": [],
                        "last": "Mori",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Proceedings of EMNLP-CoNLL",
                "volume": "",
                "issue": "",
                "pages": "843--853",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Graham Neubig, Taro Watanabe, and Shinsuke Mori. 2012. Inducing a Discriminative Parser to Optimize Machine Translation Reordering. In Proceedings of EMNLP-CoNLL, pages 843-853, Jeju Island, Korea.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Maltparser: A language-independent system for data-driven dependency parsing",
                "authors": [
                    {
                        "first": "Joakim",
                        "middle": [],
                        "last": "Nivre",
                        "suffix": ""
                    },
                    {
                        "first": "Johan",
                        "middle": [],
                        "last": "Hall",
                        "suffix": ""
                    },
                    {
                        "first": "Jens",
                        "middle": [],
                        "last": "Nilsson",
                        "suffix": ""
                    },
                    {
                        "first": "Atanas",
                        "middle": [],
                        "last": "Chanev",
                        "suffix": ""
                    },
                    {
                        "first": "G\u00fclsen",
                        "middle": [],
                        "last": "Eryigit",
                        "suffix": ""
                    },
                    {
                        "first": "Sandra",
                        "middle": [],
                        "last": "K\u00fcbler",
                        "suffix": ""
                    },
                    {
                        "first": "Svetoslav",
                        "middle": [],
                        "last": "Marinov",
                        "suffix": ""
                    },
                    {
                        "first": "Erwin",
                        "middle": [],
                        "last": "Marsi",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Natural Language Engineering",
                "volume": "13",
                "issue": "2",
                "pages": "95--135",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, G\u00fclsen Eryigit, Sandra K\u00fcbler, Svetoslav Marinov, and Erwin Marsi. 2007. Maltparser: A language-independent system for data-driven de- pendency parsing. Natural Language Engineering, 13(2):95-135.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "An efficient method for determining bilingual word classes",
                "authors": [
                    {
                        "first": "Josef",
                        "middle": [],
                        "last": "Franz",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Och",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "Proceedings of EACL",
                "volume": "",
                "issue": "",
                "pages": "71--76",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Franz Josef Och. 1999. An efficient method for de- termining bilingual word classes. In Proceedings of EACL, pages 71-76, Bergen, Norway.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Artificial Intelligence: Foundations of Computational Agents",
                "authors": [
                    {
                        "first": "David",
                        "middle": [
                            "L"
                        ],
                        "last": "Poole",
                        "suffix": ""
                    },
                    {
                        "first": "Alan",
                        "middle": [
                            "K"
                        ],
                        "last": "Mackworth",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "David L. Poole and Alan K. Mackworth. 2010. Ar- tificial Intelligence: Foundations of Computational Agents. Cambridge University Press. Full text on- line at http://artint.info.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Case markers and Morphology: Addressing the crux of the fluency problem in English-Hindi SMT",
                "authors": [
                    {
                        "first": "Ananthakrishnan",
                        "middle": [],
                        "last": "Ramanathan",
                        "suffix": ""
                    },
                    {
                        "first": "Hansraj",
                        "middle": [],
                        "last": "Choudhary",
                        "suffix": ""
                    },
                    {
                        "first": "Avishek",
                        "middle": [],
                        "last": "Ghosh",
                        "suffix": ""
                    },
                    {
                        "first": "Pushpak",
                        "middle": [],
                        "last": "Bhattacharyya",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of ACL-IJCNLP",
                "volume": "",
                "issue": "",
                "pages": "800--808",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ananthakrishnan Ramanathan, Hansraj Choudhary, Avishek Ghosh, and Pushpak Bhattacharyya. 2009. Case markers and Morphology: Addressing the crux of the fluency problem in English-Hindi SMT. In Proceedings of ACL-IJCNLP, pages 800-808, Sun- tec, Singapore.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Word Reordering in Statistical Machine Translation with a POS-Based Distortion Model",
                "authors": [
                    {
                        "first": "Kay",
                        "middle": [],
                        "last": "Rottmann",
                        "suffix": ""
                    },
                    {
                        "first": "Stephan",
                        "middle": [],
                        "last": "Vogel",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of TMI",
                "volume": "",
                "issue": "",
                "pages": "171--180",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kay Rottmann and Stephan Vogel. 2007. Word Re- ordering in Statistical Machine Translation with a POS-Based Distortion Model. In Proceedings of TMI, pages 171-180, Sk\u00f6vde, Sweden.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Learning linear ordering problems for better translation",
                "authors": [
                    {
                        "first": "Roy",
                        "middle": [],
                        "last": "Tromble",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Eisner",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "1007--1016",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Roy Tromble and Jason Eisner. 2009. Learning linear ordering problems for better translation. In Proceed- ings of EMNLP, pages 1007-1016, Singapore.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Syntax based reordering with automatically derived rules for improved statistical machine translation",
                "authors": [
                    {
                        "first": "Karthik",
                        "middle": [],
                        "last": "Visweswariah",
                        "suffix": ""
                    },
                    {
                        "first": "Jiri",
                        "middle": [],
                        "last": "Navratil",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Sorensen",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of COLING",
                "volume": "",
                "issue": "",
                "pages": "1119--1127",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen, Vijil Chenthamarakshan, and Nandakishore Kamb- hatla. 2010. Syntax based reordering with auto- matically derived rules for improved statistical ma- chine translation. In Proceedings of COLING, pages 1119-1127, Beijing, China.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "A word reordering model for improved machine translation",
                "authors": [
                    {
                        "first": "Karthik",
                        "middle": [],
                        "last": "Visweswariah",
                        "suffix": ""
                    },
                    {
                        "first": "Rajakrishnan",
                        "middle": [],
                        "last": "Rajkumar",
                        "suffix": ""
                    },
                    {
                        "first": "Ankur",
                        "middle": [],
                        "last": "Gandhe",
                        "suffix": ""
                    },
                    {
                        "first": "Ananthakrishnan",
                        "middle": [],
                        "last": "Ramanathan",
                        "suffix": ""
                    },
                    {
                        "first": "Jiri",
                        "middle": [],
                        "last": "Navratil",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "486--496",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Karthik Visweswariah, Rajakrishnan Rajkumar, Ankur Gandhe, Ananthakrishnan Ramanathan, and Jiri Navratil. 2011. A word reordering model for improved machine translation. In Proceedings of EMNLP, pages 486-496, Edinburgh, United King- dom.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Cut the noise: Mutually reinforcing reordering and alignments for improved machine translation",
                "authors": [
                    {
                        "first": "Karthik",
                        "middle": [],
                        "last": "Visweswariah",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Mitesh",
                        "suffix": ""
                    },
                    {
                        "first": "Ananthakrishnan",
                        "middle": [],
                        "last": "Khapra",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ramanathan",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of ACL",
                "volume": "",
                "issue": "",
                "pages": "1275--1284",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Karthik Visweswariah, Mitesh M. Khapra, and Anan- thakrishnan Ramanathan. 2013. Cut the noise: Mu- tually reinforcing reordering and alignments for im- proved machine translation. In Proceedings of ACL, pages 1275-1284, Sofia, Bulgaria.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Chinese Syntactic Reordering for Statistical Machine Translation",
                "authors": [
                    {
                        "first": "Chao",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Collins",
                        "suffix": ""
                    },
                    {
                        "first": "Philipp",
                        "middle": [],
                        "last": "Koehn",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of EMNLP-CoNLL",
                "volume": "",
                "issue": "",
                "pages": "737--745",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chao Wang, Michael Collins, and Philipp Koehn. 2007. Chinese Syntactic Reordering for Statistical Machine Translation. In Proceedings of EMNLP- CoNLL, pages 737-745, Prague, Czech Republic.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Extracting Pre-ordering Rules from Predicate-Argument Structures",
                "authors": [
                    {
                        "first": "Xianchao",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Katsuhito",
                        "middle": [],
                        "last": "Sudoh",
                        "suffix": ""
                    },
                    {
                        "first": "Kevin",
                        "middle": [],
                        "last": "Duh",
                        "suffix": ""
                    },
                    {
                        "first": "Hajime",
                        "middle": [],
                        "last": "Tsukada",
                        "suffix": ""
                    },
                    {
                        "first": "Masaaki",
                        "middle": [],
                        "last": "Nagata",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of IJCNLP",
                "volume": "",
                "issue": "",
                "pages": "29--37",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Hajime Tsukada, and Masaaki Nagata. 2011. Extracting Pre-ordering Rules from Predicate-Argument Struc- tures. In Proceedings of IJCNLP, pages 29-37, Chi- ang Mai, Thailand.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Improving a statistical MT system with automatically learned rewrite patterns",
                "authors": [
                    {
                        "first": "Fei",
                        "middle": [],
                        "last": "Xia",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Mccord",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of COLING",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Fei Xia and Michael McCord. 2004. Improving a statistical MT system with automatically learned rewrite patterns. In Proceedings of COLING, Geneva, Switzerland.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Using a Dependency Parser to Improve SMT for Subject-Object-Verb Languages",
                "authors": [
                    {
                        "first": "Peng",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Jaeho",
                        "middle": [],
                        "last": "Kang",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Ringgaard",
                        "suffix": ""
                    },
                    {
                        "first": "Franz",
                        "middle": [],
                        "last": "Och",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of HTL-NAACL",
                "volume": "",
                "issue": "",
                "pages": "245--253",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz Och. 2009. Using a Dependency Parser to Improve SMT for Subject-Object-Verb Languages. In Pro- ceedings of HTL-NAACL, pages 245-253, Boulder, Colorado.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "A ranking-based approach to word reordering for statistical machine translation",
                "authors": [
                    {
                        "first": "Nan",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Mu",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Dongdong",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Nenghai",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Proceedings of ACL",
                "volume": "",
                "issue": "",
                "pages": "912--920",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Nan Yang, Mu Li, Dongdong Zhang, and Nenghai Yu. 2012. A ranking-based approach to word reordering for statistical machine translation. In Proceedings of ACL, pages 912-920, Jeju Island, Korea.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: Shallow constituent tree generated from the dependency tree. Non-terminal nodes inherit the tag from the head.",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Figure2: Branch-and-bound search: Partial search space of permutations for a dependency tree node with four children. The gray node marks a goal node. For the root node of the tree in Figure1, the permutation corresponding to this path (1,4,3,2) would produce \"he the smell stand could\".",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF3": {
                "num": null,
                "text": "Figure 3: Crossing scores and classification accuracy improve with training data size.",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "FIGREF4": {
                "num": null,
                "text": "Figure 4: Average number of nodes explored in branch-and-bound search by number of children.",
                "uris": null,
                "fig_num": "4",
                "type_str": "figure"
            },
            "FIGREF5": {
                "num": null,
                "text": "25.93 +0.54 27.65 +1.03 10 multi-class 25.60 +0.21 26.10 -0.52 df-bnb 26.73 +1.34 28.09 +1.47 baseline 25.07 -25.92rule-based 26.35 +1.28 27.54 +1.62 4 multi-class 25.37 +0.30 26.31 +0.39 df-bnb 26.98 +1.91 28.13 +2.21",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF6": {
                "num": null,
                "text": "and calculate a bound depending on the size n of the permutation as bound 0 = p n\u2022(n-1) 2",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td colspan=\"2\">Require: k: maximum sequence length, : empty sequence,</td></tr><tr><td colspan=\"2\">bound0: initial bound</td></tr><tr><td colspan=\"2\">procedure BNBSEARCH( , bound0, k)</td></tr><tr><td colspan=\"2\">best path \u2190 \u22a5</td></tr><tr><td colspan=\"2\">bound \u2190 bound0</td></tr><tr><td>SEARCH(</td><td>)</td></tr><tr><td colspan=\"2\">return best path</td></tr><tr><td>end procedure</td><td/></tr><tr><td colspan=\"2\">procedure SEARCH(\u03c0 )</td></tr><tr><td colspan=\"2\">if score(\u03c0 ) &gt; bound then</td></tr><tr><td colspan=\"2\">if |\u03c0 | = k then</td></tr><tr><td colspan=\"2\">best path \u2190 \u03c0</td></tr><tr><td colspan=\"2\">bound \u2190 score(\u03c0 )</td></tr><tr><td colspan=\"2\">return</td></tr><tr><td>else</td><td/></tr><tr><td colspan=\"2\">for each i \u2208 {1, ..., k}\\\u03c0 do</td></tr><tr><td colspan=\"2\">SEARCH(\u03c0 \u2022 i )</td></tr><tr><td colspan=\"2\">end for</td></tr><tr><td>end if</td><td/></tr><tr><td>end if</td><td/></tr><tr><td>end procedure</td><td/></tr><tr><td colspan=\"2\">4 Experiments</td></tr><tr><td>4.1 Setup</td><td/></tr></table>",
                "type_str": "table",
                "text": "Algorithm 1 Depth-first branch-and-bound",
                "html": null,
                "num": null
            },
            "TABREF1": {
                "content": "<table><tr><td>l,t,hw,lm,rm,dst,gap</td><td>82.43</td><td>51.3</td></tr><tr><td>l,t,hw,lm,rm,dst</td><td>82.44</td><td>51.4</td></tr><tr><td>l,t,hw,lm,rm</td><td>82.32</td><td>53.1</td></tr><tr><td>l,t,hw</td><td>82.02</td><td>55</td></tr><tr><td>l,t</td><td>81.07</td><td>58.4</td></tr></table>",
                "type_str": "table",
                "text": "Ablation tests showing crossing scores and classification accuracy as features are removed. All models were trained on 8M samples.",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table/>",
                "type_str": "table",
                "text": "English-Japanese BLEU scores with various preordering approaches (and improvement over baseline) under two distortion limits d. Results reported both excluding and including lexicalised reordering model features (LRM).",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td>Example 1</td><td colspan=\"2\">reference source preordered [1Barlow] he [2the smell] [3stand] [4could] [5hoped] [6.] [1\u30d0\u30fc\u30ed\u30fc\u306f]Barlow [2\u60aa\u81ed\u306b]the smell [3\u6211\u6162]endure [4\u3067\u304d\u308b\u3053\u3068\u3092]could [5\u9858\u3063\u305f]hoped [6\u3002] [1Barlow] [5hoped] he [4could] [3stand] [2the smell] [6.]</td></tr><tr><td>Example 2</td><td colspan=\"2\">reference [1\u79c1\u81ea\u8eab\u306e]source [3In] [1my own] [2experience] , a [6black] [7woman] [5named] [4Rosa Parks] [14was just tired] [8one day] [14of] [13being told] [12to sit] [11in the back] [10of the bus] .</td></tr><tr><td/><td colspan=\"2\">rule-based [1my own] [2experience] [3In] [14was just tired] [13being told] [10the bus of] [11the back in] [12sit to] [14of]</td></tr><tr><td/><td/><td>[8one day] , [6a black] [7woman] [4Rosa Parks] [5named] .</td></tr><tr><td/><td>df-bnb</td><td>[1my own] [2experience] [3In] , [5named] [6a black] [7woman] [4Rosa Parks] [10the bus of] [11the back in]</td></tr><tr><td/><td/><td>[12sit to] [13told being] [14of] [8one day] [14was just tired] .</td></tr><tr><td/><td>reference</td></tr><tr><td>Example 3</td><td/></tr></table>",
                "type_str": "table",
                "text": "my own [2\u7d4c\u9a13]experience [3\u306b\u304a\u3044\u3066]in , [4\u30ed\u30fc\u30b6\u30d1\u30eb\u30af\u30b9]Rosa Parks [5\u3068\u3044\u3046]called [6\u9ed2\u4eba\u306e]black [7\u5973\u6027\u306f]woman, [8\u3042\u308b\u65e5]one day [9\u3068\u306b\u304b\u304f\u3068\u306b\u304b\u304f]somehow [10\u30d0\u30b9\u306e]bus of [11\u5f8c\u90e8\u5ea7\u5e2d\u306b]back seat in [12\u5750\u308b]sit \u3088\u3046\u306b [13\u8a00\u308f\u308c\u308b]told being [14\u3053\u3068\u306b]of [15\u3046\u3093\u3056\u308a\u3059]was fed up with \u3002",
                "html": null,
                "num": null
            }
        }
    }
}