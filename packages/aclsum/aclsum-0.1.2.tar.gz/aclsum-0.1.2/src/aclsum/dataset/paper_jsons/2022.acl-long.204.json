{
    "paper_id": "2022",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T11:54:54.972322Z"
    },
    "title": "Multimodal Dialogue Response Generation",
    "authors": [
        {
            "first": "Qingfeng",
            "middle": [],
            "last": "Sun",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Microsoft Research Asia",
                "location": {}
            },
            "email": ""
        },
        {
            "first": "Yujing",
            "middle": [],
            "last": "Wang",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Microsoft Research Asia",
                "location": {}
            },
            "email": "yujwang@microsoft.com"
        },
        {
            "first": "Can",
            "middle": [],
            "last": "Xu",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Microsoft Research Asia",
                "location": {}
            },
            "email": "caxu@microsoft.com"
        },
        {
            "first": "Kai",
            "middle": [],
            "last": "Zheng",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Microsoft Research Asia",
                "location": {}
            },
            "email": "zhengkai@microsoft.com"
        },
        {
            "first": "Yaming",
            "middle": [],
            "last": "Yang",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Microsoft Research Asia",
                "location": {}
            },
            "email": "yayaming@microsoft.com"
        },
        {
            "first": "Huang",
            "middle": [],
            "last": "Hu",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Microsoft Research Asia",
                "location": {}
            },
            "email": ""
        },
        {
            "first": "Fei",
            "middle": [],
            "last": "Xu",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Microsoft Research Asia",
                "location": {}
            },
            "email": "fexu@microsoft.com"
        },
        {
            "first": "Jessica",
            "middle": [],
            "last": "Zhang",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Microsoft Research Asia",
                "location": {}
            },
            "email": "jessicaz@microsoft.com"
        },
        {
            "first": "Xiubo",
            "middle": [],
            "last": "Geng",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Microsoft Research Asia",
                "location": {}
            },
            "email": "xigeng@microsoft.com"
        },
        {
            "first": "Daxin",
            "middle": [],
            "last": "Jiang",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Microsoft Research Asia",
                "location": {}
            },
            "email": "djiang@microsoft.com"
        },
        {
            "first": "Microsoft",
            "middle": [],
            "last": "Stc",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Microsoft Research Asia",
                "location": {}
            },
            "email": ""
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Responsing with image has been recognized as an important capability for an intelligent conversational agent. Yet existing works only focus on exploring the multimodal dialogue models which depend on retrieval-based methods, but neglecting generation methods. To fill in the gaps, we first present a new task: multimodal dialogue response generation (MDRG)given the dialogue context, one model needs to generate a text or an image as response. Learning such a MDRG model often requires multimodal dialogues containing both texts and images which are difficult to obtain. Motivated by the challenge in practice, we consider MDRG under a natural assumption that only limited training examples are available. Under such a low-resource setting, we devise a novel conversational agent, Divter, in order to isolate parameters that depend on multimodal dialogues from the entire generation model. By this means, the major part of the model can be learned from a large number of text-only dialogues and textimage pairs respectively, then the whole parameters can be well fitted using just a few training examples. Extensive experiments demonstrate our method achieves state-of-the-art results in both automatic and human evaluation, and can generate informative text and high-resolution image responses.",
    "pdf_parse": {
        "paper_id": "2022",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Responsing with image has been recognized as an important capability for an intelligent conversational agent. Yet existing works only focus on exploring the multimodal dialogue models which depend on retrieval-based methods, but neglecting generation methods. To fill in the gaps, we first present a new task: multimodal dialogue response generation (MDRG)given the dialogue context, one model needs to generate a text or an image as response. Learning such a MDRG model often requires multimodal dialogues containing both texts and images which are difficult to obtain. Motivated by the challenge in practice, we consider MDRG under a natural assumption that only limited training examples are available. Under such a low-resource setting, we devise a novel conversational agent, Divter, in order to isolate parameters that depend on multimodal dialogues from the entire generation model. By this means, the major part of the model can be learned from a large number of text-only dialogues and textimage pairs respectively, then the whole parameters can be well fitted using just a few training examples. Extensive experiments demonstrate our method achieves state-of-the-art results in both automatic and human evaluation, and can generate informative text and high-resolution image responses.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "With the development of instant messaging technology in the recent decades, the intermediary of online conversation has also changed from pure text to a variety of visual modalities (e.g., image, gif animation, short video). Similar to communicating by the messenger tools (e.g., Facebook, WhatsApp, WeChat) in reality, an excellent intelligent conversational agent should not only be able to converse freely with plain text, but also have the ability to perceive and share the real visual physical world. Although recently some large-scale pre-trained text-only dialogue generation models, such as Di-aloGPT (Zhang et al., 2020) , Blender (Roller et al., 2021) , Meena (Adiwardana et al., 2020) , have shown excellent performance, they still cannot rely exclusively on plain text to completely simulate the rich experience of visual perception. Recently, various vision-language tasks have been introduced and attracted widespread attention, such as visual question answering (Ren et al., 2015; Lu et al., 2016; Anderson et al., 2018; Li et al., 2019a; Huang et al., 2020) , image captioning (Xu et al., 2015; Anderson et al., 2016; Ghanimifard and Dobnik, 2019; Cornia et al., 2020) , image-grounded dialogue (Das et al., 2017; Yang et al., 2021; Agarwal et al., 2020; Qi et al., 2020; Chen et al., 2021; Liang et al., 2021) . Specifically, in human conversations, the images can easily show rich visual perception, which is hard to be expressed by plain text. As the example shown in Figure 1 , images are required in at least three circumstances: (i) the other speaker has little knowledge (e.g., colorful Burano, in the 1st image) of the objects only you had seen; (ii) to share more details (e.g., red wine and pasta, in the 2nd image) of the objects even you have common knowledge of them; (iii) to express your emotions (e.g., happy, in the 3rd image) about a specific event. An existing related task is photo sharing (Zang et al., 2021) , which aims to select and share the image based on the textual context, is a challenging task that requires models to understand the background story which complemented by human imaginations, rather than to locate related visual objects or explicitly mention main visible content in the image as the previous works do. Zang et al. (2021) propose a retrieval-based method to resolve the above challenge. However, the performance of the retrieval-based method is limited in specific domains by the size of the pre-constructed conversational history repository, especially for long-tail contexts that are not covered in the history, where the set of image responses of a retrieval system is also fixed. On the other hand, a better way is to generate a new one accordingly.",
                "cite_spans": [
                    {
                        "start": 609,
                        "end": 629,
                        "text": "(Zhang et al., 2020)",
                        "ref_id": "BIBREF58"
                    },
                    {
                        "start": 640,
                        "end": 661,
                        "text": "(Roller et al., 2021)",
                        "ref_id": "BIBREF39"
                    },
                    {
                        "start": 670,
                        "end": 695,
                        "text": "(Adiwardana et al., 2020)",
                        "ref_id": null
                    },
                    {
                        "start": 977,
                        "end": 995,
                        "text": "(Ren et al., 2015;",
                        "ref_id": "BIBREF38"
                    },
                    {
                        "start": 996,
                        "end": 1012,
                        "text": "Lu et al., 2016;",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 1013,
                        "end": 1035,
                        "text": "Anderson et al., 2018;",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 1036,
                        "end": 1053,
                        "text": "Li et al., 2019a;",
                        "ref_id": null
                    },
                    {
                        "start": 1054,
                        "end": 1073,
                        "text": "Huang et al., 2020)",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 1093,
                        "end": 1110,
                        "text": "(Xu et al., 2015;",
                        "ref_id": "BIBREF51"
                    },
                    {
                        "start": 1111,
                        "end": 1133,
                        "text": "Anderson et al., 2016;",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 1134,
                        "end": 1163,
                        "text": "Ghanimifard and Dobnik, 2019;",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 1164,
                        "end": 1184,
                        "text": "Cornia et al., 2020)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 1211,
                        "end": 1229,
                        "text": "(Das et al., 2017;",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 1230,
                        "end": 1248,
                        "text": "Yang et al., 2021;",
                        "ref_id": "BIBREF53"
                    },
                    {
                        "start": 1249,
                        "end": 1270,
                        "text": "Agarwal et al., 2020;",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 1271,
                        "end": 1287,
                        "text": "Qi et al., 2020;",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 1288,
                        "end": 1306,
                        "text": "Chen et al., 2021;",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 1307,
                        "end": 1326,
                        "text": "Liang et al., 2021)",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 1926,
                        "end": 1945,
                        "text": "(Zang et al., 2021)",
                        "ref_id": "BIBREF54"
                    },
                    {
                        "start": 2266,
                        "end": 2284,
                        "text": "Zang et al. (2021)",
                        "ref_id": "BIBREF54"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 1494,
                        "end": 1495,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this paper, we formulate a new problem: Multimodal Dialogue Response Generation (MDRG), that is, given the dialogue context, the model should not only generate a pure text response but also have the capacity to generate a multimodal response (e.g., containing both image and text). We argue that there are still some hindrances to application, since (1) the sophisticated neural end-to-end architecture will overfit to very few well-annotated training data (e.g., a few existing 10k multimodal dialogues). Evidence is that when discussing the topics outside the training data domain, its performance drops dramatically; and (2) as human effort is expensive, it is not easy to collect enough training data for a new domain. Based on the above facts, we take a step further to extend the assumption of MDRG to a low-resource setting where only a few multimodal dialogues are available.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "To tackle the above challenges, our key idea is to make parameters that rely on multimodal dialogues small and independent by disentangling textual response generation and image response generation, and thus we can learn the major part of the generation model from text-only dialogues and <image description, image> pairs that are much easier to be obtained. Specifically, we present Divter, a novel conversational agent powered by large-scale visual world experiences. As shown in Figure 2 , our Divter is made up of two Transformer-based (Vaswani et al., 2017a) components: a multimodal dialogue response generator, and a text-to-image translator. Divter takes the dialogue context as input, then generates a textual sequence which may contains a text response or a textual image description or both of them. The text-to-image translator takes above image description as condition, then generates a realistic and consistent high resolution image. Both components are independent with the opposite knowledge, and thus can be pre-trained using a large number of text-only dialogues and the <image description, image> pairs respectively. The end-to-end Divter depends on the multimodal dialogues constructed as the tuple: (dialogue context, text response / <image description, image>) , but the joint learning and estimation of the two components just require a few training examples depending on specific domains.",
                "cite_spans": [
                    {
                        "start": 540,
                        "end": 563,
                        "text": "(Vaswani et al., 2017a)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 489,
                        "end": 490,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Contributions of this work are three-fold:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 To the best of our knowledge, it is the first work on the multimodal dialogue response generation. We explore the task under a lowresource setting where only a few multimodal dialogues are assumed available.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 We present Divter, a novel conversational agent which can effectively understand dialogue context and generate informative text and high-resolution image responses.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 Extensive experiments on PhotoChat Corpus (Zang et al., 2021) indicate the effectiveness of Divter, it achieves a significant improvement with pure text dialogue generation model and retrieval-based image sharing method.",
                "cite_spans": [
                    {
                        "start": 44,
                        "end": 63,
                        "text": "(Zang et al., 2021)",
                        "ref_id": "BIBREF54"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "2 Related Work",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "End-to-end response generation for textual opendomain dialogues is inspired by the successful application of neural sequence-to-sequence models on machine translation (Sutskever et al., 2014) .",
                "cite_spans": [
                    {
                        "start": 167,
                        "end": 191,
                        "text": "(Sutskever et al., 2014)",
                        "ref_id": "BIBREF43"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Textual Dialogue Response Generation",
                "sec_num": "2.1"
            },
            {
                "text": "On top of the basic architecture (Shang et al., 2015; Vinyals and Le, 2015) , the vanilla encoderdecoder method is widely extended to address the critical challenges in open-domain dialogue systems, including improving the diversity of responses (Li et al., 2016a; Zhao et al., 2017; Tao et al., 2018) , modeling conversation contexts (Serban et al., 2016; Xing et al., 2017; Zhang et al., 2019; Zhao et al., 2020) , controlling attributes of responses (See et al., 2019; Zhou et al., 2018; Xu et al., 2019) , biasing responses to some specific personas (Li et al., 2016b; Zhang et al., 2018) , incorporating extra knowledge into generation (Dinan et al., 2019; Ghazvininejad et al., 2018; Kim et al., 2020; Li et al., 2020) , and building general pre-trained agents (Adiwardana et al., 2020; Zhang et al., 2020; Roller et al., 2021; Qi et al., 2021) .",
                "cite_spans": [
                    {
                        "start": 33,
                        "end": 53,
                        "text": "(Shang et al., 2015;",
                        "ref_id": "BIBREF42"
                    },
                    {
                        "start": 54,
                        "end": 75,
                        "text": "Vinyals and Le, 2015)",
                        "ref_id": "BIBREF48"
                    },
                    {
                        "start": 246,
                        "end": 264,
                        "text": "(Li et al., 2016a;",
                        "ref_id": null
                    },
                    {
                        "start": 265,
                        "end": 283,
                        "text": "Zhao et al., 2017;",
                        "ref_id": "BIBREF59"
                    },
                    {
                        "start": 284,
                        "end": 301,
                        "text": "Tao et al., 2018)",
                        "ref_id": "BIBREF52"
                    },
                    {
                        "start": 335,
                        "end": 356,
                        "text": "(Serban et al., 2016;",
                        "ref_id": "BIBREF41"
                    },
                    {
                        "start": 357,
                        "end": 375,
                        "text": "Xing et al., 2017;",
                        "ref_id": "BIBREF49"
                    },
                    {
                        "start": 376,
                        "end": 395,
                        "text": "Zhang et al., 2019;",
                        "ref_id": "BIBREF55"
                    },
                    {
                        "start": 396,
                        "end": 414,
                        "text": "Zhao et al., 2020)",
                        "ref_id": "BIBREF60"
                    },
                    {
                        "start": 453,
                        "end": 471,
                        "text": "(See et al., 2019;",
                        "ref_id": "BIBREF40"
                    },
                    {
                        "start": 472,
                        "end": 490,
                        "text": "Zhou et al., 2018;",
                        "ref_id": "BIBREF61"
                    },
                    {
                        "start": 491,
                        "end": 507,
                        "text": "Xu et al., 2019)",
                        "ref_id": "BIBREF50"
                    },
                    {
                        "start": 554,
                        "end": 572,
                        "text": "(Li et al., 2016b;",
                        "ref_id": null
                    },
                    {
                        "start": 573,
                        "end": 592,
                        "text": "Zhang et al., 2018)",
                        "ref_id": "BIBREF57"
                    },
                    {
                        "start": 641,
                        "end": 661,
                        "text": "(Dinan et al., 2019;",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 662,
                        "end": 689,
                        "text": "Ghazvininejad et al., 2018;",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 690,
                        "end": 707,
                        "text": "Kim et al., 2020;",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 708,
                        "end": 724,
                        "text": "Li et al., 2020)",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 767,
                        "end": 792,
                        "text": "(Adiwardana et al., 2020;",
                        "ref_id": null
                    },
                    {
                        "start": 793,
                        "end": 812,
                        "text": "Zhang et al., 2020;",
                        "ref_id": "BIBREF58"
                    },
                    {
                        "start": 813,
                        "end": 833,
                        "text": "Roller et al., 2021;",
                        "ref_id": "BIBREF39"
                    },
                    {
                        "start": 834,
                        "end": 850,
                        "text": "Qi et al., 2021)",
                        "ref_id": "BIBREF33"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Textual Dialogue Response Generation",
                "sec_num": "2.1"
            },
            {
                "text": "Different from the previous works on open-domain dialogue response generation that converse freely with plain text, our work lies in the research of multimodal response generation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Textual Dialogue Response Generation",
                "sec_num": "2.1"
            },
            {
                "text": "In the research of text-to-image generation, various works have been extensively studied. Mansimov et al. (2016) shown the Draw generative model (Gregor et al., 2015) could generate images from natural language descriptions. Reed et al. (2016) proposed a generative adversarial network to improve the image fidelity. Then some improvement methods continue to optimize the generation architecture, such as stacked generators (Zhang et al., 2017) , attentional network (Xu et al., 2018) , and extra knowledge (Li et al., 2019b) . Nguyen et al. (2017) provided a unified probabilistic interpretation of related activation maximization methods to produce high-quality images at higher resolutions. Separately, Cho et al. (2020) used uniform masking with a large range of masking ratios and align the suitable pre-training datasets to the proper objectives. More recently, Ramesh et al. ( 2021) and (Ding et al., 2021) adopt transformer-based methods which autoregressively model the text and image tokens as a single stream of data. For this multimodal response generation scenario, we use the textual image description to bridge above textual dialogue generation and text-to-image generation models, where the image description is the output of the former and input of the latter in a low-resource setting.",
                "cite_spans": [
                    {
                        "start": 90,
                        "end": 112,
                        "text": "Mansimov et al. (2016)",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 145,
                        "end": 166,
                        "text": "(Gregor et al., 2015)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 225,
                        "end": 243,
                        "text": "Reed et al. (2016)",
                        "ref_id": "BIBREF37"
                    },
                    {
                        "start": 424,
                        "end": 444,
                        "text": "(Zhang et al., 2017)",
                        "ref_id": "BIBREF56"
                    },
                    {
                        "start": 467,
                        "end": 484,
                        "text": "(Xu et al., 2018)",
                        "ref_id": "BIBREF52"
                    },
                    {
                        "start": 507,
                        "end": 525,
                        "text": "(Li et al., 2019b)",
                        "ref_id": null
                    },
                    {
                        "start": 528,
                        "end": 548,
                        "text": "Nguyen et al. (2017)",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 706,
                        "end": 723,
                        "text": "Cho et al. (2020)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 894,
                        "end": 913,
                        "text": "(Ding et al., 2021)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Text-to-Image Generation",
                "sec_num": "2.2"
            },
            {
                "text": "Suppose that we have dataset",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem Formalization",
                "sec_num": "3"
            },
            {
                "text": "D S = {(U i , R i )} n i=1",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem Formalization",
                "sec_num": "3"
            },
            {
                "text": ", where \u2200i \u2208 {1, . . . , n}, U i = {u i,1 , . . . , u i,n i } is the dialogue context with u i,j the j-th utterance, and R i is the response regarding to U i . u i,j and R i could contain two modalities: text, and image. The goal is to learn a generation model P (R|U ; \u03b8) (\u03b8 denotes the parameters of the model) with D S . Thus, given a new dialogue context U , one can generate a multimodal response R following P (R|U ; \u03b8).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem Formalization",
                "sec_num": "3"
            },
            {
                "text": "This section first formulates the unified tokenization method for multimodal dialogues. We then introduce the two important components in our proposed multimodal dialogue response generation model (Divter) under low-resource scenario, including (i) textual dialogue response generator; (ii) text-to-image translator. Figure 2 shows the overall of our Divter.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 324,
                        "end": 325,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Approach",
                "sec_num": "4"
            },
            {
                "text": "To learn a multimodal generation model, we should first model the unified representations of both text and image. Inspired by the success of DALLE (Esser et al., 2020) and VQGAN (Ramesh et al., 2021) , to utilize the highly expressive transformer architecture for text-to-image generation, we need to express an image in the form of a sequence, similar to what we usually do for pure text tokenization.",
                "cite_spans": [
                    {
                        "start": 147,
                        "end": 167,
                        "text": "(Esser et al., 2020)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 178,
                        "end": 199,
                        "text": "(Ramesh et al., 2021)",
                        "ref_id": "BIBREF36"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Multimodal Tokenization",
                "sec_num": "4.1"
            },
            {
                "text": "The tokenization for text is already well-studied, e.g., BPE (Gage, 1994) . This work uses 50257 BPE-encoded tokens and distributed embedding of Transformer architecture (Vaswani et al., 2017b) to model the texts in a dialogue.",
                "cite_spans": [
                    {
                        "start": 61,
                        "end": 73,
                        "text": "(Gage, 1994)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 170,
                        "end": 193,
                        "text": "(Vaswani et al., 2017b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Text Tokenization",
                "sec_num": "4.1.1"
            },
            {
                "text": "The tokenizer for image is a discrete Auto-Encoder (VQGAN1 ) V as shown in Figure 2 . V uses an encoder V E to compress each image r v of shape H \u00d7 W \u00d7 3 into \u1e91 of shape h \u00d7 w \u00d7 d z , then each vector of dimension d z would be quantized to its closest embedding z k in a learned, discrete codebook Z = {z k } K k=1 \u2208 R dz under the action of element-wise quantization q(\u22c5)",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 82,
                        "end": 83,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Image Tokenization",
                "sec_num": "4.1.2"
            },
            {
                "text": "z q = q(\u1e91) \u2236= (arg min z k \u2208Z \u2225\u1e91 ij -z k \u2225) \u2208 R h\u00d7w\u00d7dz (1)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Image Tokenization",
                "sec_num": "4.1.2"
            },
            {
                "text": "Thus r v can be represented by a spatial collection of codebook entries z q \u2208 R h\u00d7w\u00d7dz . The decoder V D maps the z q back to a image rv to reconstruct the input. In this work, H = W = 256, h = w = 16, K = 16384, d z = 256. The learning details of V and Z could be found in Ramesh et al. (2021) .",
                "cite_spans": [
                    {
                        "start": 274,
                        "end": 294,
                        "text": "Ramesh et al. (2021)",
                        "ref_id": "BIBREF36"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Image Tokenization",
                "sec_num": "4.1.2"
            },
            {
                "text": "Learning an effective multimodal generation model with a single sequence-to-sequence model often requires a large number of training instances. However, only very few multimodal dialogues are available due to the privacy restrictions on social media and the expensive human effort. On the other hand, as shown in Figure 3 , there existed a large number of open source text-only dialogues (e.g. Reddit comments2 , formulated as",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 320,
                        "end": 321,
                        "text": "3",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Low-resource Learning Model",
                "sec_num": "4.2"
            },
            {
                "text": "D C = {(U i , r e i )} N i=1",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Low-resource Learning Model",
                "sec_num": "4.2"
            },
            {
                "text": "with (U i , r e i ) a <text dialogue context, text re-sponse> pair) , and a large number of <image description, image> pairs (e.g. YFCC100M (Thomee Figure 2 illustrates the architecture of our model. The model is made up of two components: a textual dialogue response generator G and a text-to-image translator F. In the rest of this section, we will elaborate these two modules in detail.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 155,
                        "end": 156,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Low-resource Learning Model",
                "sec_num": "4.2"
            },
            {
                "text": "The textual dialogue response generator G is a sequence-to-sequence model based on the Trans-former architecture (Vaswani et al., 2017b) , it consists of a 24-layers Transformer with a hidden size of 1024 and 16 heads. Specifically, given a text dialogue context U = {u 1 , . . . , u l } from DS as source, and the target is a text R = {w 1 , \u22ef, [SEP], [DST], \u22ef, [SEP], \u22ef, w T } with w t the t-th word, the [DST] token means the following subsequence is a textual image description c. The generation loss is defined by",
                "cite_spans": [
                    {
                        "start": 113,
                        "end": 136,
                        "text": "(Vaswani et al., 2017b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Textual Dialogue Response Generator",
                "sec_num": "4.2.1"
            },
            {
                "text": "L G = E (U, R)\u223c DS [-log p( R)]",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Textual Dialogue Response Generator",
                "sec_num": "4.2.1"
            },
            {
                "text": "(2)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Textual Dialogue Response Generator",
                "sec_num": "4.2.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "p( R) = \u220f t p(w t |U, w 1\u2236t-1 )",
                        "eq_num": "(3)"
                    }
                ],
                "section": "Textual Dialogue Response Generator",
                "sec_num": "4.2.1"
            },
            {
                "text": "Inference Given a new text dialogue context U , when a generated image description c occurs, it will be fed into the following text-to-image translator, then constructed to the codebook embeddings of its synonymous image.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Textual Dialogue Response Generator",
                "sec_num": "4.2.1"
            },
            {
                "text": "The text-to-image translator F is also a sequenceto-sequence generation model based on the Transformer architecture, it consists of 24-layers Transformer with a hidden size of 1024 and 16 attention heads. Given an image r v \u2208 R H\u00d7W \u00d73 and its textual description c = {w 1 , \u22ef, w T } from DS , with the V E and Z available, we can represent r v in terms of the codebook indices of its encodings. More precisely, the quantized encoding of image r v is given by z q = q(V E (r v )) \u2208 R h\u00d7w\u00d7dz , and could be transferred to a sequence s \u2208 {0, \u22ef, |Z| -1} h\u00d7w of indices from the codebook Z, which is obtained by replacing each code with its index in the codebook Z",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Text-to-Image Translator",
                "sec_num": "4.2.2"
            },
            {
                "text": "s i,j = k such that (z q ) i,j = z k (4)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Text-to-Image Translator",
                "sec_num": "4.2.2"
            },
            {
                "text": "Then we concatenate tokenized c and s to a single stream of tokens",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Text-to-Image Translator",
                "sec_num": "4.2.2"
            },
            {
                "text": "x = {w 1 , \u22ef, w T , [SEP], s 1 , \u22ef, s h\u00d7w } (5)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Text-to-Image Translator",
                "sec_num": "4.2.2"
            },
            {
                "text": "and train an autoregressive transformer to model the joint distribution over the text and image tokens, the generation loss is defined by",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Text-to-Image Translator",
                "sec_num": "4.2.2"
            },
            {
                "text": "L F = E (c,r v )\u223c DS [-log p(x)] (6) p(x) = \u220f t p(w t |w 1\u2236t-1 ) \u220f i p(s i |c, s 1\u2236i-1 ) (7)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Text-to-Image Translator",
                "sec_num": "4.2.2"
            },
            {
                "text": "Inference Given a description c, we leverage the text-to-image translator to generate the representations \u1e91 = F(c) \u2208 R h\u00d7w\u00d7dz of its synonymous image.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Text-to-Image Translator",
                "sec_num": "4.2.2"
            },
            {
                "text": "Let us denote {\u03b8 g , \u03b8 \u03c0 , \u03b8 \u03d5 } as the parameters of textual dialogue response generator G, image tokenizer V and text-to-image translator F. In the pre-training stage, we use textual dialogues D C to estimate \u03b8 g , use the ImageNet (Deng et al., 2009) to estimate \u03b8 \u03c0 , use <image description, image> pairs D P to estimate \u03b8 \u03d5 . Then we fix \u03b8 \u03c0 , and jointly finetune \u03b8 g and \u03b8 \u03d5 with DS , thus the final objective is to minimize the integrated loss",
                "cite_spans": [
                    {
                        "start": 234,
                        "end": 253,
                        "text": "(Deng et al., 2009)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Details",
                "sec_num": "4.2.3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "L = L G + \u03bbL F (",
                        "eq_num": "8"
                    }
                ],
                "section": "Learning Details",
                "sec_num": "4.2.3"
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Details",
                "sec_num": "4.2.3"
            },
            {
                "text": "where \u03bb is a hyper parameter.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Details",
                "sec_num": "4.2.3"
            },
            {
                "text": "Remarks. In this work, we mainly focus on integrating text and image responses generation, but our proposed approach actually provides a recipe for a general solution to low-resource MDRG in which the target modality could be gifs, videos, or speech sounds, etc. To do that, one only needs to modify the text-to-image translator to make it compatible with the specific modality type, then pre-train a new text-to-<target modality> translator.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Details",
                "sec_num": "4.2.3"
            },
            {
                "text": "To evaluate the performance of Divter, we conduct comprehensive experiments on the PhotoChat dataset released by Zang et al. (2021) , which is a multimodal conversational dataset consisting of 10917 images and 12286 dialogues, each of which is paired with a user image that is shared during the conversation, and each image is paired with its text description. The dataset has been split into 10286 train, 1000 dev, and 1000 test instances. More details are described in Appendix A.1.",
                "cite_spans": [
                    {
                        "start": 113,
                        "end": 131,
                        "text": "Zang et al. (2021)",
                        "ref_id": "BIBREF54"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Dataset",
                "sec_num": "5.1"
            },
            {
                "text": "We conduct evaluation with both automatic metrics and human judgements. For automatic evaluation, we focus on four aspects: (1) Image Intent Prediction, the goal of this task is to predict whether a image should be produced in the next turn for given context; (2) Text Description Generation;",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation Metrics",
                "sec_num": "5.2"
            },
            {
                "text": "(3) Image Generation Quality ; (4) Text Response Generation. For (1), we follow Zang et al. (2021) , which formulates the problem as a binary classification task, and use F1 as metric; for (2) and ( 4), we use PPL, BLEU (Papineni et al., 2002) , Rouge (Lin, 2004) For human evaluation, we randomly sample 200 dialogue contexts and generate responses from Pho-toChat for Divter and baselines. Three human annotators are asked to score the response quality on a scale of {0, 1, 2} from four aspects: (1) Context Coherence: Whether the text response is coherent with the context; (2) Text Fluency: Whether the text response is natural and fluent; (3) Image Quality: The quality (including definition and integrity) of the image response; (4) Background Consistency of Image: For each dialogue, We select the top-8 generated/retrieved images group and ask the annotators to decide whether the group is consistent with the dialogue background, a qualitative assessment is also shown in Figure 5 . We report the average scores over three annotators, and the higher score means the better.",
                "cite_spans": [
                    {
                        "start": 80,
                        "end": 98,
                        "text": "Zang et al. (2021)",
                        "ref_id": "BIBREF54"
                    },
                    {
                        "start": 220,
                        "end": 243,
                        "text": "(Papineni et al., 2002)",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 252,
                        "end": 263,
                        "text": "(Lin, 2004)",
                        "ref_id": "BIBREF27"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 988,
                        "end": 989,
                        "text": "5",
                        "ref_id": "FIGREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Evaluation Metrics",
                "sec_num": "5.2"
            },
            {
                "text": "We also compare both pure text Divter and multimodal Divter with DialoGPT, respectively. The \"pure text Divter\" means we block the [DST] token in the vocabulary in the decoding stage, so that the responses would only contain texts. We also randomly sample 200 dialogues. To each annotator, two responses from different models are presented, which are randomly shuffled to hide their sources. The annotators then judge which response is more effective in improving the dialogue experience and attractiveness. The agreement among the annotators is measured by Fleiss' Kappa (Fleiss, 1971) .",
                "cite_spans": [
                    {
                        "start": 572,
                        "end": 586,
                        "text": "(Fleiss, 1971)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation Metrics",
                "sec_num": "5.2"
            },
            {
                "text": "For the textual dialogue response generator G, we use DialoGPT (Zhang et al., 2020) as pre-trained model initialization, trained on 147M conversationlike exchanges extracted from Reddit comment chains over a period spanning from 2005 through 2017. In the fine-tuning stage, we concatenate the context turns with the token [SEP] as a single sequence, we adopt Adam optimizer as an initial learning rate of 1e-5, and the batch size is 256, the training of PhotoChat is conducted on 16 Nvidia Tesla V100 32G GPU cards. We use beam search(size=5) to decode the text sequence.",
                "cite_spans": [
                    {
                        "start": 63,
                        "end": 83,
                        "text": "(Zhang et al., 2020)",
                        "ref_id": "BIBREF58"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementation Details",
                "sec_num": "5.3"
            },
            {
                "text": "For the image tokenizer V, we inherit the model released by Ramesh et al. (2021) .",
                "cite_spans": [
                    {
                        "start": 60,
                        "end": 80,
                        "text": "Ramesh et al. (2021)",
                        "ref_id": "BIBREF36"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementation Details",
                "sec_num": "5.3"
            },
            {
                "text": "For the text-to-image translator F, we randomly select 5M <categorical image description, image> pairs from ImageNet, and <image description, im-age> pairs from YFCC100M (Thomee et al., 2016) as training data. We set the maximum image description length as 32, then pre-train F for 3.5 million steps with a batch size of 256 accumulated on 16 Nvidia Tesla V100 32G GPUs. In the finetuning stage, we train PhotoChat for 50000 steps. In the inference stage, we use CLIP (Radford et al., 2021) to rerank the generated 256 samples.",
                "cite_spans": [
                    {
                        "start": 170,
                        "end": 191,
                        "text": "(Thomee et al., 2016)",
                        "ref_id": "BIBREF45"
                    },
                    {
                        "start": 468,
                        "end": 490,
                        "text": "(Radford et al., 2021)",
                        "ref_id": "BIBREF34"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementation Details",
                "sec_num": "5.3"
            },
            {
                "text": "In the joint learning, we first train F for 48000 steps, then jointly train G and F for 2000 steps. The \u03bb in Eq.8 is 0.2. Early stopping on validation is adopted as a regularization strategy. All the hyper parameters are determined by grid search. More details are described in Appendix A.3.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementation Details",
                "sec_num": "5.3"
            },
            {
                "text": "We implement the image Auto-Encoder using the code https://github.com/CompVis/ taming-transformers, implement the Textual Dialogue Response Generator using the code https://github.com/microsoft/ DialoGPT, and implement the Text-to-Image Translator using the code https://github. com/lucidrains/DALLE-pytorch.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementation Details",
                "sec_num": "5.3"
            },
            {
                "text": "Two pre-trained models BERT-base (Devlin et al., 2019) and T5-3B (Raffel et al., 2020) are selected as baselines to measure the \"Image Intent Prediction\" task in Section 5.2. They takes the text dialogue context as input, and predict \"whether a image will be shared in the next turn\". SCAN is proposed by Lee et al. (2018) , the model captures interplay between image regions and text tokens to infer image-text similarity, SCAN achieves state-of-the-art performance of the \"Image Retrieval\" task on PhotoChat. S2S-TF is a single sequence-to-sequence model with 24-layers Transformer, we only use PhotoChat to train this multimodal generation model.",
                "cite_spans": [
                    {
                        "start": 33,
                        "end": 54,
                        "text": "(Devlin et al., 2019)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 65,
                        "end": 86,
                        "text": "(Raffel et al., 2020)",
                        "ref_id": "BIBREF35"
                    },
                    {
                        "start": 305,
                        "end": 322,
                        "text": "Lee et al. (2018)",
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baselines",
                "sec_num": "5.4"
            },
            {
                "text": "As shown in Table 1 , our Divter achieves not only comparable performance with the state-of-the-art retrieval-based image response intent prediction model but also achieves remarkable performance in all the generation metrics. This indicates that Divter can accurately judge the timing of generating image response with the given dialogue context, and produce text responses that are coherent to the context, and generate high-quality image responses. The significant performance gap between Divter and the baseline models (e.g. S2S-TF, Divter variants) without pre-training indicates the superiority of our proposed learning strategy. Table 2 reports the results of human evaluation, our Divter also significantly outperforms the baselines on most of the aspects. The comparison results shown in Table 3 indicates (1): out Divter can achieve comparable performance on pure text response generation with DialoGPT; (2): the multimodal responses generated by Divter achieve a significant improvement on the dialogue experience and attractiveness in contrast to pure text dialogue model (DialoGPT).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 18,
                        "end": 19,
                        "text": "1",
                        "ref_id": "TABREF0"
                    },
                    {
                        "start": 642,
                        "end": 643,
                        "text": "2",
                        "ref_id": "TABREF1"
                    },
                    {
                        "start": 803,
                        "end": 804,
                        "text": "3",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Evaluation Results",
                "sec_num": "5.5"
            },
            {
                "text": "We conduct extensive ablation experiments over different variants to better understand their relative importance to the MDRG task. As shown in Table 1, all the variants lead to worse performance ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Ablation Study",
                "sec_num": "5.6"
            },
            {
                "text": "To further investigate the quality of multimodal responses generated by Divter, we show two examples on the PhotoChat test data in Table 4 . The given context of the first one is about \"ice cream\", and the second one is about \"honey bee\". As we can see, Divter can not only generate a realistic high-resolution image which is coherent to the background, but also generate the informative text responses grounded on the image. Separately, The high-quality generated images are comparable to those real-world ground truths, which demonstrates the practicability of Divter.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 137,
                        "end": 138,
                        "text": "4",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Case Study",
                "sec_num": "5.7"
            },
            {
                "text": "Benefits over retrieval-based methods To further investigate and compare the generalization capability between Divter and the retrieval-based method, we also get top-10 generated images from Divter and equivalent retrieved images from SCAN model given the same context. As shown in Figure 5 , on the one hand, the diversity and richness of the generated images are desirable, on the other hand, those retrieved results often suffer from wrong consistency with dialogue background. For example in the second case, the dialogue is talking about \"coffee\", but the retrieved images contain some uncorrelated objects like \"milk\", \"cake\", \"dog' and \"snack\". And in the third example, all the retrieval results are mistaken since there is little \"curtain\" in the training and retrieval space. This demonstrates the fact that the performance of retrieval-based method is extremely limited in specific domains by the size of the pre-constructed conversational history repository, especially in the low-resource scenario. Furthermore, our proposed generation based method shows better generalization capability to tackle the low-resource challenge.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 289,
                        "end": 290,
                        "text": "5",
                        "ref_id": "FIGREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Discussions",
                "sec_num": "5.8"
            },
            {
                "text": "In this paper, we explore multimodal dialogue response generation under a low-resource setting. To overcome the challenges from the new task and insufficient training data, we propose Divter, a neural conversational agent which incorporates text-toimage generation into text-only dialogue response generation, in which most parameters do not rely on the training data any more and can be estimated from large scale textual open domain dialogues and <image description, image> pairs. Extensive experiments demonstrate Divter achieves state-of-the-art results in automatic and human evaluation. In the future, we will explore more efficient methods to inject more modalities into response generation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "6"
            },
            {
                "text": "https://github.com/CompVis/ taming-transformers",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "https://files.pushshift.io/reddit/",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "We thank anonymous reviewers for their insightful suggestions to improve this paper.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgement",
                "sec_num": null
            },
            {
                "text": "A.1 Dataset ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Appendix",
                "sec_num": null
            },
            {
                "text": "The CLIP model assigns a score based on how well the image matches the description, we use CLIP to rerank the generated 256 samples, and select the best image as the final response. To obtain high-quality training set, we discard the instances with the prefix of \"The photo has your * #\" in descriptions, \"*\" includes \"mom\", \"dad\", \"daughter\", \"sister\", \"uncle\", etc. \"#\" is name of a person. To build the training set for text-to-image translator F from ImageNet, we combine the text \"Objects in the photo:\" and textual categorical name of each image to build the <categorical image description, image> pair. To train the baseline S2S-TF model, we also use the image tokenizer V to tokenize each image, and combine the image tokens with text tokens to form a single stream as the generation source/target.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A.3 More Implementation Details",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF1": {
                "ref_id": "b1",
                "title": "History for visual dialog: Do we really need it?",
                "authors": [
                    {
                        "first": "Shubham",
                        "middle": [],
                        "last": "Agarwal",
                        "suffix": ""
                    },
                    {
                        "first": "Trung",
                        "middle": [],
                        "last": "Bui",
                        "suffix": ""
                    },
                    {
                        "first": "Joon-Young",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Ioannis",
                        "middle": [],
                        "last": "Konstas",
                        "suffix": ""
                    },
                    {
                        "first": "Verena",
                        "middle": [],
                        "last": "Rieser",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "8182--8197",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.acl-main.728"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Shubham Agarwal, Trung Bui, Joon-Young Lee, Ioan- nis Konstas, and Verena Rieser. 2020. History for visual dialog: Do we really need it? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8182-8197, On- line. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Spice: Semantic propositional image caption evaluation",
                "authors": [
                    {
                        "first": "Peter",
                        "middle": [],
                        "last": "Anderson",
                        "suffix": ""
                    },
                    {
                        "first": "Basura",
                        "middle": [],
                        "last": "Fernando",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Johnson",
                        "suffix": ""
                    },
                    {
                        "first": "Stephen",
                        "middle": [],
                        "last": "Gould",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "ECCV",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. 2016. Spice: Semantic propositional image caption evaluation. In ECCV.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Bottom-up and top-down attention for image captioning and visual question answering",
                "authors": [
                    {
                        "first": "Peter",
                        "middle": [],
                        "last": "Anderson",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaodong",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Buehler",
                        "suffix": ""
                    },
                    {
                        "first": "Damien",
                        "middle": [],
                        "last": "Teney",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Johnson",
                        "suffix": ""
                    },
                    {
                        "first": "Stephen",
                        "middle": [],
                        "last": "Gould",
                        "suffix": ""
                    },
                    {
                        "first": "Lei",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "CVPR",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. 2018. Bottom-up and top-down attention for image captioning and visual question answering. In CVPR.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Multimodal incremental transformer with visual grounding for visual dialogue generation",
                "authors": [
                    {
                        "first": "Feilong",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Fandong",
                        "middle": [],
                        "last": "Meng",
                        "suffix": ""
                    },
                    {
                        "first": "Xiuyi",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Peng",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Jie",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
                "volume": "",
                "issue": "",
                "pages": "436--446",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2021.findings-acl.38"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Feilong Chen, Fandong Meng, Xiuyi Chen, Peng Li, and Jie Zhou. 2021. Multimodal incremental transformer with visual grounding for visual dialogue generation. In Findings of the Association for Computational Lin- guistics: ACL-IJCNLP 2021, pages 436-446, Online. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "X-lxmert: Paint, caption and answer questions with multi-modal transformers",
                "authors": [
                    {
                        "first": "Jaemin",
                        "middle": [],
                        "last": "Cho",
                        "suffix": ""
                    },
                    {
                        "first": "Jiasen",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    },
                    {
                        "first": "Dustin",
                        "middle": [],
                        "last": "Schwenk",
                        "suffix": ""
                    },
                    {
                        "first": "Hannaneh",
                        "middle": [],
                        "last": "Hajishirzi",
                        "suffix": ""
                    },
                    {
                        "first": "Aniruddha",
                        "middle": [],
                        "last": "Kembhavi",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jaemin Cho, Jiasen Lu, Dustin Schwenk, Hannaneh Ha- jishirzi, and Aniruddha Kembhavi. 2020. X-lxmert: Paint, caption and answer questions with multi-modal transformers. In EMNLP.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Meshed-Memory Transformer for Image Captioning",
                "authors": [
                    {
                        "first": "Marcella",
                        "middle": [],
                        "last": "Cornia",
                        "suffix": ""
                    },
                    {
                        "first": "Matteo",
                        "middle": [],
                        "last": "Stefanini",
                        "suffix": ""
                    },
                    {
                        "first": "Lorenzo",
                        "middle": [],
                        "last": "Baraldi",
                        "suffix": ""
                    },
                    {
                        "first": "Rita",
                        "middle": [],
                        "last": "Cucchiara",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Marcella Cornia, Matteo Stefanini, Lorenzo Baraldi, and Rita Cucchiara. 2020. Meshed-Memory Trans- former for Image Captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pat- tern Recognition.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Visual dialog",
                "authors": [
                    {
                        "first": "Abhishek",
                        "middle": [],
                        "last": "Das",
                        "suffix": ""
                    },
                    {
                        "first": "Satwik",
                        "middle": [],
                        "last": "Kottur",
                        "suffix": ""
                    },
                    {
                        "first": "Khushi",
                        "middle": [],
                        "last": "Gupta",
                        "suffix": ""
                    },
                    {
                        "first": "Avi",
                        "middle": [],
                        "last": "Singh",
                        "suffix": ""
                    },
                    {
                        "first": "Deshraj",
                        "middle": [],
                        "last": "Yadav",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [
                            "F"
                        ],
                        "last": "Jos\u00e9",
                        "suffix": ""
                    },
                    {
                        "first": "Devi",
                        "middle": [],
                        "last": "Moura",
                        "suffix": ""
                    },
                    {
                        "first": "Dhruv",
                        "middle": [],
                        "last": "Parikh",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Batra",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "326--335",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jos\u00e9 M. F. Moura, Devi Parikh, and Dhruv Batra. 2017. Visual dialog. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, page 326-335.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Imagenet: A large-scale hierarchical image database",
                "authors": [
                    {
                        "first": "Jia",
                        "middle": [],
                        "last": "Deng",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Dong",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Li-Jia",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Li",
                        "middle": [],
                        "last": "Fei-Fei",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "2009 IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "248--255",
                "other_ids": {
                    "DOI": [
                        "10.1109/CVPR.2009.5206848"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hier- archical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248-255.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
                "authors": [
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Devlin",
                        "suffix": ""
                    },
                    {
                        "first": "Ming-Wei",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Kenton",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Kristina",
                        "middle": [],
                        "last": "Toutanova",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "1",
                "issue": "",
                "pages": "4171--4186",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/N19-1423"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Wizard of wikipedia: Knowledge-powered conversational agents",
                "authors": [
                    {
                        "first": "Emily",
                        "middle": [],
                        "last": "Dinan",
                        "suffix": ""
                    },
                    {
                        "first": "Stephen",
                        "middle": [],
                        "last": "Roller",
                        "suffix": ""
                    },
                    {
                        "first": "Kurt",
                        "middle": [],
                        "last": "Shuster",
                        "suffix": ""
                    },
                    {
                        "first": "Angela",
                        "middle": [],
                        "last": "Fan",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Auli",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Weston",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "International Conference on Learning Representations",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. 2019. Wizard of wikipedia: Knowledge-powered conversational agents. In International Conference on Learning Representations.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Cogview: Mastering text-to-image generation via transformers",
                "authors": [
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Ding",
                        "suffix": ""
                    },
                    {
                        "first": "Zhuoyi",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Wenyi",
                        "middle": [],
                        "last": "Hong",
                        "suffix": ""
                    },
                    {
                        "first": "Wendi",
                        "middle": [],
                        "last": "Zheng",
                        "suffix": ""
                    },
                    {
                        "first": "Chang",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Da",
                        "middle": [],
                        "last": "Yin",
                        "suffix": ""
                    },
                    {
                        "first": "Junyang",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Xu",
                        "middle": [],
                        "last": "Zou",
                        "suffix": ""
                    },
                    {
                        "first": "Zhou",
                        "middle": [],
                        "last": "Shao",
                        "suffix": ""
                    },
                    {
                        "first": "Hongxia",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Jie",
                        "middle": [],
                        "last": "Tang",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, and Jie Tang. 2021. Cogview: Mastering text-to-image generation via transformers.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Taming transformers for high-resolution image synthesis",
                "authors": [
                    {
                        "first": "Patrick",
                        "middle": [],
                        "last": "Esser",
                        "suffix": ""
                    },
                    {
                        "first": "Robin",
                        "middle": [],
                        "last": "Rombach",
                        "suffix": ""
                    },
                    {
                        "first": "Bj\u00f6rn",
                        "middle": [],
                        "last": "Ommer",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Patrick Esser, Robin Rombach, and Bj\u00f6rn Ommer. 2020. Taming transformers for high-resolution image syn- thesis.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Measuring nominal scale agreement among many raters",
                "authors": [
                    {
                        "first": "Joseph",
                        "middle": [
                            "L"
                        ],
                        "last": "Fleiss",
                        "suffix": ""
                    }
                ],
                "year": 1971,
                "venue": "Psychological Bulletin",
                "volume": "76",
                "issue": "",
                "pages": "378--382",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Joseph L. Fleiss. 1971. Measuring nominal scale agree- ment among many raters. Psychological Bulletin, 76:378-382.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "A new algorithm for data compression",
                "authors": [
                    {
                        "first": "Philip",
                        "middle": [],
                        "last": "Gage",
                        "suffix": ""
                    }
                ],
                "year": 1994,
                "venue": "The C Users Journal archive",
                "volume": "12",
                "issue": "",
                "pages": "23--38",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Philip Gage. 1994. A new algorithm for data compres- sion. The C Users Journal archive, 12:23-38.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "What goes into a word: generating image descriptions with top-down spatial knowledge",
                "authors": [
                    {
                        "first": "Mehdi",
                        "middle": [],
                        "last": "Ghanimifard",
                        "suffix": ""
                    },
                    {
                        "first": "Simon",
                        "middle": [],
                        "last": "Dobnik",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 12th International Conference on Natural Language Generation",
                "volume": "",
                "issue": "",
                "pages": "540--551",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/W19-8668"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Mehdi Ghanimifard and Simon Dobnik. 2019. What goes into a word: generating image descriptions with top-down spatial knowledge. In Proceedings of the 12th International Conference on Natural Language Generation, pages 540-551, Tokyo, Japan. Associa- tion for Computational Linguistics.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "A knowledge-grounded neural conversation model",
                "authors": [
                    {
                        "first": "Marjan",
                        "middle": [],
                        "last": "Ghazvininejad",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Brockett",
                        "suffix": ""
                    },
                    {
                        "first": "Ming-Wei",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Bill",
                        "middle": [],
                        "last": "Dolan",
                        "suffix": ""
                    },
                    {
                        "first": "Jianfeng",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Wen-Tau",
                        "middle": [],
                        "last": "Yih",
                        "suffix": ""
                    },
                    {
                        "first": "Michel",
                        "middle": [],
                        "last": "Galley",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wen-tau Yih, and Michel Galley. 2018. A knowledge-grounded neu- ral conversation model. Proceedings of the AAAI Conference on Artificial Intelligence, 32.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Draw: A recurrent neural network for image generation",
                "authors": [
                    {
                        "first": "Karol",
                        "middle": [],
                        "last": "Gregor",
                        "suffix": ""
                    },
                    {
                        "first": "Ivo",
                        "middle": [],
                        "last": "Danihelka",
                        "suffix": ""
                    },
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Graves",
                        "suffix": ""
                    },
                    {
                        "first": "Danilo",
                        "middle": [],
                        "last": "Rezende",
                        "suffix": ""
                    },
                    {
                        "first": "Daan",
                        "middle": [],
                        "last": "Wierstra",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 32nd International Conference on Machine Learning",
                "volume": "37",
                "issue": "",
                "pages": "1462--1471",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Rezende, and Daan Wierstra. 2015. Draw: A re- current neural network for image generation. In Pro- ceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Ma- chine Learning Research, pages 1462-1471, Lille, France. PMLR.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Aligned dual channel graph convolutional network for visual question answering",
                "authors": [
                    {
                        "first": "Qingbao",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Jielong",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Yi",
                        "middle": [],
                        "last": "Cai",
                        "suffix": ""
                    },
                    {
                        "first": "Changmeng",
                        "middle": [],
                        "last": "Zheng",
                        "suffix": ""
                    },
                    {
                        "first": "Junying",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Ho-Fung",
                        "middle": [],
                        "last": "Leung",
                        "suffix": ""
                    },
                    {
                        "first": "Qing",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "7166--7176",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.acl-main.642"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Qingbao Huang, Jielong Wei, Yi Cai, Changmeng Zheng, Junying Chen, Ho-fung Leung, and Qing Li. 2020. Aligned dual channel graph convolutional net- work for visual question answering. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7166-7176, On- line. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Sequential latent knowledge selection for knowledge-grounded dialogue",
                "authors": [
                    {
                        "first": "Byeongchang",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Jaewoo",
                        "middle": [],
                        "last": "Ahn",
                        "suffix": ""
                    },
                    {
                        "first": "Gunhee",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "International Conference on Learning Representations",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Byeongchang Kim, Jaewoo Ahn, and Gunhee Kim. 2020. Sequential latent knowledge selection for knowledge-grounded dialogue. In International Con- ference on Learning Representations.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Gang Hua, Houdong Hu, and Xiaodong He",
                "authors": [
                    {
                        "first": "Kuang-Huei",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Xi",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Stacked cross attention for image-text matching",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1803.08024"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Kuang-Huei Lee, Xi Chen, Gang Hua, Houdong Hu, and Xiaodong He. 2018. Stacked cross at- tention for image-text matching. arXiv preprint arXiv:1803.08024.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "2016a. A diversity-promoting objective function for neural conversation models",
                "authors": [
                    {
                        "first": "Jiwei",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Michel",
                        "middle": [],
                        "last": "Galley",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Brockett",
                        "suffix": ""
                    },
                    {
                        "first": "Jianfeng",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Bill",
                        "middle": [],
                        "last": "Dolan",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "",
                "issue": "",
                "pages": "110--119",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/N16-1014"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. 2016a. A diversity-promoting ob- jective function for neural conversation models. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, pages 110-119, San Diego, California. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Georgios Spithourakis, Jianfeng Gao, and Bill Dolan. 2016b. A persona-based neural conversation model",
                "authors": [
                    {
                        "first": "Jiwei",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Michel",
                        "middle": [],
                        "last": "Galley",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Brockett",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics",
                "volume": "1",
                "issue": "",
                "pages": "994--1003",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/P16-1094"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jiwei Li, Michel Galley, Chris Brockett, Georgios Sp- ithourakis, Jianfeng Gao, and Bill Dolan. 2016b. A persona-based neural conversation model. In Pro- ceedings of the 54th Annual Meeting of the Associa- tion for Computational Linguistics (Volume 1: Long Papers), pages 994-1003, Berlin, Germany. Associa- tion for Computational Linguistics.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "2019a. Relation-aware graph attention network for visual question answering",
                "authors": [
                    {
                        "first": "Linjie",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Zhe",
                        "middle": [],
                        "last": "Gan",
                        "suffix": ""
                    },
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Cheng",
                        "suffix": ""
                    },
                    {
                        "first": "Jingjing",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Linjie Li, Zhe Gan, Yu Cheng, and Jingjing Liu. 2019a. Relation-aware graph attention network for visual question answering. ICCV.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Zero-resource knowledge-grounded dialogue generation",
                "authors": [
                    {
                        "first": "Linxiao",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Can",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Yufan",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Xueliang",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Chongyang",
                        "middle": [],
                        "last": "Tao",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2008.12918"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Linxiao Li, Can Xu, Wei Wu, Yufan Zhao, Xueliang Zhao, and Chongyang Tao. 2020. Zero-resource knowledge-grounded dialogue generation. arXiv preprint arXiv:2008.12918.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Object-driven text-to-image synthesis via adversarial training",
                "authors": [
                    {
                        "first": "Wenbo",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Pengchuan",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Lei",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Qiuyuan",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaodong",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Siwei",
                        "middle": [],
                        "last": "Lyu",
                        "suffix": ""
                    },
                    {
                        "first": "Jianfeng",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wenbo Li, Pengchuan Zhang, Lei Zhang, Qiuyuan Huang, Xiaodong He, Siwei Lyu, and Jianfeng Gao. 2019b. Object-driven text-to-image synthesis via adversarial training.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Maria: A visual experience powered conversational agent",
                "authors": [
                    {
                        "first": "Zujie",
                        "middle": [],
                        "last": "Liang",
                        "suffix": ""
                    },
                    {
                        "first": "Huang",
                        "middle": [],
                        "last": "Hu",
                        "suffix": ""
                    },
                    {
                        "first": "Can",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Chongyang",
                        "middle": [],
                        "last": "Tao",
                        "suffix": ""
                    },
                    {
                        "first": "Xiubo",
                        "middle": [],
                        "last": "Geng",
                        "suffix": ""
                    },
                    {
                        "first": "Yining",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Fan",
                        "middle": [],
                        "last": "Liang",
                        "suffix": ""
                    },
                    {
                        "first": "Daxin",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
                "volume": "1",
                "issue": "",
                "pages": "5596--5611",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2021.acl-long.435"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Zujie Liang, Huang Hu, Can Xu, Chongyang Tao, Xi- ubo Geng, Yining Chen, Fan Liang, and Daxin Jiang. 2021. Maria: A visual experience powered conver- sational agent. In Proceedings of the 59th Annual Meeting of the Association for Computational Lin- guistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 5596-5611, Online. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "ROUGE: A package for automatic evaluation of summaries",
                "authors": [
                    {
                        "first": "Chin-Yew",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Text Summarization Branches Out",
                "volume": "",
                "issue": "",
                "pages": "74--81",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chin-Yew Lin. 2004. ROUGE: A package for auto- matic evaluation of summaries. In Text Summariza- tion Branches Out, pages 74-81, Barcelona, Spain. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Hierarchical question-image co-attention for visual question answering",
                "authors": [
                    {
                        "first": "Jiasen",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    },
                    {
                        "first": "Jianwei",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Dhruv",
                        "middle": [],
                        "last": "Batra",
                        "suffix": ""
                    },
                    {
                        "first": "Devi",
                        "middle": [],
                        "last": "Parikh",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "NIPS",
                "volume": "",
                "issue": "",
                "pages": "289--297",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. 2016. Hierarchical question-image co-attention for visual question answering. In NIPS, page 289-297.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Generating images from captions with attention",
                "authors": [
                    {
                        "first": "Elman",
                        "middle": [],
                        "last": "Mansimov",
                        "suffix": ""
                    },
                    {
                        "first": "Emilio",
                        "middle": [],
                        "last": "Parisotto",
                        "suffix": ""
                    },
                    {
                        "first": "Jimmy",
                        "middle": [],
                        "last": "Ba",
                        "suffix": ""
                    },
                    {
                        "first": "Ruslan",
                        "middle": [],
                        "last": "Salakhutdinov",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Elman Mansimov, Emilio Parisotto, Jimmy Ba, and Rus- lan Salakhutdinov. 2016. Generating images from captions with attention. In ICLR.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Plug & play generative networks: Conditional iterative generation of images in latent space",
                "authors": [
                    {
                        "first": "Anh",
                        "middle": [],
                        "last": "Nguyen",
                        "suffix": ""
                    },
                    {
                        "first": "Jeff",
                        "middle": [],
                        "last": "Clune",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Anh Nguyen, Jeff Clune, Yoshua Bengio, Alexey Doso- vitskiy, and Jason Yosinski. 2017. Plug & play gen- erative networks: Conditional iterative generation of images in latent space. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recog- nition. IEEE.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Bleu: a method for automatic evaluation of machine translation",
                "authors": [
                    {
                        "first": "Kishore",
                        "middle": [],
                        "last": "Papineni",
                        "suffix": ""
                    },
                    {
                        "first": "Salim",
                        "middle": [],
                        "last": "Roukos",
                        "suffix": ""
                    },
                    {
                        "first": "Todd",
                        "middle": [],
                        "last": "Ward",
                        "suffix": ""
                    },
                    {
                        "first": "Wei-Jing",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "311--318",
                "other_ids": {
                    "DOI": [
                        "10.3115/1073083.1073135"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic evalu- ation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Compu- tational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Two causal principles for improving visual dialog",
                "authors": [
                    {
                        "first": "Jiaxin",
                        "middle": [],
                        "last": "Qi",
                        "suffix": ""
                    },
                    {
                        "first": "Yulei",
                        "middle": [],
                        "last": "Niu",
                        "suffix": ""
                    },
                    {
                        "first": "Jianqiang",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Hanwang",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jiaxin Qi, Yulei Niu, Jianqiang Huang, and Hanwang Zhang. 2020. Two causal principles for improving visual dialog. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "Prophetnet-x: Large-scale pre-training models for english, chinese, multi-lingual, dialog, and code generation",
                "authors": [
                    {
                        "first": "Weizhen",
                        "middle": [],
                        "last": "Qi",
                        "suffix": ""
                    },
                    {
                        "first": "Yeyun",
                        "middle": [],
                        "last": "Gong",
                        "suffix": ""
                    },
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Yan",
                        "suffix": ""
                    },
                    {
                        "first": "Can",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Bolun",
                        "middle": [],
                        "last": "Yao",
                        "suffix": ""
                    },
                    {
                        "first": "Bartuer",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Biao",
                        "middle": [],
                        "last": "Cheng",
                        "suffix": ""
                    },
                    {
                        "first": "Daxin",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    },
                    {
                        "first": "Jiusheng",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Ruofei",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2104.08006"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Weizhen Qi, Yeyun Gong, Yu Yan, Can Xu, Bolun Yao, Bartuer Zhou, Biao Cheng, Daxin Jiang, Jiusheng Chen, Ruofei Zhang, et al. 2021. Prophetnet-x: Large-scale pre-training models for english, chinese, multi-lingual, dialog, and code generation. arXiv preprint arXiv:2104.08006.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "Learning transferable visual models from natural language supervision",
                "authors": [
                    {
                        "first": "Alec",
                        "middle": [],
                        "last": "Radford",
                        "suffix": ""
                    },
                    {
                        "first": "Jong",
                        "middle": [
                            "Wook"
                        ],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Hallacy",
                        "suffix": ""
                    },
                    {
                        "first": "Aditya",
                        "middle": [],
                        "last": "Ramesh",
                        "suffix": ""
                    },
                    {
                        "first": "Gabriel",
                        "middle": [],
                        "last": "Goh",
                        "suffix": ""
                    },
                    {
                        "first": "Sandhini",
                        "middle": [],
                        "last": "Agarwal",
                        "suffix": ""
                    },
                    {
                        "first": "Girish",
                        "middle": [],
                        "last": "Sastry",
                        "suffix": ""
                    },
                    {
                        "first": "Amanda",
                        "middle": [],
                        "last": "Askell",
                        "suffix": ""
                    },
                    {
                        "first": "Pamela",
                        "middle": [],
                        "last": "Mishkin",
                        "suffix": ""
                    },
                    {
                        "first": "Jack",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    },
                    {
                        "first": "Gretchen",
                        "middle": [],
                        "last": "Krueger",
                        "suffix": ""
                    },
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 38th International Conference on Machine Learning",
                "volume": "139",
                "issue": "",
                "pages": "8748--8763",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sas- try, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learn- ing transferable visual models from natural language supervision. In Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 8748-8763. PMLR.",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
                "authors": [
                    {
                        "first": "Colin",
                        "middle": [],
                        "last": "Raffel",
                        "suffix": ""
                    },
                    {
                        "first": "Noam",
                        "middle": [],
                        "last": "Shazeer",
                        "suffix": ""
                    },
                    {
                        "first": "Adam",
                        "middle": [],
                        "last": "Roberts",
                        "suffix": ""
                    },
                    {
                        "first": "Katherine",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Sharan",
                        "middle": [],
                        "last": "Narang",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Matena",
                        "suffix": ""
                    },
                    {
                        "first": "Yanqi",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [
                            "J"
                        ],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Journal of Machine Learning Research",
                "volume": "21",
                "issue": "140",
                "pages": "1--67",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Colin Raffel, Noam Shazeer, Adam Roberts, Kather- ine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67.",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "b36",
                "title": "Zero-shot text-to-image generation",
                "authors": [
                    {
                        "first": "Aditya",
                        "middle": [],
                        "last": "Ramesh",
                        "suffix": ""
                    },
                    {
                        "first": "Mikhail",
                        "middle": [],
                        "last": "Pavlov",
                        "suffix": ""
                    },
                    {
                        "first": "Gabriel",
                        "middle": [],
                        "last": "Goh",
                        "suffix": ""
                    },
                    {
                        "first": "Scott",
                        "middle": [],
                        "last": "Gray",
                        "suffix": ""
                    },
                    {
                        "first": "Chelsea",
                        "middle": [],
                        "last": "Voss",
                        "suffix": ""
                    },
                    {
                        "first": "Alec",
                        "middle": [],
                        "last": "Radford",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 38th International Conference on Machine Learning",
                "volume": "139",
                "issue": "",
                "pages": "8821--8831",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. Zero-shot text-to-image gen- eration. In Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 8821-8831. PMLR.",
                "links": null
            },
            "BIBREF37": {
                "ref_id": "b37",
                "title": "Generative adversarial text to image synthesis",
                "authors": [
                    {
                        "first": "Scott",
                        "middle": [],
                        "last": "Reed",
                        "suffix": ""
                    },
                    {
                        "first": "Zeynep",
                        "middle": [],
                        "last": "Akata",
                        "suffix": ""
                    },
                    {
                        "first": "Xinchen",
                        "middle": [],
                        "last": "Yan",
                        "suffix": ""
                    },
                    {
                        "first": "Lajanugen",
                        "middle": [],
                        "last": "Logeswaran",
                        "suffix": ""
                    },
                    {
                        "first": "Bernt",
                        "middle": [],
                        "last": "Schiele",
                        "suffix": ""
                    },
                    {
                        "first": "Honglak",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of The 33rd International Conference on Machine Learning",
                "volume": "48",
                "issue": "",
                "pages": "1060--1069",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. 2016. Generative adversarial text to image synthesis. In Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pages 1060-1069, New York, New York, USA. PMLR.",
                "links": null
            },
            "BIBREF38": {
                "ref_id": "b38",
                "title": "Exploring models and data for image question answering",
                "authors": [
                    {
                        "first": "Mengye",
                        "middle": [],
                        "last": "Ren",
                        "suffix": ""
                    },
                    {
                        "first": "Ryan",
                        "middle": [],
                        "last": "Kiros",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Zemel",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "NIPS",
                "volume": "",
                "issue": "",
                "pages": "2953--2961",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mengye Ren, Ryan Kiros, and Richard Zemel. 2015. Exploring models and data for image question an- swering. In NIPS, page 2953-2961.",
                "links": null
            },
            "BIBREF39": {
                "ref_id": "b39",
                "title": "Recipes for building an open-domain chatbot",
                "authors": [
                    {
                        "first": "Stephen",
                        "middle": [],
                        "last": "Roller",
                        "suffix": ""
                    },
                    {
                        "first": "Emily",
                        "middle": [],
                        "last": "Dinan",
                        "suffix": ""
                    },
                    {
                        "first": "Naman",
                        "middle": [],
                        "last": "Goyal",
                        "suffix": ""
                    },
                    {
                        "first": "Da",
                        "middle": [],
                        "last": "Ju",
                        "suffix": ""
                    },
                    {
                        "first": "Mary",
                        "middle": [],
                        "last": "Williamson",
                        "suffix": ""
                    },
                    {
                        "first": "Yinhan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Jing",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Myle",
                        "middle": [],
                        "last": "Ott",
                        "suffix": ""
                    },
                    {
                        "first": "Eric",
                        "middle": [
                            "Michael"
                        ],
                        "last": "Smith",
                        "suffix": ""
                    },
                    {
                        "first": "Y-Lan",
                        "middle": [],
                        "last": "Boureau",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Weston",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
                "volume": "",
                "issue": "",
                "pages": "300--325",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Eric Michael Smith, Y-Lan Boureau, and Jason We- ston. 2021. Recipes for building an open-domain chatbot. In Proceedings of the 16th Conference of the European Chapter of the Association for Compu- tational Linguistics: Main Volume, pages 300-325, Online. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF40": {
                "ref_id": "b40",
                "title": "What makes a good conversation? how controllable attributes affect human judgments",
                "authors": [
                    {
                        "first": "Abigail",
                        "middle": [],
                        "last": "See",
                        "suffix": ""
                    },
                    {
                        "first": "Stephen",
                        "middle": [],
                        "last": "Roller",
                        "suffix": ""
                    },
                    {
                        "first": "Douwe",
                        "middle": [],
                        "last": "Kiela",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Weston",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Abigail See, Stephen Roller, Douwe Kiela, and Jason Weston. 2019. What makes a good conversation? how controllable attributes affect human judgments.",
                "links": null
            },
            "BIBREF41": {
                "ref_id": "b41",
                "title": "Building end-to-end dialogue systems using generative hierarchical neural network models",
                "authors": [
                    {
                        "first": "Iulian",
                        "middle": [],
                        "last": "Serban",
                        "suffix": ""
                    },
                    {
                        "first": "Alessandro",
                        "middle": [],
                        "last": "Sordoni",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    },
                    {
                        "first": "Aaron",
                        "middle": [],
                        "last": "Courville",
                        "suffix": ""
                    },
                    {
                        "first": "Joelle",
                        "middle": [],
                        "last": "Pineau",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Iulian Serban, Alessandro Sordoni, Yoshua Bengio, Aaron Courville, and Joelle Pineau. 2016. Build- ing end-to-end dialogue systems using generative hierarchical neural network models. Proceedings of the AAAI Conference on Artificial Intelligence, 30.",
                "links": null
            },
            "BIBREF42": {
                "ref_id": "b42",
                "title": "Neural responding machine for short-text conversation",
                "authors": [
                    {
                        "first": "Lifeng",
                        "middle": [],
                        "last": "Shang",
                        "suffix": ""
                    },
                    {
                        "first": "Zhengdong",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    },
                    {
                        "first": "Hang",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing",
                "volume": "1",
                "issue": "",
                "pages": "1577--1586",
                "other_ids": {
                    "DOI": [
                        "10.3115/v1/P15-1152"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Lifeng Shang, Zhengdong Lu, and Hang Li. 2015. Neu- ral responding machine for short-text conversation. In Proceedings of the 53rd Annual Meeting of the As- sociation for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1577- 1586, Beijing, China. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF43": {
                "ref_id": "b43",
                "title": "Sequence to sequence learning with neural networks",
                "authors": [
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    },
                    {
                        "first": "Oriol",
                        "middle": [],
                        "last": "Vinyals",
                        "suffix": ""
                    },
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Quoc",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Le",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "NIPS",
                "volume": "",
                "issue": "",
                "pages": "3104--3112",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to sequence learning with neural networks. In NIPS, page 3104-3112.",
                "links": null
            },
            "BIBREF44": {
                "ref_id": "b44",
                "title": "Get the point of my utterance! learning towards effective responses with multi-head attention mechanism",
                "authors": [
                    {
                        "first": "Chongyang",
                        "middle": [],
                        "last": "Tao",
                        "suffix": ""
                    },
                    {
                        "first": "Shen",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Mingyue",
                        "middle": [],
                        "last": "Shang",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Dongyan",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Rui",
                        "middle": [],
                        "last": "Yan",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "IJCAI-18",
                "volume": "",
                "issue": "",
                "pages": "4418--4424",
                "other_ids": {
                    "DOI": [
                        "10.24963/ijcai.2018/614"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Chongyang Tao, Shen Gao, Mingyue Shang, Wei Wu, Dongyan Zhao, and Rui Yan. 2018. Get the point of my utterance! learning towards effective responses with multi-head attention mechanism. In IJCAI-18, pages 4418-4424. International Joint Conferences on Artificial Intelligence Organization.",
                "links": null
            },
            "BIBREF45": {
                "ref_id": "b45",
                "title": "Yfcc100m. Communications of the ACM",
                "authors": [
                    {
                        "first": "Bart",
                        "middle": [],
                        "last": "Thomee",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [
                            "A"
                        ],
                        "last": "Shamma",
                        "suffix": ""
                    },
                    {
                        "first": "Gerald",
                        "middle": [],
                        "last": "Friedland",
                        "suffix": ""
                    },
                    {
                        "first": "Benjamin",
                        "middle": [],
                        "last": "Elizalde",
                        "suffix": ""
                    },
                    {
                        "first": "Karl",
                        "middle": [],
                        "last": "Ni",
                        "suffix": ""
                    },
                    {
                        "first": "Douglas",
                        "middle": [],
                        "last": "Poland",
                        "suffix": ""
                    },
                    {
                        "first": "Damian",
                        "middle": [],
                        "last": "Borth",
                        "suffix": ""
                    },
                    {
                        "first": "Li-Jia",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "59",
                "issue": "",
                "pages": "64--73",
                "other_ids": {
                    "DOI": [
                        "10.1145/2812802"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Bart Thomee, David A. Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. 2016. Yfcc100m. Communica- tions of the ACM, 59(2):64-73.",
                "links": null
            },
            "BIBREF46": {
                "ref_id": "b46",
                "title": "Attention is all you need",
                "authors": [
                    {
                        "first": "Ashish",
                        "middle": [],
                        "last": "Vaswani",
                        "suffix": ""
                    },
                    {
                        "first": "Noam",
                        "middle": [],
                        "last": "Shazeer",
                        "suffix": ""
                    },
                    {
                        "first": "Niki",
                        "middle": [],
                        "last": "Parmar",
                        "suffix": ""
                    },
                    {
                        "first": "Jakob",
                        "middle": [],
                        "last": "Uszkoreit",
                        "suffix": ""
                    },
                    {
                        "first": "Llion",
                        "middle": [],
                        "last": "Jones",
                        "suffix": ""
                    },
                    {
                        "first": "Aidan",
                        "middle": [
                            "N"
                        ],
                        "last": "Gomez",
                        "suffix": ""
                    },
                    {
                        "first": "\u0141 Ukasz",
                        "middle": [],
                        "last": "Kaiser",
                        "suffix": ""
                    },
                    {
                        "first": "Illia",
                        "middle": [],
                        "last": "Polosukhin",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "30",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. 2017a. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
                "links": null
            },
            "BIBREF47": {
                "ref_id": "b47",
                "title": "Attention is all you need",
                "authors": [
                    {
                        "first": "Ashish",
                        "middle": [],
                        "last": "Vaswani",
                        "suffix": ""
                    },
                    {
                        "first": "Noam",
                        "middle": [],
                        "last": "Shazeer",
                        "suffix": ""
                    },
                    {
                        "first": "Niki",
                        "middle": [],
                        "last": "Parmar",
                        "suffix": ""
                    },
                    {
                        "first": "Jakob",
                        "middle": [],
                        "last": "Uszkoreit",
                        "suffix": ""
                    },
                    {
                        "first": "Llion",
                        "middle": [],
                        "last": "Jones",
                        "suffix": ""
                    },
                    {
                        "first": "Aidan",
                        "middle": [
                            "N"
                        ],
                        "last": "Gomez",
                        "suffix": ""
                    },
                    {
                        "first": "\u0141ukasz",
                        "middle": [],
                        "last": "Kaiser",
                        "suffix": ""
                    },
                    {
                        "first": "Illia",
                        "middle": [],
                        "last": "Polosukhin",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Advances in neural information processing systems",
                "volume": "",
                "issue": "",
                "pages": "5998--6008",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017b. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008.",
                "links": null
            },
            "BIBREF48": {
                "ref_id": "b48",
                "title": "A neural conversational model",
                "authors": [
                    {
                        "first": "Oriol",
                        "middle": [],
                        "last": "Vinyals",
                        "suffix": ""
                    },
                    {
                        "first": "Quoc",
                        "middle": [],
                        "last": "Le",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Oriol Vinyals and Quoc Le. 2015. A neural conversa- tional model.",
                "links": null
            },
            "BIBREF49": {
                "ref_id": "b49",
                "title": "Hierarchical recurrent attention network for response generation",
                "authors": [
                    {
                        "first": "Chen",
                        "middle": [],
                        "last": "Xing",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Yalou",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Wei-Ying",
                        "middle": [],
                        "last": "Ma",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chen Xing, Wei Wu, Yu Wu, Ming Zhou, Yalou Huang, and Wei-Ying Ma. 2017. Hierarchical recurrent at- tention network for response generation.",
                "links": null
            },
            "BIBREF50": {
                "ref_id": "b50",
                "title": "Neural response generation with meta-words",
                "authors": [
                    {
                        "first": "Can",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Chongyang",
                        "middle": [],
                        "last": "Tao",
                        "suffix": ""
                    },
                    {
                        "first": "Huang",
                        "middle": [],
                        "last": "Hu",
                        "suffix": ""
                    },
                    {
                        "first": "Matt",
                        "middle": [],
                        "last": "Schuerman",
                        "suffix": ""
                    },
                    {
                        "first": "Ying",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1906.06050"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Can Xu, Wei Wu, Chongyang Tao, Huang Hu, Matt Schuerman, and Ying Wang. 2019. Neural re- sponse generation with meta-words. arXiv preprint arXiv:1906.06050.",
                "links": null
            },
            "BIBREF51": {
                "ref_id": "b51",
                "title": "Show, attend and tell: Neural image caption generation with visual attention",
                "authors": [
                    {
                        "first": "Kelvin",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Jimmy",
                        "middle": [],
                        "last": "Ba",
                        "suffix": ""
                    },
                    {
                        "first": "Ryan",
                        "middle": [],
                        "last": "Kiros",
                        "suffix": ""
                    },
                    {
                        "first": "Kyunghyun",
                        "middle": [],
                        "last": "Cho",
                        "suffix": ""
                    },
                    {
                        "first": "Aaron",
                        "middle": [],
                        "last": "Courville",
                        "suffix": ""
                    },
                    {
                        "first": "Ruslan",
                        "middle": [],
                        "last": "Salakhutdinov",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Zemel",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 32nd International Conference on Machine Learning",
                "volume": "37",
                "issue": "",
                "pages": "2048--2057",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, and Yoshua Bengio. 2015. Show, attend and tell: Neural image caption generation with visual at- tention. In Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Pro- ceedings of Machine Learning Research, pages 2048- 2057, Lille, France. PMLR.",
                "links": null
            },
            "BIBREF52": {
                "ref_id": "b52",
                "title": "Attngan: Fine-grained text to image generation with attentional generative adversarial networks",
                "authors": [
                    {
                        "first": "Tao",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Pengchuan",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Qiuyuan",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Han",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Zhe",
                        "middle": [],
                        "last": "Gan",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaolei",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaodong",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. 2018. Attngan: Fine-grained text to image generation with attentional generative adversarial networks.",
                "links": null
            },
            "BIBREF53": {
                "ref_id": "b53",
                "title": "Open domain dialogue generation with latent images",
                "authors": [
                    {
                        "first": "Ze",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Huang",
                        "middle": [],
                        "last": "Hu",
                        "suffix": ""
                    },
                    {
                        "first": "Can",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Zhoujun",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "volume": "35",
                "issue": "",
                "pages": "14239--14247",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ze Yang, Wei Wu, Huang Hu, Can Xu, Wei Wang, and Zhoujun Li. 2021. Open domain dialogue generation with latent images. Proceedings of the AAAI Confer- ence on Artificial Intelligence, 35(16):14239-14247.",
                "links": null
            },
            "BIBREF54": {
                "ref_id": "b54",
                "title": "PhotoChat: A human-human dialogue dataset with photo sharing behavior for joint image-text modeling",
                "authors": [
                    {
                        "first": "Xiaoxue",
                        "middle": [],
                        "last": "Zang",
                        "suffix": ""
                    },
                    {
                        "first": "Lijuan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Maria",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Yang",
                        "middle": [],
                        "last": "Song",
                        "suffix": ""
                    },
                    {
                        "first": "Hao",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Jindong",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
                "volume": "1",
                "issue": "",
                "pages": "6142--6152",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2021.acl-long.479"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Xiaoxue Zang, Lijuan Liu, Maria Wang, Yang Song, Hao Zhang, and Jindong Chen. 2021. PhotoChat: A human-human dialogue dataset with photo sharing behavior for joint image-text modeling. In Proceed- ings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6142-6152, Online. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF55": {
                "ref_id": "b55",
                "title": "ReCoSa: Detecting the relevant contexts with self-attention for multi-turn dialogue generation",
                "authors": [
                    {
                        "first": "Hainan",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Yanyan",
                        "middle": [],
                        "last": "Lan",
                        "suffix": ""
                    },
                    {
                        "first": "Liang",
                        "middle": [],
                        "last": "Pang",
                        "suffix": ""
                    },
                    {
                        "first": "Jiafeng",
                        "middle": [],
                        "last": "Guo",
                        "suffix": ""
                    },
                    {
                        "first": "Xueqi",
                        "middle": [],
                        "last": "Cheng",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "3721--3730",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/P19-1362"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Hainan Zhang, Yanyan Lan, Liang Pang, Jiafeng Guo, and Xueqi Cheng. 2019. ReCoSa: Detecting the relevant contexts with self-attention for multi-turn dialogue generation. In Proceedings of the 57th An- nual Meeting of the Association for Computational Linguistics, pages 3721-3730, Florence, Italy. Asso- ciation for Computational Linguistics.",
                "links": null
            },
            "BIBREF56": {
                "ref_id": "b56",
                "title": "Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks",
                "authors": [
                    {
                        "first": "Han",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Tao",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Hongsheng",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Shaoting",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaogang",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaolei",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Dimitris",
                        "middle": [],
                        "last": "Metaxas",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "ICCV",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xi- aogang Wang, Xiaolei Huang, and Dimitris Metaxas. 2017. Stackgan: Text to photo-realistic image syn- thesis with stacked generative adversarial networks. In ICCV.",
                "links": null
            },
            "BIBREF57": {
                "ref_id": "b57",
                "title": "Personalizing dialogue agents: I have a dog, do you have pets too?",
                "authors": [
                    {
                        "first": "Saizheng",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Emily",
                        "middle": [],
                        "last": "Dinan",
                        "suffix": ""
                    },
                    {
                        "first": "Jack",
                        "middle": [],
                        "last": "Urbanek",
                        "suffix": ""
                    },
                    {
                        "first": "Arthur",
                        "middle": [],
                        "last": "Szlam",
                        "suffix": ""
                    },
                    {
                        "first": "Douwe",
                        "middle": [],
                        "last": "Kiela",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Weston",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
                "volume": "1",
                "issue": "",
                "pages": "2204--2213",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/P18-1205"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston. 2018. Per- sonalizing dialogue agents: I have a dog, do you have pets too? In Proceedings of the 56th Annual Meeting of the Association for Computational Lin- guistics (Volume 1: Long Papers), pages 2204-2213, Melbourne, Australia. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF58": {
                "ref_id": "b58",
                "title": "Dialogpt: Large-scale generative pre-training for conversational response generation",
                "authors": [
                    {
                        "first": "Yizhe",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Siqi",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Michel",
                        "middle": [],
                        "last": "Galley",
                        "suffix": ""
                    },
                    {
                        "first": "Yen-Chun",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Brockett",
                        "suffix": ""
                    },
                    {
                        "first": "Xiang",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Jianfeng",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Jingjing",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Bill",
                        "middle": [],
                        "last": "Dolan",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill Dolan. 2020. Dialogpt: Large-scale generative pre-training for conversational response generation. In ACL, system demonstration.",
                "links": null
            },
            "BIBREF59": {
                "ref_id": "b59",
                "title": "Learning discourse-level diversity for neural dialog 2864 models using conditional variational autoencoders",
                "authors": [
                    {
                        "first": "Tiancheng",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Ran",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Maxine",
                        "middle": [],
                        "last": "Eskenazi",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
                "volume": "1",
                "issue": "",
                "pages": "654--664",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/P17-1061"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Tiancheng Zhao, Ran Zhao, and Maxine Eskenazi. 2017. Learning discourse-level diversity for neural dialog 2864 models using conditional variational autoencoders. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 654-664, Vancouver, Canada. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF60": {
                "ref_id": "b60",
                "title": "Learning a simple and effective model for multiturn response generation with auxiliary tasks",
                "authors": [
                    {
                        "first": "Yufan",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Can",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Lei",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2004.01972"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yufan Zhao, Can Xu, Wei Wu, and Lei Yu. 2020. Learning a simple and effective model for multi- turn response generation with auxiliary tasks. arXiv preprint arXiv:2004.01972.",
                "links": null
            },
            "BIBREF61": {
                "ref_id": "b61",
                "title": "Emotional chatting machine: Emotional conversation generation with internal and external memory",
                "authors": [
                    {
                        "first": "Hao",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Minlie",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Tianyang",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaoyan",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "Bing",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hao Zhou, Minlie Huang, Tianyang Zhang, Xiaoyan Zhu, and Bing Liu. 2018. Emotional chatting ma- chine: Emotional conversation generation with inter- nal and external memory.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: An example of human conversations. They are talking about vacation and outdoors with both text and various images.",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 2: The overview of our multimodal dialogue response generation model. The Textual Dialogue Response Generator takes the text dialogue context U as input and generates a sequence contains text response and a image description (e.g., \"a parrot with red belly and green back is standing on the railing.\"). With the description as a condition, the Text-to-Image Translator generates image representation \u1e91. The Image Decoder V D reconstructs \u1e91 to a realistic and consistent high resolution image.",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Figure 3: Abstract Logic of the proposed approach. Solid lines mean that there exists large-scale training set to pre-train the generation model, while dotted lines mean that only very few training instances are available, \"\u00d7\" means bad generation quality.",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "FIGREF3": {
                "num": null,
                "text": "Figure 4: Qualitative assessment of various variants for image generation with same context as input in Pho-toChat test set. 1st column: Divter. 2nd column: Divter w/o G pre-train. 3rd column: Divter w/o F pre-train.",
                "uris": null,
                "fig_num": "4",
                "type_str": "figure"
            },
            "FIGREF4": {
                "num": null,
                "text": "Figure 5: Examples of the images generated by Divter and the images retrieved by SCAN. The dialogue contexts are presented in Appendix A.2.",
                "uris": null,
                "fig_num": "5",
                "type_str": "figure"
            },
            "FIGREF5": {
                "num": null,
                "text": "...the new ice cream shop is amazing. ...... A: I had the twist chocolate and vanilla but it was so fresh tasiting. like you just made it. like you just made it. B: I call it the malado gilato. A: Sam wouldn't let me have another lick bc he thought I'd eat it all. D: That sounds interesting. D: Yes, could you please share it with me? D: Objects in the photo: Chocolate Ice cream, Dairy, Drink. D: D: Wow! The ice cream looks so delicious. D: Sure, it tastes pretty good. A: Have you been out in nature lately? B: Yes. ...... A: I'm sitting at home now looking through some old photographs. B: I see. than. A: Would you like to see one of my favorites It's a cool shot of a honey bee near a beautiful flower. D: Objects in the photo: Honey bee, Insect, Animal, Flower. D: D: It is a nice picture. Thank you for sharing. D: Haha, just enjoy the beautiful scenery. D: Yeah, definitely. Table 4: Examples of PhotoChat test set. In each example, the turns with the prefix of \"A\"/\"B\" are the given context; the blue text is the text description generated by Divter; the left image and the red response are generated by Divter, the right image is the ground-truth image. in most of the metrics. For a more intuitive comparison, the qualitative assessment results are also shown in Figure 4. In particular, both quantitative and qualitative results on the ablation study validate that: (1) pre-training is crucial to low-resource multimodal dialogue response generation, since removing any component from pre-training causes performance drop when training data is small; (2) in terms of impact to performance of image generation, F > G, in terms of impact to performance of text generation, G > F ; (3) The joint learning also has contributions to Divter, indicating that leveraging the integrated learning of textual context and visual image benefits more in contrast to any single one of them.",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td>Models</td><td/><td/><td>Intent F1</td><td colspan=\"4\">Image Description Generation PPL B-1 B-2 Rouge</td><td colspan=\"2\">Image Generation FID \u2193 IS \u2191</td><td colspan=\"3\">Text Response Generation PPL B-1 B-2 Rouge</td></tr><tr><td colspan=\"2\">BERT-base</td><td/><td>53.2  *</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>T5-3B</td><td/><td/><td>58.9  *</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>S2S-TF</td><td/><td/><td>47.6</td><td>213.81</td><td>1.65</td><td>0.17</td><td>1.84</td><td>278.63</td><td>4.4 \u00b1 0.8</td><td>329.43</td><td>3.61</td><td>0.40</td><td>3.05</td></tr><tr><td>Divter</td><td/><td/><td>56.2</td><td>5.12</td><td>15.08</td><td>11.42</td><td>15.81</td><td>29.16</td><td>15.8 \u00b1 0.6</td><td>59.63</td><td>6.52</td><td>1.66</td><td>5.69</td></tr><tr><td colspan=\"2\">Divter (w/o G pre-train)</td><td/><td>47.3</td><td>122.56</td><td>1.99</td><td>0.23</td><td>2.60</td><td>29.78</td><td>15.5 \u00b1 0.5</td><td>153.62</td><td>4.82</td><td>0.53</td><td>3.83</td></tr><tr><td colspan=\"2\">Divter (w/o F \u03d5 pre-train)</td><td/><td>55.9</td><td>5.23</td><td>15.01</td><td>11.20</td><td>15.63</td><td>262.09</td><td>4.9 \u00b1 0.7</td><td>63.76</td><td>6.28</td><td>1.51</td><td>5.40</td></tr><tr><td colspan=\"3\">Divter (w/o G, F \u03d5 pre-train)</td><td>47.1</td><td>128.87</td><td>1.75</td><td>0.21</td><td>2.38</td><td>254.31</td><td>5.2 \u00b1 0.6</td><td>163.85</td><td>4.53</td><td>0.48</td><td>3.55</td></tr><tr><td colspan=\"2\">Divter (w/o joint learning)</td><td/><td>55.6</td><td>5.20</td><td>15.00</td><td>11.36</td><td>15.73</td><td>29.04</td><td>15.4 \u00b1 0.6</td><td>59.21</td><td>6.47</td><td>1.58</td><td>5.63</td></tr><tr><td>Models</td><td colspan=\"6\">Context Coherence Fluency Quality Consistency Text Image Background Kappa</td><td/><td/><td/><td/><td/></tr><tr><td>SCAN</td><td>-</td><td>-</td><td>1.95</td><td>0.96</td><td/><td>0.65</td><td/><td/><td/><td/><td/></tr><tr><td>S2S-TF</td><td>0.42</td><td>0.58</td><td>0.25</td><td>0.20</td><td/><td>0.67</td><td/><td/><td/><td/><td/></tr><tr><td>Divter</td><td>1.59</td><td>1.95</td><td>1.83</td><td>1.61</td><td/><td>0.63</td><td/><td/><td/><td/><td/></tr></table>",
                "type_str": "table",
                "text": "Automatic evaluation results of Divter and baselines on the test set. (w/o joint learning) means fine-tuning G and F \u03d5 respectively rather than using Eq. 8. Numbers in bold mean that the improvement to the best baseline is statistically significant (t-test with p-value < 0.01). * reported byZang et al. (2021).",
                "html": null,
                "num": null
            },
            "TABREF1": {
                "content": "<table><tr><td>Models</td><td colspan=\"3\">Overall Improvement</td><td>Kappa</td></tr><tr><td/><td colspan=\"3\">W(%) L(%) T(%)</td><td/></tr><tr><td>Divter (pure text) vs. DialoGPT</td><td>34.4</td><td>35.7</td><td>29.9</td><td>0.64</td></tr><tr><td>Divter vs. DialoGPT</td><td>53.5</td><td>27.4</td><td>19.1</td><td>0.68</td></tr></table>",
                "type_str": "table",
                "text": "Human evaluation results.",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table/>",
                "type_str": "table",
                "text": "",
                "html": null,
                "num": null
            }
        }
    }
}