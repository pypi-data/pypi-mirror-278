{
    "paper_id": "E06-1051",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:16:02.814392Z"
    },
    "title": "Exploiting Shallow Linguistic Information for Relation Extraction from Biomedical Literature",
    "authors": [
        {
            "first": "Claudio",
            "middle": [],
            "last": "Giuliano",
            "suffix": "",
            "affiliation": {},
            "email": "giuliano@itc.it"
        },
        {
            "first": "Alberto",
            "middle": [],
            "last": "Lavelli",
            "suffix": "",
            "affiliation": {},
            "email": "lavelli@itc.it"
        },
        {
            "first": "Lorenza",
            "middle": [],
            "last": "Romano",
            "suffix": "",
            "affiliation": {},
            "email": "romano@itc.it"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "We propose an approach for extracting relations between entities from biomedical literature based solely on shallow linguistic information. We use a combination of kernel functions to integrate two different information sources: (i) the whole sentence where the relation appears, and (ii) the local contexts around the interacting entities. We performed experiments on extracting gene and protein interactions from two different data sets. The results show that our approach outperforms most of the previous methods based on syntactic and semantic information.",
    "pdf_parse": {
        "paper_id": "E06-1051",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "We propose an approach for extracting relations between entities from biomedical literature based solely on shallow linguistic information. We use a combination of kernel functions to integrate two different information sources: (i) the whole sentence where the relation appears, and (ii) the local contexts around the interacting entities. We performed experiments on extracting gene and protein interactions from two different data sets. The results show that our approach outperforms most of the previous methods based on syntactic and semantic information.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Information Extraction (IE) is the process of finding relevant entities and their relationships within textual documents. Applications of IE range from Semantic Web to Bioinformatics. For example, there is an increasing interest in automatically extracting relevant information from biomedical literature. Recent evaluation campaigns on bio-entity recognition, such as BioCreAtIvE and JNLPBA 2004 shared task, have shown that several systems are able to achieve good performance (even if it is a bit worse than that reported on news articles). However, relation identification is more useful from an applicative perspective but it is still a considerable challenge for automatic tools.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this work, we propose a supervised machine learning approach to relation extraction which is applicable even when (deep) linguistic processing is not available or reliable. In particular, we explore a kernel-based approach based solely on shallow linguistic processing, such as tokeniza-tion, sentence splitting, Part-of-Speech (PoS) tagging and lemmatization.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Kernel methods (Shawe-Taylor and Cristianini, 2004) show their full potential when an explicit computation of the feature map becomes computationally infeasible, due to the high or even infinite dimension of the feature space. For this reason, kernels have been recently used to develop innovative approaches to relation extraction based on syntactic information, in which the examples preserve their original representations (i.e. parse trees) and are compared by the kernel function (Zelenko et al., 2003; Culotta and Sorensen, 2004; Zhao and Grishman, 2005) .",
                "cite_spans": [
                    {
                        "start": 15,
                        "end": 51,
                        "text": "(Shawe-Taylor and Cristianini, 2004)",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 485,
                        "end": 507,
                        "text": "(Zelenko et al., 2003;",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 508,
                        "end": 535,
                        "text": "Culotta and Sorensen, 2004;",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 536,
                        "end": 560,
                        "text": "Zhao and Grishman, 2005)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Despite the positive results obtained exploiting syntactic information, we claim that there is still room for improvement relying exclusively on shallow linguistic information for two main reasons. First of all, previous comparative evaluations put more stress on the deep linguistic approaches and did not put as much effort on developing effective methods based on shallow linguistic information. A second reason concerns the fact that syntactic parsing is not always robust enough to deal with real-world sentences. This may prevent approaches based on syntactic features from producing any result. Another related issue concerns the fact that parsers are available only for few languages and may not produce reliable results when used on domain specific texts (as is the case of the biomedical literature). For example, most of the participants at the Learning Language in Logic (LLL) challenge on Genic Interaction Extraction (see Section 4.2) were unable to successfully exploit linguistic information provided by parsers. It is still an open issue whether the use of domainspecific treebanks (such as the Genia treebank 1 ) can be successfully exploited to overcome this problem. Therefore it is essential to better investigate the potential of approaches based exclusively on simple linguistic features.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In our approach we use a combination of kernel functions to represent two distinct information sources: the global context where entities appear and their local contexts. The whole sentence where the entities appear (global context) is used to discover the presence of a relation between two entities, similarly to what was done by Bunescu and Mooney (2005b) . Windows of limited size around the entities (local contexts) provide useful clues to identify the roles of the entities within a relation. The approach has some resemblance with what was proposed by Roth and Yih (2002) . The main difference is that we perform the extraction task in a single step via a combined kernel, while they used two separate classifiers to identify entities and relations and their output is later combined with a probabilistic global inference.",
                "cite_spans": [
                    {
                        "start": 332,
                        "end": 358,
                        "text": "Bunescu and Mooney (2005b)",
                        "ref_id": null
                    },
                    {
                        "start": 560,
                        "end": 579,
                        "text": "Roth and Yih (2002)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We evaluated our relation extraction algorithm on two biomedical data sets (i.e. the AImed corpus and the LLL challenge data set; see Section 4). The motivations for using these benchmarks derive from the increasing applicative interest in tools able to extract relations between relevant entities in biomedical texts and, consequently, from the growing availability of annotated data sets. The experiments show clearly that our approach consistently improves previous results. Surprisingly, it outperforms most of the systems based on syntactic or semantic information, even when this information is manually annotated (i.e. the LLL challenge).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The problem considered here is that of identifying interactions between genes and proteins from biomedical literature. More specifically, we performed experiments on two slightly different benchmark data sets (see Section 4 for a detailed description). In the former (AImed) gene/protein interactions are annotated without distinguishing the type and roles of the two interacting entities. The latter (LLL challenge) is more realistic (and complex) because it also aims at identifying the roles played by the interacting entities (agent and target). For example, in Figure 1 First of all, we describe the complex case, namely the protein/gene interactions (LLL challenge). For this data set entity recognition is performed using a dictionary of protein and gene names in which the type of the entities is unknown.",
                "cite_spans": [
                    {
                        "start": 435,
                        "end": 448,
                        "text": "(and complex)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 573,
                        "end": 574,
                        "text": "1",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Problem Formalization",
                "sec_num": "2"
            },
            {
                "text": "We generate examples for all the sentences containing at least two entities. Thus the number of examples generated for each sentence is given by the combinations of distinct entities (N ) selected two at a time, i.e. N C 2 . For example, as the sentence shown in Figure 1 contains three entities, the total number of examples generated is 3 C 2 = 3. In each example we assign the attribute CANDIDATE to each of the candidate interacting entities, while the other entities in the example are assigned the attribute OTHER, meaning that they do not participate in the relation. If a relation holds between the two candidate interacting entities the example is labeled 1 or 2 (according to the roles of the interacting entities, agent and target, i.e. to the direction of the relation); 0 otherwise. Figure 2 Note that in generating the examples from the sentence in Figure 1 we did not create three neg-ative examples (there are six potential ordered relations between three entities), thereby implicitly under-sampling the data set. This allows us to make the classification task simpler without loosing information. As a matter of fact, generating examples for each ordered pair of entities would produce two subsets of the same size containing similar examples (differing only for the attributes CANDIDATE and OTHER), but with different classification labels. Furthermore, under-sampling allows us to halve the data set size and reduce the data skewness.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 270,
                        "end": 271,
                        "text": "1",
                        "ref_id": "FIGREF1"
                    },
                    {
                        "start": 803,
                        "end": 804,
                        "text": "2",
                        "ref_id": "FIGREF3"
                    },
                    {
                        "start": 870,
                        "end": 871,
                        "text": "1",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Problem Formalization",
                "sec_num": "2"
            },
            {
                "text": "For the protein-protein interaction task (AImed) we use the correct entities provided by the manual annotation. As said at the beginning of this section, this task is simpler than the LLL challenge because there is no distinction between types (all entities are proteins) and roles (the relation is symmetric). As a consequence, the examples are generated as described above with the following difference: an example is labeled 1 if a relation holds between the two candidate interacting entities; 0 otherwise.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem Formalization",
                "sec_num": "2"
            },
            {
                "text": "The basic idea behind kernel methods is to embed the input data into a suitable feature space F via a mapping function \u03c6 : X \u2192 F, and then use a linear algorithm for discovering nonlinear patterns. Instead of using the explicit mapping \u03c6, we can use a kernel function K : X \u00d7 X \u2192 R, that corresponds to the inner product in a feature space which is, in general, different from the input space. Kernel methods allow us to design a modular system, in which the kernel function acts as an interface between the data and the learning algorithm. Thus the kernel function is the only domain specific module of the system, while the learning algorithm is a general purpose component. Potentially any kernel function can work with any kernel-based algorithm. In our approach we use Support Vector Machines (Vapnik, 1998) .",
                "cite_spans": [
                    {
                        "start": 798,
                        "end": 812,
                        "text": "(Vapnik, 1998)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Kernel Methods for Relation Extraction",
                "sec_num": "3"
            },
            {
                "text": "In order to implement the approach based on shallow linguistic information we employed a linear combination of kernels. Different works (Gliozzo et al., 2005; Zhao and Grishman, 2005; Culotta and Sorensen, 2004) empirically demonstrate the effectiveness of combining kernels in this way, showing that the combined kernel always improves the performance of the individual ones.",
                "cite_spans": [
                    {
                        "start": 136,
                        "end": 158,
                        "text": "(Gliozzo et al., 2005;",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 159,
                        "end": 183,
                        "text": "Zhao and Grishman, 2005;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 184,
                        "end": 211,
                        "text": "Culotta and Sorensen, 2004)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Kernel Methods for Relation Extraction",
                "sec_num": "3"
            },
            {
                "text": "In addition, this formulation allows us to evaluate the individual contribution of each information source. We designed two families of kernels: Global Context kernels and Local Context kernels, in which each single kernel is explicitly calculated as follows",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Kernel Methods for Relation Extraction",
                "sec_num": "3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "K(x1, x2) = \u03c6(x1), \u03c6(x2) \u03c6(x1) \u03c6(x2) ,",
                        "eq_num": "(1)"
                    }
                ],
                "section": "Kernel Methods for Relation Extraction",
                "sec_num": "3"
            },
            {
                "text": "where \u03c6(\u2022) is the embedding vector and \u2022 is the 2-norm. The kernel is normalized (divided) by the product of the norms of embedding vectors. The normalization factor plays an important role in allowing us to integrate information from heterogeneous feature spaces. Even though the resulting feature space has high dimensionality, an efficient computation of Equation 1 can be carried out explicitly since the input representations defined below are extremely sparse.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Kernel Methods for Relation Extraction",
                "sec_num": "3"
            },
            {
                "text": "In (Bunescu and Mooney, 2005b) , the authors observed that a relation between two entities is generally expressed using only words that appear simultaneously in one of the following three patterns:",
                "cite_spans": [
                    {
                        "start": 3,
                        "end": 30,
                        "text": "(Bunescu and Mooney, 2005b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Global Context Kernel",
                "sec_num": "3.1"
            },
            {
                "text": "Fore-Between: tokens before and between the two candidate interacting entities. For instance: binding of Our global context kernels operate on the patterns above, where each pattern is represented using a bag-of-words instead of sparse subsequences of words, PoS tags, entity and chunk types, or Word-Net synsets as in (Bunescu and Mooney, 2005b) . More formally, given a relation example R, we represent a pattern P as a row vector",
                "cite_spans": [
                    {
                        "start": 319,
                        "end": 346,
                        "text": "(Bunescu and Mooney, 2005b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Global Context Kernel",
                "sec_num": "3.1"
            },
            {
                "text": "\u03c6P (R) = (tf (t1, P ), tf (t2, P ), . . . , tf (t l , P )) \u2208 R l , (2)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Global Context Kernel",
                "sec_num": "3.1"
            },
            {
                "text": "where the function tf (t i , P ) records how many times a particular token t i is used in P . Note that, this approach differs from the standard bag-ofwords as punctuation and stop words are included in \u03c6 P , while the entities (with attribute CANDI-DATE and OTHER) are not. To improve the classification performance, we have further extended \u03c6 P to embed n-grams of (contiguous) tokens (up to n = 3). By substituting \u03c6 P into Equation 1, we obtain the n-gram kernel K n , which counts common uni-grams, bi-grams, . . . , n-grams that two patterns have in common2 . The Global Context kernel K GC (R 1 , R 2 ) is then defined as",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Global Context Kernel",
                "sec_num": "3.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "KF B (R1, R2) + KB(R1, R2) + KBA(R1, R2),",
                        "eq_num": "(3)"
                    }
                ],
                "section": "Global Context Kernel",
                "sec_num": "3.1"
            },
            {
                "text": "where K F B , K B and K BA are n-gram kernels that operate on the Fore-Between, Between and Between-After patterns respectively.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Global Context Kernel",
                "sec_num": "3.1"
            },
            {
                "text": "The type of the candidate interacting entities can provide useful clues for detecting the agent and target of the relation, as well as the presence of the relation itself. As the type is not known, we use the information provided by the two local contexts of the candidate interacting entities, called left and right local context respectively. As typically done in entity recognition, we represent each local context by using the following basic features:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Local Context Kernel",
                "sec_num": "3.2"
            },
            {
                "text": "Token The token itself. Lemma The lemma of the token.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Local Context Kernel",
                "sec_num": "3.2"
            },
            {
                "text": "The PoS tag of the token. Orthographic This feature maps each token into equivalence classes that encode attributes such as capitalization, punctuation, numerals and so on.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "PoS",
                "sec_num": null
            },
            {
                "text": "Formally, given a relation example R, a local context L = t -w , . . . , t -1 , t 0 , t +1 , . . . , t +w is represented as a row vector",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "PoS",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\u03c8L(R) = (f1(L), f2(L), . . . , fm(L)) \u2208 {0, 1} m , (",
                        "eq_num": "4"
                    }
                ],
                "section": "PoS",
                "sec_num": null
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "PoS",
                "sec_num": null
            },
            {
                "text": "where f i is a feature function that returns 1 if it is active in the specified position of L, 0 otherwise3 . The Local Context kernel K LC (R 1 , R 2 ) is defined as",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "PoS",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "K lef t (R1, R2) + K right (R1, R2),",
                        "eq_num": "(5)"
                    }
                ],
                "section": "PoS",
                "sec_num": null
            },
            {
                "text": "where K lef t and K right are defined by substituting the embedding of the left and right local context into Equation 1 respectively.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "PoS",
                "sec_num": null
            },
            {
                "text": "Notice that K LC differs substantially from K GC as it considers the ordering of the tokens and the feature space is enriched with PoS, lemma and orthographic features.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "PoS",
                "sec_num": null
            },
            {
                "text": "Finally, the Shallow Linguistic kernel",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Shallow Linguistic Kernel",
                "sec_num": "3.3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "K SL (R 1 , R 2 ) is defined as KGC (R1, R2) + KLC (R1, R2).",
                        "eq_num": "(6)"
                    }
                ],
                "section": "Shallow Linguistic Kernel",
                "sec_num": "3.3"
            },
            {
                "text": "It follows directly from the explicit construction of the feature space and from closure properties of kernels that K SL is a valid kernel.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Shallow Linguistic Kernel",
                "sec_num": "3.3"
            },
            {
                "text": "The two data sets used for the experiments concern the same domain (i.e. gene/protein interactions). However, they present a crucial difference which makes it worthwhile to show the experimental results on both of them. In one case (AImed) interactions are considered symmetric, while in the other (LLL challenge) agents and targets of genic interactions have to be identified.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data sets",
                "sec_num": "4"
            },
            {
                "text": "The first data set used in the experiments is the AImed corpus4 , previously used for training protein interaction extraction systems in (Bunescu et al., 2005; Bunescu and Mooney, 2005b) . It consists of 225 Medline abstracts: 200 are known to describe interactions between human proteins, while the other 25 do not refer to any interaction. There are 4,084 protein references and around 1,000 tagged interactions in this data set. In this data set there is no distinction between genes and proteins and the relations are symmetric.",
                "cite_spans": [
                    {
                        "start": 137,
                        "end": 159,
                        "text": "(Bunescu et al., 2005;",
                        "ref_id": null
                    },
                    {
                        "start": 160,
                        "end": 186,
                        "text": "Bunescu and Mooney, 2005b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "AImed corpus",
                "sec_num": "4.1"
            },
            {
                "text": "This data set was used in the Learning Language in Logic (LLL) challenge on Genic Interaction extraction5 (Ned\u00e9llec, 2005) . The objective of the challenge was to evaluate the performance of systems based on machine learning techniques to identify gene/protein interactions and their roles, agent or target. ",
                "cite_spans": [
                    {
                        "start": 106,
                        "end": 122,
                        "text": "(Ned\u00e9llec, 2005)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "LLL Challenge",
                "sec_num": "4.2"
            },
            {
                "text": "Before describing the results of the experiments, a note concerning the evaluation methodology.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "5"
            },
            {
                "text": "There are different ways of evaluating performance in extracting information, as noted in (Lavelli et al., 2004) for the extraction of slot fillers in the Seminar Announcement and the Job Posting data sets. Adapting the proposed classification to relation extraction, the following two cases can be identified:",
                "cite_spans": [
                    {
                        "start": 90,
                        "end": 112,
                        "text": "(Lavelli et al., 2004)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "5"
            },
            {
                "text": "\u2022 One Answer per Occurrence in the Document -OAOD (each individual occurrence of a protein interaction has to be extracted from the document);",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "5"
            },
            {
                "text": "\u2022 One Answer per Relation in a given Document -OARD (where two occurrences of the same protein interaction are considered one correct answer).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "5"
            },
            {
                "text": "Figure 3 shows a fragment of tagged text drawn from the AImed corpus. It contains three different interactions between pairs of proteins, for a total of seven occurrences of interactions. For example, there are three occurrences of the interaction between IGF-IR and p52Shc (i.e. number 1, 3 and 7). If we adopt the OAOD methodology, all the seven occurrences have to be extracted to achieve the maximum score. On the other hand, if we use the OARD methodology, only one occurrence for each interaction has to be extracted to maximize the score.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "3",
                        "ref_id": "FIGREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "5"
            },
            {
                "text": "On the AImed data set both evaluations were performed, while on the LLL challenge only the OAOD evaluation methodology was performed because this is the only one provided by the evaluation server of the challenge. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "5"
            },
            {
                "text": "All the experiments were performed using the SVM package LIBSVM6 customized to embed our own kernel. For the LLL challenge submission, we optimized the regularization parameter C by 10-fold cross validation; while we used its default value for the AImed experiment. In both experiments, we set the cost-factor W i to be the ratio between the number of negative and positive examples.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementation Details",
                "sec_num": "5.1"
            },
            {
                "text": "K SL performance was first evaluated on the AImed data set (Section 4.1). We first give an evaluation of the kernel combination and then we compare our results with the Subsequence Kernel for Relation Extraction (ERK) described in (Bunescu and Mooney, 2005b) . All experiments are conducted using 10-fold cross validation on the same data splitting used in (Bunescu et al., 2005; Bunescu and Mooney, 2005b) .",
                "cite_spans": [
                    {
                        "start": 231,
                        "end": 258,
                        "text": "(Bunescu and Mooney, 2005b)",
                        "ref_id": null
                    },
                    {
                        "start": 357,
                        "end": 379,
                        "text": "(Bunescu et al., 2005;",
                        "ref_id": null
                    },
                    {
                        "start": 380,
                        "end": 406,
                        "text": "Bunescu and Mooney, 2005b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results on AImed",
                "sec_num": "5.2"
            },
            {
                "text": "Table 1 shows the performance of the three kernels defined in Section 3 for protein-protein interactions using the two evaluation methodologies described above.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "1",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Results on AImed",
                "sec_num": "5.2"
            },
            {
                "text": "We report in Figure 4 the precision-recall curves of ERK and K SL using OARD evaluation methodology (the evaluation performed by Bunescu and Mooney (2005b) ). As in (Bunescu et al., 2005; Bunescu and Mooney, 2005b) , the graph points are obtained by varying the threshold on the classifi- Finally, Figure 5 shows the learning curve of the combined kernel K SL using the OARD evaluation methodology. The curve reaches a plateau with around 100 Medline abstracts.",
                "cite_spans": [
                    {
                        "start": 129,
                        "end": 155,
                        "text": "Bunescu and Mooney (2005b)",
                        "ref_id": null
                    },
                    {
                        "start": 165,
                        "end": 187,
                        "text": "(Bunescu et al., 2005;",
                        "ref_id": null
                    },
                    {
                        "start": 188,
                        "end": 214,
                        "text": "Bunescu and Mooney, 2005b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 20,
                        "end": 21,
                        "text": "4",
                        "ref_id": "FIGREF5"
                    },
                    {
                        "start": 305,
                        "end": 306,
                        "text": "5",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Results on AImed",
                "sec_num": "5.2"
            },
            {
                "text": "The system was evaluated on the \"basic\" version of the LLL challenge data set (Section 4.2).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results on LLL challenge",
                "sec_num": "5.3"
            },
            {
                "text": "Table 2 shows the results of K SL returned by the scoring service8 for the three subsets of the training set (with and without coreferences, and with their union). Table 3 systems of the LLL challenge9 . Notice that the best results at the challenge were obtained by different groups and exploiting the linguistic \"enriched\" version of the data set. As observed in (Ned\u00e9llec, 2005) , the scores obtained using the training set without coreferences and the whole training set are similar. We also report in Table 4 an analysis of the kernel combination. Given that we are interested here in the contribution of each kernel, we evaluated the experiments by 10-fold cross-validation on the whole training set avoiding the submission process.",
                "cite_spans": [
                    {
                        "start": 365,
                        "end": 381,
                        "text": "(Ned\u00e9llec, 2005)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "2",
                        "ref_id": "TABREF3"
                    },
                    {
                        "start": 170,
                        "end": 171,
                        "text": "3",
                        "ref_id": "TABREF4"
                    },
                    {
                        "start": 512,
                        "end": 513,
                        "text": "4",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Results on LLL challenge",
                "sec_num": "5.3"
            },
            {
                "text": "The experimental results show that the combined kernel K SL outperforms the basic kernels K GC and K LC on both data sets. In particular, precision significantly increases at the expense of a lower recall. High precision is particularly advantageous when extracting knowledge from large corpora, because it avoids overloading end users with too many false positives.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion of Results",
                "sec_num": "5.4"
            },
            {
                "text": "Although the basic kernels were designed to model complementary aspects of the task (i.e. Table 4 : Comparison of the performance of kernel combination on the LLL challenge using 10-fold cross validation.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 96,
                        "end": 97,
                        "text": "4",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Discussion of Results",
                "sec_num": "5.4"
            },
            {
                "text": "presence of the relation and roles of the interacting entities), they perform reasonably well even when considered separately. In particular, K GC achieved good performance on both data sets. This result was not expected on the LLL challenge because this task requires not only to recognize the presence of relationships between entities but also to identify their roles. On the other hand, the outcomes of K LC on the AImed data set show that such kernel helps to identify the presence of relationships as well.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion of Results",
                "sec_num": "5.4"
            },
            {
                "text": "At first glance, it may seem strange that K GC outperforms ERK on AImed, as the latter approach exploits a richer representation: sparse sub-sequences of words, PoS tags, entity and chunk types, or WordNet synsets. However, an approach based on n-grams is sufficient to identify the presence of a relationship. This result sounds less surprising, if we recall that both approaches cast the relation extraction problem as a text categorization task. Approaches to text categorization based on rich linguistic information have obtained less accuracy than the traditional bag-of-words approach (e.g. (Koster and Seutter, 2003) ). Shallow linguistics information seems to be more effective to model the local context of the entities.",
                "cite_spans": [
                    {
                        "start": 597,
                        "end": 623,
                        "text": "(Koster and Seutter, 2003)",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion of Results",
                "sec_num": "5.4"
            },
            {
                "text": "Finally, we obtained worse results performing dimensionality reduction either based on generic linguistic assumptions (e.g. by removing words from stop lists or with certain PoS tags) or using statistical methods (e.g. tf.idf weighting schema). This may be explained by the fact that, in tasks like entity recognition and relation extraction, useful clues are also provided by high frequency tokens, such as stop words or punctuation marks, and by the relative positions in which they appear.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion of Results",
                "sec_num": "5.4"
            },
            {
                "text": "First of all, the obvious references for our work are the approaches evaluated on AImed and LLL challenge data sets.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "6"
            },
            {
                "text": "In (Bunescu and Mooney, 2005b) , the authors present a generalized subsequence kernel that works with sparse sequences containing combinations of words and PoS tags.",
                "cite_spans": [
                    {
                        "start": 3,
                        "end": 30,
                        "text": "(Bunescu and Mooney, 2005b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "6"
            },
            {
                "text": "The best results on the LLL challenge were obtained by the group from the University of Edinburgh (Reidel and Klein, 2005) , which used Markov Logic, a framework that combines loglinear models and First Order Logic, to create a set of weighted clauses which can classify pairs of gene named entities as genic interactions. These clauses are based on chains of syntactic and semantic relations in the parse or Discourse Representation Structure (DRS) of a sentence, respectively.",
                "cite_spans": [
                    {
                        "start": 98,
                        "end": 122,
                        "text": "(Reidel and Klein, 2005)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "6"
            },
            {
                "text": "Other relevant approaches include those that adopt kernel methods to perform relation extraction. Zelenko et al. (2003) describe a relation extraction algorithm that uses a tree kernel defined over a shallow parse tree representation of sentences. The approach is vulnerable to unrecoverable parsing errors. Culotta and Sorensen (2004) describe a slightly generalized version of this kernel based on dependency trees, in which a bag-ofwords kernel is used to compensate for errors in syntactic analysis. A further extension is proposed by Zhao and Grishman (2005) . They use composite kernels to integrate information from different syntactic sources (tokenization, sentence parsing, and deep dependency analysis) so that processing errors occurring at one level may be overcome by information from other levels. Bunescu and Mooney (2005a) present an alternative approach which uses information concentrated in the shortest path in the dependency tree between the two entities.",
                "cite_spans": [
                    {
                        "start": 98,
                        "end": 119,
                        "text": "Zelenko et al. (2003)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 308,
                        "end": 335,
                        "text": "Culotta and Sorensen (2004)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 539,
                        "end": 563,
                        "text": "Zhao and Grishman (2005)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 813,
                        "end": 839,
                        "text": "Bunescu and Mooney (2005a)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "6"
            },
            {
                "text": "As mentioned in Section 1, another relevant approach is presented in (Roth and Yih, 2002) . Classifiers that identify entities and relations among them are first learned from local information in the sentence. This information, along with constraints induced among entity types and relations, is used to perform global probabilistic inference that accounts for the mutual dependencies among the entities.",
                "cite_spans": [
                    {
                        "start": 69,
                        "end": 89,
                        "text": "(Roth and Yih, 2002)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "6"
            },
            {
                "text": "All the previous approaches have been evaluated on different data sets so that it is not possible to have a clear idea of which approach is better than the other.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "6"
            },
            {
                "text": "The good results obtained using only shallow linguistic features provide a higher baseline against which it is possible to measure improvements obtained using methods based on deep linguistic processing. In the near future, we plan to extend our work in several ways.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions and Future Work",
                "sec_num": "7"
            },
            {
                "text": "First, we would like to evaluate the contribution of syntactic information to relation extraction from biomedical literature. With this aim, we will integrate the output of a parser (possibly trained on a domain-specific resource such the Genia Treebank). Second, we plan to test the portability of our model on ACE and MUC data sets. Third, we would like to use a named entity recognizer instead of assuming that entities are already extracted or given by a dictionary. Our long term goal is to populate databases and ontologies by extracting information from large text collections such as Medline.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions and Future Work",
                "sec_num": "7"
            },
            {
                "text": "http://www-tsujii.is.s.u-tokyo.ac.jp/",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "In the literature, it is also called n-spectrum kernel.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "In the reported experiments, we used a context window of \u00b12 tokens around the candidate entity.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "ftp://ftp.cs.utexas.edu/pub/mooney/ bio-data/interactions.tar.gz",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "http://genome.jouy.inra.fr/texte/ LLLchallenge/",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "http://www.csie.ntu.edu.tw/\u02dccjlin/ libsvm/",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "For this purpose the probability estimate output of LIB-SVM is used.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "8 http://genome.jouy.inra.fr/texte/ LLLchallenge/scoringService.php",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "After the challenge deadline,Reidel and Klein (2005) achieved a significant improvement, F1 = 68.4% (without coreferences) and F1 = 64.7% (with and without coreferences).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "We would like to thank Razvan Bunescu for providing detailed information about the AImed data set and the settings of the experiments. Claudio Giuliano and Lorenza Romano have been supported by the ONTOTEXT project, funded by the Autonomous Province of Trento under the FUP-2004 research program.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgements",
                "sec_num": "8"
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "A shortest path dependency kernel for relation extraction",
                "authors": [
                    {
                        "first": "Razvan",
                        "middle": [],
                        "last": "Bunescu",
                        "suffix": ""
                    },
                    {
                        "first": "Raymond",
                        "middle": [
                            "J"
                        ],
                        "last": "Mooney",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Razvan Bunescu and Raymond J. Mooney. 2005a. A shortest path dependency kernel for relation ex- traction. In Proceedings of the Human Language Technology Conference and Conference on Empiri- cal Methods in Natural Language Processing, Van- couver, B.C, October.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Subsequence kernels for relation extraction",
                "authors": [
                    {
                        "first": "Razvan",
                        "middle": [],
                        "last": "Bunescu",
                        "suffix": ""
                    },
                    {
                        "first": "Raymond",
                        "middle": [
                            "J"
                        ],
                        "last": "Mooney",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of the 19th Conference on Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Razvan Bunescu and Raymond J. Mooney. 2005b. Subsequence kernels for relation extraction. In Proceedings of the 19th Conference on Neural In- formation Processing Systems, Vancouver, British Columbia.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Comparative experiments on learning information extractors for proteins and their interactions",
                "authors": [
                    {
                        "first": "Razvan",
                        "middle": [],
                        "last": "Bunescu",
                        "suffix": ""
                    },
                    {
                        "first": "Ruifang",
                        "middle": [],
                        "last": "Ge",
                        "suffix": ""
                    },
                    {
                        "first": "Rohit",
                        "middle": [
                            "J"
                        ],
                        "last": "Kate",
                        "suffix": ""
                    },
                    {
                        "first": "Edward",
                        "middle": [
                            "M"
                        ],
                        "last": "Marcotte",
                        "suffix": ""
                    },
                    {
                        "first": "Raymond",
                        "middle": [
                            "J"
                        ],
                        "last": "Mooney",
                        "suffix": ""
                    },
                    {
                        "first": "Arun",
                        "middle": [
                            "K"
                        ],
                        "last": "Ramani",
                        "suffix": ""
                    },
                    {
                        "first": "Yuk",
                        "middle": [
                            "Wah"
                        ],
                        "last": "Wong",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Artificial Intelligence in Medicine",
                "volume": "33",
                "issue": "2",
                "pages": "139--155",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Razvan Bunescu, Ruifang Ge, Rohit J. Kate, Ed- ward M. Marcotte, Raymond J. Mooney, Arun K. Ramani, and Yuk Wah Wong. 2005. Comparative experiments on learning information extractors for proteins and their interactions. Artificial Intelligence in Medicine, 33(2):139-155. Special Issue on Sum- marization and Information Extraction from Medi- cal Documents.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Dependency tree kernels for relation extraction",
                "authors": [
                    {
                        "first": "Aron",
                        "middle": [],
                        "last": "Culotta",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Sorensen",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL 2004)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree kernels for relation extraction. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL 2004), Barcelona, Spain.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Domain kernels for word sense disambiguation",
                "authors": [
                    {
                        "first": "Alfio",
                        "middle": [],
                        "last": "Gliozzo",
                        "suffix": ""
                    },
                    {
                        "first": "Claudio",
                        "middle": [],
                        "last": "Giuliano",
                        "suffix": ""
                    },
                    {
                        "first": "Carlo",
                        "middle": [],
                        "last": "Strapparava",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL 2005)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alfio Gliozzo, Claudio Giuliano, and Carlo Strappar- ava. 2005. Domain kernels for word sense disam- biguation. In Proceedings of the 43rd Annual Meet- ing of the Association for Computational Linguistics (ACL 2005), Ann Arbor, Michigan, June.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Taming wild phrases",
                "authors": [
                    {
                        "first": "H",
                        "middle": [
                            "A"
                        ],
                        "last": "Cornelis",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Koster",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Seutter",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Advances in Information Retrieval, 25th European Conference on IR Research (ECIR 2003)",
                "volume": "",
                "issue": "",
                "pages": "161--176",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Cornelis H. A. Koster and Mark Seutter. 2003. Taming wild phrases. In Advances in Information Retrieval, 25th European Conference on IR Research (ECIR 2003), pages 161-176, Pisa, Italy.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "IE evaluation: Criticisms and recommendations",
                "authors": [
                    {
                        "first": "Alberto",
                        "middle": [],
                        "last": "Lavelli",
                        "suffix": ""
                    },
                    {
                        "first": "Mary",
                        "middle": [
                            "Elaine"
                        ],
                        "last": "Califf",
                        "suffix": ""
                    },
                    {
                        "first": "Fabio",
                        "middle": [],
                        "last": "Ciravegna",
                        "suffix": ""
                    },
                    {
                        "first": "Dayne",
                        "middle": [],
                        "last": "Freitag",
                        "suffix": ""
                    },
                    {
                        "first": "Claudio",
                        "middle": [],
                        "last": "Giuliano",
                        "suffix": ""
                    },
                    {
                        "first": "Nicholas",
                        "middle": [],
                        "last": "Kushmerick",
                        "suffix": ""
                    },
                    {
                        "first": "Lorenza",
                        "middle": [],
                        "last": "Romano",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of the AAAI 2004 Workshop on Adaptive Text Extraction and Mining",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alberto Lavelli, Mary Elaine Califf, Fabio Ciravegna, Dayne Freitag, Claudio Giuliano, Nicholas Kushm- erick, and Lorenza Romano. 2004. IE evaluation: Criticisms and recommendations. In Proceedings of the AAAI 2004 Workshop on Adaptive Text Extrac- tion and Mining (ATEM 2004), San Jose, California.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Learning language in logicgenic interaction extraction challenge",
                "authors": [
                    {
                        "first": "Claire",
                        "middle": [],
                        "last": "Ned\u00e9llec",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of the ICML-2005 Workshop on Learning Language in Logic (LLL05)",
                "volume": "",
                "issue": "",
                "pages": "31--37",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Claire Ned\u00e9llec. 2005. Learning language in logic - genic interaction extraction challenge. In Proceed- ings of the ICML-2005 Workshop on Learning Lan- guage in Logic (LLL05), pages 31-37, Bonn, Ger- many, August.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Genic interaction extraction with semantic and syntactic chains",
                "authors": [
                    {
                        "first": "Sebastian",
                        "middle": [],
                        "last": "Reidel",
                        "suffix": ""
                    },
                    {
                        "first": "Ewan",
                        "middle": [],
                        "last": "Klein",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of the ICML-2005 Workshop on Learning Language in Logic (LLL05)",
                "volume": "",
                "issue": "",
                "pages": "69--74",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sebastian Reidel and Ewan Klein. 2005. Genic interaction extraction with semantic and syntactic chains. In Proceedings of the ICML-2005 Workshop on Learning Language in Logic (LLL05), pages 69- 74, Bonn, Germany, August.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Probabilistic reasoning for entity & relation recognition",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Roth",
                        "suffix": ""
                    },
                    {
                        "first": "W",
                        "middle": [],
                        "last": "Yih",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of the 19th International Conference on Computational Linguistics (COLING-02)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. Roth and W. Yih. 2002. Probabilistic reasoning for entity & relation recognition. In Proceedings of the 19th International Conference on Computational Linguistics (COLING-02), Taipei, Taiwan.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Kernel Methods for Pattern Analysis",
                "authors": [
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Shawe",
                        "suffix": ""
                    },
                    {
                        "first": "-",
                        "middle": [],
                        "last": "Taylor",
                        "suffix": ""
                    },
                    {
                        "first": "Nello",
                        "middle": [],
                        "last": "Cristianini",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "John Shawe-Taylor and Nello Cristianini. 2004. Ker- nel Methods for Pattern Analysis. Cambridge Uni- versity Press, New York, NY, USA.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Statistical Learning Theory",
                "authors": [
                    {
                        "first": "Vladimir",
                        "middle": [],
                        "last": "Vapnik",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Vladimir Vapnik. 1998. Statistical Learning Theory. John Wiley and Sons, New York.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Kernel methods for information extraction",
                "authors": [
                    {
                        "first": "Dmitry",
                        "middle": [],
                        "last": "Zelenko",
                        "suffix": ""
                    },
                    {
                        "first": "Chinatsu",
                        "middle": [],
                        "last": "Aone",
                        "suffix": ""
                    },
                    {
                        "first": "Anthony",
                        "middle": [],
                        "last": "Richardella",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Journal of Machine Learning Research",
                "volume": "3",
                "issue": "",
                "pages": "1083--1106",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella. 2003. Kernel methods for information extraction. Journal of Machine Learning Research, 3:1083-1106.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Extracting relations with integrated information using kernel methods",
                "authors": [
                    {
                        "first": "Shubin",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Ralph",
                        "middle": [],
                        "last": "Grishman",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL 2005)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Shubin Zhao and Ralph Grishman. 2005. Extracting relations with integrated information using kernel methods. In Proceedings of the 43rd Annual Meet- ing of the Association for Computational Linguistics (ACL 2005), Ann Arbor, Michigan, June.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "three entities are mentioned and two of the six ordered pairs of GENIA/topics/Corpus/GTB.html entities actually interact: (sigma(K), cwlH) and (gerE, cwlH).",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 1: A sentence with two relations, R 12 and R 32 , between three entities, E 1 , E 2 and E 3 . In our approach we cast relation extraction as a classification problem, in which examples are generated from sentences as follows.First of all, we describe the complex case, namely the protein/gene interactions (LLL challenge). For this data set entity recognition is performed using a dictionary of protein and gene names in which the type of the entities is unknown.We generate examples for all the sentences containing at least two entities. Thus the number of examples generated for each sentence is given by the combinations of distinct entities (N ) selected two at a time, i.e. N C 2 . For example, as the sentence shown in Figure1contains three entities, the total number of examples generated is 3 C 2 = 3. In each example we assign the attribute CANDIDATE to each of the candidate interacting entities, while the other entities in the example are assigned the attribute OTHER, meaning that they do not participate in the relation. If a relation holds between the two candidate interacting entities the example is labeled 1 or 2 (according to the roles of the interacting entities, agent and target, i.e. to the direction of the relation); 0 otherwise. Figure2shows the examples generated from the sentence in Figure 1.",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "shows the examples generated from the sentence in Figure 1.",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF3": {
                "num": null,
                "text": "Figure 2: The three protein-gene examples generated from the sentence in Figure 1.",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF4": {
                "num": null,
                "text": "Figure 3: Fragment of the AImed corpus with all proteins and their interactions tagged. The protein names have been highlighted in bold face and their same subscript numbers indicate interaction between the proteins.",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "FIGREF5": {
                "num": null,
                "text": "Figure 4: Precision-recall curves on the AImed data set using OARD evaluation methodology.",
                "uris": null,
                "fig_num": "4",
                "type_str": "figure"
            },
            "TABREF2": {
                "content": "<table><tr><td/><td>OAOD</td><td/><td/></tr><tr><td colspan=\"3\">Kernel Precision Recall</td><td>F1</td></tr><tr><td>KGC</td><td>57.7</td><td>60.1</td><td>58.9</td></tr><tr><td>KLC</td><td>37.3</td><td>56.3</td><td>44.9</td></tr><tr><td>KSL</td><td>60.9</td><td>57.2</td><td>59.0</td></tr><tr><td/><td>OARD</td><td/><td/></tr><tr><td colspan=\"3\">Kernel Precision Recall</td><td>F1</td></tr><tr><td>KGC</td><td>58.9</td><td>66.2</td><td>62.2</td></tr><tr><td>KLC</td><td>44.8</td><td>67.8</td><td>54.0</td></tr><tr><td>KSL</td><td>64.5</td><td>63.2</td><td>63.9</td></tr><tr><td>ERK</td><td>65.0</td><td>46.4</td><td>54.2</td></tr></table>",
                "type_str": "table",
                "text": "Performance on the AImed data set using the two evaluation methodologies, OAOD and OARD. cation confidence 7 . The results clearly show that K SL outperforms ERK, especially in term of recall (see Table1).",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td>1</td><td/><td/><td/><td/></tr><tr><td>0.8</td><td/><td/><td/><td/></tr><tr><td>0.6</td><td/><td/><td/><td/></tr><tr><td>F 1</td><td/><td/><td/><td/></tr><tr><td>0.4</td><td/><td/><td/><td/></tr><tr><td>0.2</td><td/><td/><td/><td/></tr><tr><td>0</td><td/><td/><td/><td/></tr><tr><td>0</td><td>50</td><td>100</td><td>150</td><td>200</td></tr><tr><td/><td/><td colspan=\"2\">Number of documents</td><td/></tr><tr><td colspan=\"5\">Figure 5: K SL learning curve on the AImed data</td></tr><tr><td colspan=\"5\">set using OARD evaluation methodology.</td></tr><tr><td>Coref.</td><td/><td colspan=\"2\">Precision Recall</td><td>F1</td></tr><tr><td>all</td><td/><td>56.0</td><td>61.4</td><td>58.6</td></tr><tr><td>with</td><td/><td>29.0</td><td>31.0</td><td>30.0</td></tr><tr><td colspan=\"2\">without</td><td>54.8</td><td>62.9</td><td>58.6</td></tr></table>",
                "type_str": "table",
                "text": "shows the best results obtained at the official competition performed in April 2005. Comparing the results we see that K SL trained on each subset outperforms the best K SL performance on the LLL challenge test set using only the basic linguistic information.",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table><tr><td>Test set</td><td>Coref.</td><td colspan=\"3\">Precision Recall</td><td>F1</td></tr><tr><td colspan=\"2\">Enriched all</td><td>55.6</td><td/><td>53.0</td><td>54.3</td></tr><tr><td/><td>with</td><td>29.0</td><td/><td>31.0</td><td>24.4</td></tr><tr><td/><td>without</td><td>60.9</td><td/><td>46.2</td><td>52.6</td></tr><tr><td>Basic</td><td>all</td><td>n/a</td><td/><td>n/a</td><td>n/a</td></tr><tr><td/><td>with</td><td>14.0</td><td/><td>82.7</td><td>24.0</td></tr><tr><td/><td>without</td><td>50.0</td><td/><td>53.8</td><td>51.8</td></tr><tr><td colspan=\"4\">Kernel Precision Recall</td><td>F1</td></tr><tr><td>KGC</td><td colspan=\"2\">55.1</td><td>66.3</td><td>60.2</td></tr><tr><td>KLC</td><td colspan=\"2\">44.8</td><td>60.1</td><td>53.8</td></tr><tr><td>KSL</td><td colspan=\"2\">62.1</td><td>61.3</td><td>61.7</td></tr></table>",
                "type_str": "table",
                "text": "Best performance on basic and enriched test sets obtained by participants in the official competition at the LLL challenge.",
                "html": null,
                "num": null
            }
        }
    }
}