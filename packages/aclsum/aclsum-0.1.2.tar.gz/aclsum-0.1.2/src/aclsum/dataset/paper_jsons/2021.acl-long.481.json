{
    "paper_id": "2021",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:00:14.844687Z"
    },
    "title": "Attend What You Need: Motion-Appearance Synergistic Networks for Video Question Answering",
    "authors": [
        {
            "first": "Ahjeong",
            "middle": [],
            "last": "Seo",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Seoul National University",
                "location": {}
            },
            "email": "ajseo@bi.snu.ac.kr"
        },
        {
            "first": "Gi-Cheon",
            "middle": [],
            "last": "Kang",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Seoul National University",
                "location": {}
            },
            "email": "gckang@bi.snu.ac.kr"
        },
        {
            "first": "Joonhan",
            "middle": [],
            "last": "Park",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Hanyang University",
                "location": {}
            },
            "email": "jhpark@bi.snu.ac.kr"
        },
        {
            "first": "Byoung-Tak",
            "middle": [],
            "last": "Zhang",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Seoul National University",
                "location": {}
            },
            "email": "btzhang@bi.snu.ac.kr"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Video Question Answering is a task which requires an AI agent to answer questions grounded in video. This task entails three key challenges: (1) understand the intention of various questions, (2) capturing various elements of the input video (e.g., object, action, causality), and (3) cross-modal grounding between language and vision information. We propose Motion-Appearance Synergistic Networks (MASN), which embed two crossmodal features grounded on motion and appearance information and selectively utilize them depending on the question's intentions. MASN consists of a motion module, an appearance module, and a motion-appearance fusion module. The motion module computes the action-oriented cross-modal joint representations, while the appearance module focuses on the appearance aspect of the input video. Finally, the motion-appearance fusion module takes each output of the motion module and the appearance module as input, and performs question-guided fusion. As a result, MASN achieves new state-of-the-art performance on the TGIF-QA and MSVD-QA datasets. We also conduct qualitative analysis by visualizing the inference results of MASN. The code is available at https://github.com/ ahjeongseo/MASN-pytorch.",
    "pdf_parse": {
        "paper_id": "2021",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Video Question Answering is a task which requires an AI agent to answer questions grounded in video. This task entails three key challenges: (1) understand the intention of various questions, (2) capturing various elements of the input video (e.g., object, action, causality), and (3) cross-modal grounding between language and vision information. We propose Motion-Appearance Synergistic Networks (MASN), which embed two crossmodal features grounded on motion and appearance information and selectively utilize them depending on the question's intentions. MASN consists of a motion module, an appearance module, and a motion-appearance fusion module. The motion module computes the action-oriented cross-modal joint representations, while the appearance module focuses on the appearance aspect of the input video. Finally, the motion-appearance fusion module takes each output of the motion module and the appearance module as input, and performs question-guided fusion. As a result, MASN achieves new state-of-the-art performance on the TGIF-QA and MSVD-QA datasets. We also conduct qualitative analysis by visualizing the inference results of MASN. The code is available at https://github.com/ ahjeongseo/MASN-pytorch.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Recently, research in natural language processing and computer vision has made significant progress in artificial intelligence (AI). Thanks to this, visionlanguage tasks such as image captioning (Xu et al., 2015) , visual question answering (VQA) (Antol et al., 2015; Goyal et al., 2017) , and visual commonsense reasoning (VCR) (Zellers et al., 2019) have been introduced to the research community, along with some benchmark datasets. In particular, video question answering (video QA) tasks (Xu et al., 2016; Jang et al., 2017; Lei et al., 2018; Yu et al., 2019; Choi et al., 2020) have been proposed with the goal of reasoning over higher-level visionlanguage interactions. In contrast to QA tasks based on static images, the questions presented in the video QA dataset vary from frame-level questions regarding the appearance of objects (e.g., what is the color of the hat?) to questions regarding action and causality (e.g., what does the man do after opening a door?).",
                "cite_spans": [
                    {
                        "start": 195,
                        "end": 212,
                        "text": "(Xu et al., 2015)",
                        "ref_id": "BIBREF39"
                    },
                    {
                        "start": 247,
                        "end": 267,
                        "text": "(Antol et al., 2015;",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 268,
                        "end": 287,
                        "text": "Goyal et al., 2017)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 329,
                        "end": 351,
                        "text": "(Zellers et al., 2019)",
                        "ref_id": "BIBREF42"
                    },
                    {
                        "start": 493,
                        "end": 510,
                        "text": "(Xu et al., 2016;",
                        "ref_id": "BIBREF38"
                    },
                    {
                        "start": 511,
                        "end": 529,
                        "text": "Jang et al., 2017;",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 530,
                        "end": 547,
                        "text": "Lei et al., 2018;",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 548,
                        "end": 564,
                        "text": "Yu et al., 2019;",
                        "ref_id": "BIBREF41"
                    },
                    {
                        "start": 565,
                        "end": 583,
                        "text": "Choi et al., 2020)",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "There are three crucial challenges in video QA: (1) understand the intention of various questions, (2) capturing various elements of the input video (e.g., object, action, and causality), and (3) crossmodal grounding between language and vision information. To tackle these challenges, previous studies (Li et al., 2019; Jiang et al., 2020; Huang et al., 2020) have mainly explored this task by jointly embedding the features from the pre-trained word embedding model (Pennington et al., 2014) and the object detection models (He et al., 2016; Ren et al., 2016) . However, as discussed in (Gao et al., 2018) , the use of the visual features extracted from the object detection models suffers from motion analysis since the object detection model lacks temporal modeling. To enforce the motion analysis, a few approaches (Xu et al., 2017; Gao et al., 2018) have employed additional visual features (Tran et al., 2015) (i.e., motion features) which were widely used in the action recognition domain, but their reasoning capability is still limited. They typically employed recurrent models (e.g., LSTM) to embed a long sequence of the visual features. Due to the problem of long-term dependency in recurrent models (Bengio et al., 1993) , their proposed methods may fail to learn dependencies between distant features.",
                "cite_spans": [
                    {
                        "start": 303,
                        "end": 320,
                        "text": "(Li et al., 2019;",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 321,
                        "end": 340,
                        "text": "Jiang et al., 2020;",
                        "ref_id": null
                    },
                    {
                        "start": 341,
                        "end": 360,
                        "text": "Huang et al., 2020)",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 468,
                        "end": 493,
                        "text": "(Pennington et al., 2014)",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 526,
                        "end": 543,
                        "text": "(He et al., 2016;",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 544,
                        "end": 561,
                        "text": "Ren et al., 2016)",
                        "ref_id": null
                    },
                    {
                        "start": 589,
                        "end": 607,
                        "text": "(Gao et al., 2018)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 820,
                        "end": 837,
                        "text": "(Xu et al., 2017;",
                        "ref_id": "BIBREF37"
                    },
                    {
                        "start": 838,
                        "end": 855,
                        "text": "Gao et al., 2018)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 897,
                        "end": 916,
                        "text": "(Tran et al., 2015)",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 1213,
                        "end": 1234,
                        "text": "(Bengio et al., 1993)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this paper, we propose Motion-Appearance The output from the fusion module is used to derive answers. For question features, the word-level representation F Q is integrated with the visual features in the VQ interaction submodule. The last hidden units q from the bi-LSTM are used to combine appearance and motion features.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Synergistic Networks (MASN) for video question answering which consist of three kinds of modules: the motion module, the appearance module, and the motion-appearance fusion module. As shown in Figure 1 , the motion module and the appearance module aim to embed rich cross-modal representations. These two modules have the same architecture except that the motion module takes the motion features extracted from I3D as visual features and the appearance module utilizes the appearance features extracted from ResNet. Each of these modules first constructs the object graphs via graph convolutional networks (GCN) to compute the relationships among objects in each visual feature. Then, the vision-question interaction module performs cross-modal grounding between the output of the GCNs and the question features. The motion module and the appearance module each yield cross-modal representations of the motion and the appearance aspects of the input video respectively. The motion-appearance fusion module finally integrates these two features based on the question features.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 200,
                        "end": 201,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The main contributions of our paper are as follows. First, we propose Motion-Appearance Synergistic Networks (MASN) for video question answering based on three modules, the motion module, the appearance module, and the motionappearance fusion module. Second, we validate MASN on the large-scale video question answering datasets TGIF-QA, MSVD-QA, and MSRVTT-QA.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "MASN achieves the new state-of-the-art performance on TGIF-QA and MSVD-QA. We perform ablation studies to validate the effectiveness of our proposed methods. Finally, we conduct a qualitative analysis of MASN by visualizing inference results.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Visual Question Answering (VQA) is a task that requires both understanding questions and finding clues from visual information. VQA can be classified into two categories based on the type of the visual source: image QA and video QA. In image QA, earlier works approach the task by applying attention between the question and the spatial dimensions of the image (Yang et al., 2016; Anderson et al., 2018; Kim et al., 2018a; Kang et al., 2019) . In video QA, since a video is represented as a sequence of images over time, recognizing the movement of objects or causality in the temporal dimension should also be considered along with the details from the spatial dimension (Jang et al., 2017; On et al., 2020) . There have been some attempts (Xu et al., 2017; Gao et al., 2018; Fan et al., 2019) to extract motion and appearance features and integrate them on a spatio-temporal dimension via memory networks. Li et al. (2019) , Huang et al. (2020) , Jiang et al. (2020) proposed better performing models using attention in order to overcome the long-range dependency problem in memory networks. However, they do not represent motion in-formation sufficiently since they only use features pre-trained on image or object classification. To better address this, we model spatio-temporal reasoning on multiple visual information (i.e., ResNet, I3D) while also solving the long-range dependency problem that occurred in previous studies. Action Classification is a task of recognizing actions, which are composed of interactions between actors and objects. Therefore, this task has much in common with video QA, in that the model should perform spatio-temporal reasoning. For better spatio-temporal reasoning, Tran et al. (2015) introduced C3D, which extends the 2D CNN filters to the temporal dimension. Carreira and Zisserman (2017) proposed I3D, which integrates 3D convolutions into a state-of-the-art 2D CNN architecture, which now acts as a baseline in action classification tasks (Murray et al., 2012; Girdhar et al., 2018) . Feichtenhofer et al. (2019) introduced SlowFast, a network which encodes images in two streams with different frame rates and temporal resolutions of convolution. This study based on a two-stream architecture inspired us in terms of assigning different inputs to each encoder module. However, our method differs from the former studies in two aspects: (1) we utilize language features as well as vision features, and (2) we expand the two-stream structure to solve more than motion-oriented tasks. Attention Mechanism explicitly calculates the correlation between two features (Bahdanau et al., 2015; Lin et al., 2017) , and has been widely used in a variety of fields. For machine translation, the Transformer architecture first introduced by Vaswani et al. (2017) , utilizes multi-head selfattention that captures diverse aspects in the input features (Voita et al., 2019) . For video QA, Kim et al. (2018b) ; Li et al. (2019) use self and guidedattention to encode temporal dynamics in video and ground them in the question. For multi-modal alignment, Tsai et al. (2019) apply the Transformer to merge cross-modal time series between vision, language, and audio features. We utilize the attention mechanism to capture various relations between appearance and motion and to aggregate them.",
                "cite_spans": [
                    {
                        "start": 361,
                        "end": 380,
                        "text": "(Yang et al., 2016;",
                        "ref_id": "BIBREF40"
                    },
                    {
                        "start": 381,
                        "end": 403,
                        "text": "Anderson et al., 2018;",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 404,
                        "end": 422,
                        "text": "Kim et al., 2018a;",
                        "ref_id": null
                    },
                    {
                        "start": 423,
                        "end": 441,
                        "text": "Kang et al., 2019)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 672,
                        "end": 691,
                        "text": "(Jang et al., 2017;",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 692,
                        "end": 708,
                        "text": "On et al., 2020)",
                        "ref_id": null
                    },
                    {
                        "start": 741,
                        "end": 758,
                        "text": "(Xu et al., 2017;",
                        "ref_id": "BIBREF37"
                    },
                    {
                        "start": 759,
                        "end": 776,
                        "text": "Gao et al., 2018;",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 777,
                        "end": 794,
                        "text": "Fan et al., 2019)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 908,
                        "end": 924,
                        "text": "Li et al. (2019)",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 927,
                        "end": 946,
                        "text": "Huang et al. (2020)",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 949,
                        "end": 968,
                        "text": "Jiang et al. (2020)",
                        "ref_id": null
                    },
                    {
                        "start": 1704,
                        "end": 1722,
                        "text": "Tran et al. (2015)",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 1799,
                        "end": 1828,
                        "text": "Carreira and Zisserman (2017)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 1981,
                        "end": 2002,
                        "text": "(Murray et al., 2012;",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 2003,
                        "end": 2024,
                        "text": "Girdhar et al., 2018)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 2027,
                        "end": 2054,
                        "text": "Feichtenhofer et al. (2019)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 2604,
                        "end": 2627,
                        "text": "(Bahdanau et al., 2015;",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 2628,
                        "end": 2645,
                        "text": "Lin et al., 2017)",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 2771,
                        "end": 2792,
                        "text": "Vaswani et al. (2017)",
                        "ref_id": "BIBREF35"
                    },
                    {
                        "start": 2881,
                        "end": 2901,
                        "text": "(Voita et al., 2019)",
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 2918,
                        "end": 2936,
                        "text": "Kim et al. (2018b)",
                        "ref_id": null
                    },
                    {
                        "start": 2939,
                        "end": 2955,
                        "text": "Li et al. (2019)",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 3082,
                        "end": 3100,
                        "text": "Tsai et al. (2019)",
                        "ref_id": "BIBREF34"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "In this section, we introduce a detailed description of our MASN network. First, we explain how to obtain appearance and motion features in Section 3.1. Then, we describe the Appearance and Motion modules, which encode visual features and com-bine them with the question in Section 3.2. Finally, the Motion-Appearance Fusion module modulates the amount of motion and appearance information utilized and integrates them based on question context.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model",
                "sec_num": "3"
            },
            {
                "text": "We first extract appearance and motion features from the video frames. For the appearance representation, we use ResNet (He et al., 2016) pre-trained on an object and its attribute classification task as a feature extractor. For the motion representation, we use I3D (Carreira and Zisserman, 2017) pretrained on the action classification task. We obtain local features representing object-level information without background noise and global features representing each frame's context for both appearance and motion features.",
                "cite_spans": [
                    {
                        "start": 120,
                        "end": 137,
                        "text": "(He et al., 2016)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 267,
                        "end": 297,
                        "text": "(Carreira and Zisserman, 2017)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Visual and Linguistic Representation",
                "sec_num": "3.1"
            },
            {
                "text": "Appearance Representation. For local features, given a video containing T frames, we obtain N objects from each frame using Faster R-CNN (Ren et al., 2016) that applies RoIAlign to extract the region of interest from ResNet's convolutional layer. We denote the appearance-object set as",
                "cite_spans": [
                    {
                        "start": 137,
                        "end": 155,
                        "text": "(Ren et al., 2016)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Visual and Linguistic Representation",
                "sec_num": "3.1"
            },
            {
                "text": "R a = {o a t,n , b t,n } t=T,n=N t=1,n=1",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Visual and Linguistic Representation",
                "sec_num": "3.1"
            },
            {
                "text": ", where o, b indicate object feature and bounding box location, respectively. Therefore, there are K = N \u00d7 T objects in a single video. Following previous works, we extract the feature map from ResNet-152's Conv5 layer and apply a linear projection (Jiang et al., 2020; Huang et al., 2020) . We denote global features as v a global \u2208 R T \u00d7d , where d is the size of the hidden dimension.",
                "cite_spans": [
                    {
                        "start": 249,
                        "end": 269,
                        "text": "(Jiang et al., 2020;",
                        "ref_id": null
                    },
                    {
                        "start": 270,
                        "end": 289,
                        "text": "Huang et al., 2020)",
                        "ref_id": "BIBREF15"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Visual and Linguistic Representation",
                "sec_num": "3.1"
            },
            {
                "text": "Motion Representation. We obtain a feature map from the last convolutional layer in I3D (Carreira and Zisserman, 2017) whose dimension is (time, width, height, feature) = ( t 8 , 7, 7, 2048). That is, each set of 8 frames is represented as a single feature map with dimension 7 \u00d7 7 \u00d7 2048. For local features, we apply RoIAlign (He et al., 2017) on the feature map using object bounding box location b. We define the motion-object set as R m = {o m t,n , b t,n } t=T,n=N t=1,n=1 . We apply average pooling in the feature map and linear projection to obtain global features v m global \u2208 R T \u00d7d .",
                "cite_spans": [
                    {
                        "start": 328,
                        "end": 345,
                        "text": "(He et al., 2017)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Visual and Linguistic Representation",
                "sec_num": "3.1"
            },
            {
                "text": "Location Encoding. To reason about relations between objects as in Section 3.2, it is required to consider each object's spatial and temporal location. As appearance and motion features share identical operations until the Motion-Appearance",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Visual and Linguistic Representation",
                "sec_num": "3.1"
            },
            {
                "text": "Fusion module, we combine superscript a and m for simplicity. Following L-GCN (Huang et al., 2020) , we add a location encoding and define local features as:",
                "cite_spans": [
                    {
                        "start": 78,
                        "end": 98,
                        "text": "(Huang et al., 2020)",
                        "ref_id": "BIBREF15"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Visual and Linguistic Representation",
                "sec_num": "3.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "v a/m local = FFN([o a/m ; d s ; d t ])",
                        "eq_num": "(1)"
                    }
                ],
                "section": "Visual and Linguistic Representation",
                "sec_num": "3.1"
            },
            {
                "text": "where d s = FFN(b) and d t is obtained by position encoding according to each frame's index.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Visual and Linguistic Representation",
                "sec_num": "3.1"
            },
            {
                "text": "Here o a/m denotes the object features mentioned above while FFN denotes a feed-forward network. Analogous to local features, position encoding information d t is added to global features as well.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Visual and Linguistic Representation",
                "sec_num": "3.1"
            },
            {
                "text": "We then concatenate object features with global features to reflect the frame-level context in objects and obtain the visual representation v a/m \u2208 R K\u00d7d :",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Visual and Linguistic Representation",
                "sec_num": "3.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "v a/m = FFN([v a/m local ; v a/m global ])",
                        "eq_num": "(2)"
                    }
                ],
                "section": "Visual and Linguistic Representation",
                "sec_num": "3.1"
            },
            {
                "text": "Linguistic Representation. We apply the pretrained GloVe to convert each question word into a 300-dimensional vector, following previous work (Jang et al., 2017) . To represent contextual information in a sentence, we feed the word representations into a bidirectional LSTM (bi-LSTM). Word-level features and last hidden units from the bi-LSTM are denoted by F q \u2208 R L\u00d7d , and q \u2208 R d respectively.",
                "cite_spans": [
                    {
                        "start": 142,
                        "end": 161,
                        "text": "(Jang et al., 2017)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Visual and Linguistic Representation",
                "sec_num": "3.1"
            },
            {
                "text": "L denotes the number of words in a question.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Visual and Linguistic Representation",
                "sec_num": "3.1"
            },
            {
                "text": "In this section, we explain the modules generating high-level visual representations and integrate them with linguistic representations. Each module consists of (1) an Object Graph: spatio-temporal reasoning between object-level visual features, and (2) VQ interaction: calculating correlations between objects and words and obtaining cross-modal feature embeddings. Since the modules share the same architecture, we describe each module's components only once with a shared superscript to avoid redundancy.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Motion and Appearance Module",
                "sec_num": "3.2"
            },
            {
                "text": "In this section, we define object graphs G a/m = (V a/m , E a/m ) to capture spatio-temporal relations between objects. V, E denotes the node and edge set of the graph. As equation 2 provides visual features v a/m , we define these as the graph input X a/m \u2208 R K\u00d7d . We denote the graph as G a/m . The nodes of graph G a/m are given by v a/m i \u2208 X a/m , and edges are given by (v",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Object Graph Construction",
                "sec_num": "3.2.1"
            },
            {
                "text": "a/m i , v a/m j",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Object Graph Construction",
                "sec_num": "3.2.1"
            },
            {
                "text": "), representing a relationship between the two nodes. Given the constructed graph G, we perform graph convolution (Kipf and Welling, 2016) to obtain the relationaware object features. We obtain the similarity scores of nodes by calculating the dot-product after projecting input features to the interaction space and define the adjacency matrix A a/m \u2208 R K\u00d7K as follows:",
                "cite_spans": [
                    {
                        "start": 114,
                        "end": 138,
                        "text": "(Kipf and Welling, 2016)",
                        "ref_id": "BIBREF23"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Object Graph Construction",
                "sec_num": "3.2.1"
            },
            {
                "text": "A a/m = softmax((X a/m W 1 )(X a/m W 2 ) ) (3)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Object Graph Construction",
                "sec_num": "3.2.1"
            },
            {
                "text": "We denote the two-layer graph convolution on input X with adjacency matrix A as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Object Graph Construction",
                "sec_num": "3.2.1"
            },
            {
                "text": "GCN(X; A) = ReLU(A ReLU(AXW 3 ) W 4 ) F = LayerNorm(X + GCN(X; A))",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Object Graph Construction",
                "sec_num": "3.2.1"
            },
            {
                "text": "(4) We omit superscripts in the graph convolution equation for simplicity. We add a skip connection for residual learning between self-information X and smoothed-information with neighbor objects.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Object Graph Construction",
                "sec_num": "3.2.1"
            },
            {
                "text": "We compute both appearance-question and motionquestion interaction to obtain correlations between language and each of the visual features. As we encode visual feature F a/m and question feature F q in Equation 4 and Section 3.1, we calculate every pair of relations between two modalities using the bilinear operation introduced in BAN (Kim et al., 2018a) as follows:",
                "cite_spans": [
                    {
                        "start": 337,
                        "end": 356,
                        "text": "(Kim et al., 2018a)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Vision-question (VQ) Interaction",
                "sec_num": "3.2.2"
            },
            {
                "text": "H i = 1 \u2022 BAN i (H i-1 , V ; A i ) + H i-1 (5)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Vision-question (VQ) Interaction",
                "sec_num": "3.2.2"
            },
            {
                "text": "where H 0 = F q , 1 \u2208 R L , 1 \u2264 i \u2264 g and A denotes the attention map. F a/m is substituted for V respectively in our method. In the equation above, calculating the result BAN(H, V ; A) \u2208 R d and adding it to the H is repeated in g times. Afterwards, H represents the combined visual and language features in the question space incorporating diverse aspects from the two modalities (Yang et al., 2016) .",
                "cite_spans": [
                    {
                        "start": 382,
                        "end": 401,
                        "text": "(Yang et al., 2016)",
                        "ref_id": "BIBREF40"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Vision-question (VQ) Interaction",
                "sec_num": "3.2.2"
            },
            {
                "text": "In this section, we introduce the Motion-Appearance Fusion module which is our key contribution. Depending on what the question ultimately asks about, the model is supposed to decide which features are more relevant among appearance and motion information, or a combination of both. To do this, we produce appearance-centered, motion-centered, and all-mixed features and aggregate them depending on question context. Based on the previous step, we obtain cross-modal combined features H a and H m in terms of appearance and motion. We concatenate these two matrices and define U as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Motion-Appearance Fusion",
                "sec_num": "3.3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "U = H a H m , U \u2208 R 2L\u00d7d",
                        "eq_num": "(6)"
                    }
                ],
                "section": "Motion-Appearance Fusion",
                "sec_num": "3.3"
            },
            {
                "text": "Motion-Appearance-centered Attention. We first define regular scaled dot-product attention to attend features to diverse aspects:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Motion-Appearance Fusion",
                "sec_num": "3.3"
            },
            {
                "text": "Attention(Q, K, V ) = softmax( QK \u221a d k )V (7)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Motion-Appearance Fusion",
                "sec_num": "3.3"
            },
            {
                "text": "where Q, K, V denotes the query, key, and value, respectively. To obtain motion-centered, appearance-centered and mixed attention, we substitute U with the query, and H a , H m , U with the key and value in the equation 7 as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Motion-Appearance Fusion",
                "sec_num": "3.3"
            },
            {
                "text": "P a = Attention(U, H a , H a ) P m = Attention(U, H m , H m ) P all = Attention(U, U, U )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Motion-Appearance Fusion",
                "sec_num": "3.3"
            },
            {
                "text": "Z a/m/all = LayerNorm(P a/m/all + U )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Motion-Appearance Fusion",
                "sec_num": "3.3"
            },
            {
                "text": "where P \u2208 R 2L\u00d7d and Z \u2208 R 2L\u00d7d . As in the first line of the equation 8, we add projected appearance features P a on each appearance and motion feature to obtain Z a , since the matrix U is the concatenation of H a and H m . Therefore, we argue that Z a contains appearance-centered information. Similarly, Z m/all contains motioncentered and all-mixed features, respectively. We argue that the Motion-Appearance-centered attention fuses appearance and motion features in various proportions and these three matrices work like multi-head attention sharing the task of capturing diverse information, and become synergistic when combined.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Motion-Appearance Fusion",
                "sec_num": "3.3"
            },
            {
                "text": "Question-Guided Fusion. For question-guided fusion, we first define z a/m/all as the sum of matrix Z a/m/all \u2208 R 2L\u00d7d over sequence length 2L. We obtain attention scores between each z a/m/all and question context vector q:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Motion-Appearance Fusion",
                "sec_num": "3.3"
            },
            {
                "text": "\u03b1 a/m/all = softmax( q(z a/m/all ) \u221a d z ) (9)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Motion-Appearance Fusion",
                "sec_num": "3.3"
            },
            {
                "text": "where q denotes the last hidden vector. The attention score \u03b1 a/m/all can be interpreted as the importance of each matrix Z based on question context. We obtain the question-guided fusion matrix O as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Motion-Appearance Fusion",
                "sec_num": "3.3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "S = \u03b1 a Z a + \u03b1 m Z m + \u03b1 all Z all O = LayerNorm(S + FFN(S))",
                        "eq_num": "(10)"
                    }
                ],
                "section": "Motion-Appearance Fusion",
                "sec_num": "3.3"
            },
            {
                "text": "where O \u2208 R 2L\u00d7d is obtained by linear transformation and a residual connection after weighted sum. We aggregate information by attention over the sequence length of O:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Motion-Appearance Fusion",
                "sec_num": "3.3"
            },
            {
                "text": "\u03b2 i = softmax(FFN(O i )) f = 2L i=1 \u03b2 i O i (11)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Motion-Appearance Fusion",
                "sec_num": "3.3"
            },
            {
                "text": "The final output vector f \u2208 R d is used for answer prediction.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Motion-Appearance Fusion",
                "sec_num": "3.3"
            },
            {
                "text": "The video QA task can be divided into counting, open-ended word, and multiple-choice tasks (Jang et al., 2017) . Our method trains the model and predicts the answer based on the three tasks similar to previous work. The counting task is formulated as a linear regression of the final output vector f . We obtain the final answer by rounding the result and we minimize Mean Squared Error (MSE) loss.",
                "cite_spans": [
                    {
                        "start": 91,
                        "end": 110,
                        "text": "(Jang et al., 2017)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Answer Prediction and Loss Function",
                "sec_num": "3.4"
            },
            {
                "text": "The open-ended word task is essentially a classification task over the whole answer set. We calculate a classification score by applying a linear classifier and softmax function on the final output f and train the model by minimizing cross-entropy loss.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Answer Prediction and Loss Function",
                "sec_num": "3.4"
            },
            {
                "text": "For the multiple-choice task, like in previous work (Jang et al., 2017) , we attach an answer to the question and obtain M candidates. Then, we obtain the score for each of the M candidates by a linear transformation to the output vector f . We minimize the hinge loss within every pair of candidates, max(0, 1 + s n -s p ), where s n and s p are scores from incorrect and correct answers respectively.",
                "cite_spans": [
                    {
                        "start": 52,
                        "end": 71,
                        "text": "(Jang et al., 2017)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Answer Prediction and Loss Function",
                "sec_num": "3.4"
            },
            {
                "text": "In this section, we evaluate our proposed model on three Video QA datasets: TGIF-QA, MSVD-QA, and MSRVTT-QA. We first introduce each dataset and compare our results with the state-of-the-art methods. Then, we report ablation studies and include visualizations to show how each module in MASN works.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "4"
            },
            {
                "text": "TGIF-QA (Jang et al., 2017 ) is a large-scale dataset that consists of 165K QA pairs collected from 72K animated GIFs. The length of video clips is very short, in general. TGIF-QA consists of four types of tasks: Count, Action, State transition (Trans.), and FrameQA.",
                "cite_spans": [
                    {
                        "start": 8,
                        "end": 26,
                        "text": "(Jang et al., 2017",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Datasets",
                "sec_num": "4.1"
            },
            {
                "text": "Count is an open-ended question to count how many times an action repeats. Action is a task to find action repeated at certain times, and Transition aims to identify a state transition over time. Both types are multiple-choice tasks. Lastly, FrameQA is an open-ended question that can be solved from just one frame, similar to image QA.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Datasets",
                "sec_num": "4.1"
            },
            {
                "text": "MSVD-QA & MSRVTT-QA (Xu et al., 2017) ",
                "cite_spans": [
                    {
                        "start": 20,
                        "end": 37,
                        "text": "(Xu et al., 2017)",
                        "ref_id": "BIBREF37"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Datasets",
                "sec_num": "4.1"
            },
            {
                "text": "We first extract frames with 6 fps for all datasets.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementation Details",
                "sec_num": "4.2"
            },
            {
                "text": "In the case of appearance features, we sample 1 frame out of 4 to avoid information redundancy. We apply Faster R-CNN (Ren et al., 2016) pretrained on Visual Genome (Krishna et al., 2017) to obtain local features. The number of extracted objects is N = 10. For global features, we use ResNet-152 pre-trained on ImageNet (Deng et al., 2009) . In the the case of motion features, we apply I3D pre-trained on the Kinetics action recognition dataset (Kay et al., 2017) . For the input of I3D, we concatenate a set of 8 frames around the sampled frame mentioned above. In terms of training details, we employ Adam optimizer with learning rate as 10 -4 . The number of BAN glimpse g is 4. We set the batch size as 32 for the Count and FrameQA tasks and 16 for Action and Trans. tasks. ",
                "cite_spans": [
                    {
                        "start": 118,
                        "end": 136,
                        "text": "(Ren et al., 2016)",
                        "ref_id": null
                    },
                    {
                        "start": 165,
                        "end": 187,
                        "text": "(Krishna et al., 2017)",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 320,
                        "end": 339,
                        "text": "(Deng et al., 2009)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 446,
                        "end": 464,
                        "text": "(Kay et al., 2017)",
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Implementation Details",
                "sec_num": "4.2"
            },
            {
                "text": "We compare MASN with state-of-the-art (SoTA) models on the aforementioned datasets.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Comparison with State-of-the-arts",
                "sec_num": "4.3"
            },
            {
                "text": "TGIF-QA. Compared with ST-VQA (Jang et al., 2017) , Co-Mem (Gao et al., 2018) , PSAC (Li et al., 2019) , STA (Gao et al., 2019) , HME (Fan et al., 2019) , and recent SoTA models: HGA, L-GCN, QueST, HCRN (Jiang and Han, 2020; Huang et al., 2020; Jiang et al., 2020; Le et al., 2020) , MASN shows the best results for three tasks: Count, Trans., and Action, outperforming the baseline methods by a large margin as shown in Table 1 . In the case of FrameQA, the performance is similar to QueST. However, considering that there exists some tradeoff between the performance of Count and FrameQA since Count focuses on identifying temporal information and FrameQA focuses on spatial information, MASN shows the best overall performance on the entire task. ",
                "cite_spans": [
                    {
                        "start": 30,
                        "end": 49,
                        "text": "(Jang et al., 2017)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 59,
                        "end": 77,
                        "text": "(Gao et al., 2018)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 85,
                        "end": 102,
                        "text": "(Li et al., 2019)",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 109,
                        "end": 127,
                        "text": "(Gao et al., 2019)",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 134,
                        "end": 152,
                        "text": "(Fan et al., 2019)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 203,
                        "end": 224,
                        "text": "(Jiang and Han, 2020;",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 225,
                        "end": 244,
                        "text": "Huang et al., 2020;",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 245,
                        "end": 264,
                        "text": "Jiang et al., 2020;",
                        "ref_id": null
                    },
                    {
                        "start": 265,
                        "end": 281,
                        "text": "Le et al., 2020)",
                        "ref_id": "BIBREF25"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 427,
                        "end": 428,
                        "text": "1",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Comparison with State-of-the-arts",
                "sec_num": "4.3"
            },
            {
                "text": "Analyzing the impact of motion module and appearance module. We investigate the effect of each module as seen in Figure 1 . In Table 3 , the 1 st and 2 nd row represent the result of using only the Appearance and Motion module, respectively. The 3 rd row shows the result of just concatenating appearance and motion features from each module and flattening them, by substituting the input X for O in equation 11. Most existing SOTA models utilize only ResNet features for spatio-temporal reasoning based on the difference of vectors over time. Using only the Appearance module is similar to most of these existing methods, which can catch spatio-temporal relations relatively well. On the other hand, we found that the accuracy on FrameQA when only using the Motion module is about 7% lower than when using the Appearance module. This means the Motion module is limited in its ability to capture the appearance details. However, comparing the 1 st and 3 rd row in Table 3 , the performance in the Action and Trans. tasks increase consistently when the Motion module is added compared to using only the Appearance module. This indicates that the Motion module is a meaningful addition. Lastly, compared to the 1 st , 2 nd and 3 rd row, when integrating the output from both modules there is a further overall performance improvement. This indicates a synergistic effect occurs when integrating both the appearance and motion feature after obtaining them as high-level features.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 120,
                        "end": 121,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    },
                    {
                        "start": 133,
                        "end": 134,
                        "text": "3",
                        "ref_id": "TABREF1"
                    },
                    {
                        "start": 970,
                        "end": 971,
                        "text": "3",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Ablation Study",
                "sec_num": "4.4"
            },
            {
                "text": "Analyzing the impact of fusion module. We show ablation studies inside the fusion module represented in Table 3 . The 4 th row indicates the performance of our proposed MASN architecture.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 110,
                        "end": 111,
                        "text": "3",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Ablation Study",
                "sec_num": "4.4"
            },
            {
                "text": "The results in the 'Single-Attention Fusion' row use only one type of attention among appearancecentered, motion-centered, and all-mixed as seen in equation 8. The results in the 'Dual-Attention Fusion' row utilize two among the three types of attention mentioned above. Due to the nature of video, when a question such as \"How many times does the man in the white shirt put his hand on the head?\" is given, the model is supposed to find the motion information \"put\" while catching the appearance information \"man in white shirt\" or \"hand on head\", and finally mixing them in different proportions depending on the context of question. Comparing the result of the 3 rd (without fusion) row and MASN first, MASN shows better performance across tasks. This means mixing appearance and motion features in various proportions using the Motion-Appearance-centered Fusion module and computing the weighted fusion via the Question-Guided Fusion module contributes to the performance. When comparing the general performance with the number of attention types in fusion module, using single, dual, and triple attention (MASN) shows increasingly better performance in the same order. This indicates that focusing on different aspects and integrating each attended feature performs better than calculating attention at once. Additionally, comparing the result of using only appearance or motion-centered attention in 'Single' with both of them in 'Dual', we find that using both features shows better performance, which means they play complementary roles for each other. Similarly, we argue the reason for the performance increase in FrameQA in the 'Motion' row of 'Single-Att. Fusion' is due to the fact that the model can find relevant appearance information better based on motion information.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Ablation Study",
                "sec_num": "4.4"
            },
            {
                "text": "We give examples of each attention score matrix from Motion-Appearance Fusion module in Figure 3 . We draw two conclusions from the Figure : (1) each attention map catches different relations similarly to multi-head attention, (2) each attention map is used to a different extend depending on the type of task. For example, in FrameQA, the appearancecentered's attention map captures which appearance trait to find focusing on 'how many'. On the other hand, the motion-centered's and all-mixed's attention map attend on 'waving' or 'hands' to catch motion-related information. In Action, similar to FrameQA, the appearance-centered's attention map attends on 'head' which is the object of action, while the motion-centered's attention map catch 'nod' which is related to movement. However, in the case of the Count task, the two attention weights are not as sparse as scores in the other tasks. We think this dense attention map causes the inconsistency in the performance increase between Count task and Action and Trans. task, although questions for all of these three tasks ask for motion information.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 95,
                        "end": 96,
                        "text": "3",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Qualitative Results",
                "sec_num": "4.5"
            },
            {
                "text": "In this paper, we proposed a Motion-Appearance Synergistic Networks to fuse and create a synergy between motion and appearance features. Through the Motion and Appearance modules, MASN manages to find motion and appearance clues to solve the question, while modulating the amount of information used of each type through the Fusion module. Experimental results on three benchmark datasets show the effectiveness of our proposed MASN architecture compared to other models.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "5"
            }
        ],
        "back_matter": [
            {
                "text": "Acknowledgement The authors would like to thank Ho-Joon Song, Yu-Jung Heo, Bjorn Bebensee, Seonil Son, Kyoung-Woon On, Seongho Choi and Woo-Suk Choi for helpful comments and editing. This work was partly supported by the Institute of Information & Communications Technology Planning & Evaluation (2015-0-00310-SW.StarLab/25%, 2017-0-01772-VTT/25%, 2018-0-00622-RMI/25%, 2019-0-01371-BabyMind/25%) grant funded by the Korean government.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "acknowledgement",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Bottom-up and top-down attention for image captioning and visual question answering",
                "authors": [
                    {
                        "first": "Peter",
                        "middle": [],
                        "last": "Anderson",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaodong",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Buehler",
                        "suffix": ""
                    },
                    {
                        "first": "Damien",
                        "middle": [],
                        "last": "Teney",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Johnson",
                        "suffix": ""
                    },
                    {
                        "first": "Stephen",
                        "middle": [],
                        "last": "Gould",
                        "suffix": ""
                    },
                    {
                        "first": "Lei",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
                "volume": "",
                "issue": "",
                "pages": "6077--6086",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. 2018. Bottom-up and top-down attention for image captioning and visual question answering. In Proceedings of the IEEE conference on computer vi- sion and pattern recognition, pages 6077-6086.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Vqa: Visual question answering",
                "authors": [
                    {
                        "first": "Stanislaw",
                        "middle": [],
                        "last": "Antol",
                        "suffix": ""
                    },
                    {
                        "first": "Aishwarya",
                        "middle": [],
                        "last": "Agrawal",
                        "suffix": ""
                    },
                    {
                        "first": "Jiasen",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    },
                    {
                        "first": "Margaret",
                        "middle": [],
                        "last": "Mitchell",
                        "suffix": ""
                    },
                    {
                        "first": "Dhruv",
                        "middle": [],
                        "last": "Batra",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Lawrence Zitnick",
                        "suffix": ""
                    },
                    {
                        "first": "Devi",
                        "middle": [],
                        "last": "Parikh",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the IEEE international conference on computer vision",
                "volume": "",
                "issue": "",
                "pages": "2425--2433",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar- garet Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. 2015. Vqa: Visual question an- swering. In Proceedings of the IEEE international conference on computer vision, pages 2425-2433.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Neural machine translation by jointly learning to align and translate",
                "authors": [
                    {
                        "first": "Dzmitry",
                        "middle": [],
                        "last": "Bahdanau",
                        "suffix": ""
                    },
                    {
                        "first": "Kyung",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Hyun",
                        "middle": [],
                        "last": "Cho",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "3rd International Conference on Learning Representations",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dzmitry Bahdanau, Kyung Hyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In 3rd Inter- national Conference on Learning Representations, ICLR 2015.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "The problem of learning long-term dependencies in recurrent networks",
                "authors": [
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    },
                    {
                        "first": "Paolo",
                        "middle": [],
                        "last": "Frasconi",
                        "suffix": ""
                    },
                    {
                        "first": "Patrice",
                        "middle": [],
                        "last": "Simard",
                        "suffix": ""
                    }
                ],
                "year": 1993,
                "venue": "IEEE international conference on neural networks",
                "volume": "",
                "issue": "",
                "pages": "1183--1188",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yoshua Bengio, Paolo Frasconi, and Patrice Simard. 1993. The problem of learning long-term dependen- cies in recurrent networks. In IEEE international conference on neural networks, pages 1183-1188. IEEE.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Quo vadis, action recognition? a new model and the kinetics dataset",
                "authors": [
                    {
                        "first": "Joao",
                        "middle": [],
                        "last": "Carreira",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Zisserman",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "6299--6308",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Joao Carreira and Andrew Zisserman. 2017. Quo vadis, action recognition? a new model and the kinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6299-6308.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Dramaqa: Character-centered video story understanding with hierarchical qa",
                "authors": [
                    {
                        "first": "Seongho",
                        "middle": [],
                        "last": "Choi",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Kyoung-Woon",
                        "suffix": ""
                    },
                    {
                        "first": "Yu-Jung",
                        "middle": [],
                        "last": "On",
                        "suffix": ""
                    },
                    {
                        "first": "Ahjeong",
                        "middle": [],
                        "last": "Heo",
                        "suffix": ""
                    },
                    {
                        "first": "Youwon",
                        "middle": [],
                        "last": "Seo",
                        "suffix": ""
                    },
                    {
                        "first": "Seungchan",
                        "middle": [],
                        "last": "Jang",
                        "suffix": ""
                    },
                    {
                        "first": "Minsu",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Byoung-Tak",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2005.03356"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Seongho Choi, Kyoung-Woon On, Yu-Jung Heo, Ah- jeong Seo, Youwon Jang, Seungchan Lee, Minsu Lee, and Byoung-Tak Zhang. 2020. Dramaqa: Character-centered video story understanding with hierarchical qa. arXiv preprint arXiv:2005.03356.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Imagenet: A large-scale hierarchical image database",
                "authors": [
                    {
                        "first": "Jia",
                        "middle": [],
                        "last": "Deng",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Dong",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Li-Jia",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Li",
                        "middle": [],
                        "last": "Fei-Fei",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "2009 IEEE conference on computer vision and pattern recognition",
                "volume": "",
                "issue": "",
                "pages": "248--255",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hier- archical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248-255. Ieee.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Heterogeneous memory enhanced multimodal attention model for video question answering",
                "authors": [
                    {
                        "first": "Chenyou",
                        "middle": [],
                        "last": "Fan",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaofan",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Shu",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Wensheng",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Chi",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Heng",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "1999--2007",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chenyou Fan, Xiaofan Zhang, Shu Zhang, Wensheng Wang, Chi Zhang, and Heng Huang. 2019. Het- erogeneous memory enhanced multimodal attention model for video question answering. In Proceedings of the IEEE Conference on Computer Vision and Pat- tern Recognition, pages 1999-2007.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Slowfast networks for video recognition",
                "authors": [
                    {
                        "first": "Christoph",
                        "middle": [],
                        "last": "Feichtenhofer",
                        "suffix": ""
                    },
                    {
                        "first": "Haoqi",
                        "middle": [],
                        "last": "Fan",
                        "suffix": ""
                    },
                    {
                        "first": "Jitendra",
                        "middle": [],
                        "last": "Malik",
                        "suffix": ""
                    },
                    {
                        "first": "Kaiming",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
                "volume": "",
                "issue": "",
                "pages": "6202--6211",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. 2019. Slowfast networks for video recognition. In Proceedings of the IEEE/CVF In- ternational Conference on Computer Vision, pages 6202-6211.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Motion-appearance co-memory networks for video question answering",
                "authors": [
                    {
                        "first": "Jiyang",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Runzhou",
                        "middle": [],
                        "last": "Ge",
                        "suffix": ""
                    },
                    {
                        "first": "Kan",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Ram",
                        "middle": [],
                        "last": "Nevatia",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "6576--6585",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jiyang Gao, Runzhou Ge, Kan Chen, and Ram Neva- tia. 2018. Motion-appearance co-memory networks for video question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6576-6585.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Structured two-stream attention network for video question answering",
                "authors": [
                    {
                        "first": "Lianli",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Pengpeng",
                        "middle": [],
                        "last": "Zeng",
                        "suffix": ""
                    },
                    {
                        "first": "Jingkuan",
                        "middle": [],
                        "last": "Song",
                        "suffix": ""
                    },
                    {
                        "first": "Yuan-Fang",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Wu",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Tao",
                        "middle": [],
                        "last": "Mei",
                        "suffix": ""
                    },
                    {
                        "first": "Heng Tao",
                        "middle": [],
                        "last": "Shen",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "volume": "33",
                "issue": "",
                "pages": "6391--6398",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Lianli Gao, Pengpeng Zeng, Jingkuan Song, Yuan- Fang Li, Wu Liu, Tao Mei, and Heng Tao Shen. 2019. Structured two-stream attention network for video question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 6391-6398.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "A better baseline for ava",
                "authors": [
                    {
                        "first": "Rohit",
                        "middle": [],
                        "last": "Girdhar",
                        "suffix": ""
                    },
                    {
                        "first": "Joao",
                        "middle": [],
                        "last": "Carreira",
                        "suffix": ""
                    },
                    {
                        "first": "Carl",
                        "middle": [],
                        "last": "Doersch",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Zisserman",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1807.10066"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Rohit Girdhar, Joao Carreira, Carl Doersch, and An- drew Zisserman. 2018. A better baseline for ava. arXiv preprint arXiv:1807.10066.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
                "authors": [
                    {
                        "first": "Yash",
                        "middle": [],
                        "last": "Goyal",
                        "suffix": ""
                    },
                    {
                        "first": "Tejas",
                        "middle": [],
                        "last": "Khot",
                        "suffix": ""
                    },
                    {
                        "first": "Douglas",
                        "middle": [],
                        "last": "Summers-Stay",
                        "suffix": ""
                    },
                    {
                        "first": "Dhruv",
                        "middle": [],
                        "last": "Batra",
                        "suffix": ""
                    },
                    {
                        "first": "Devi",
                        "middle": [],
                        "last": "Parikh",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "6904--6913",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017. Making the v in vqa matter: Elevating the role of image under- standing in visual question answering. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6904-6913.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Mask r-cnn",
                "authors": [
                    {
                        "first": "Kaiming",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Georgia",
                        "middle": [],
                        "last": "Gkioxari",
                        "suffix": ""
                    },
                    {
                        "first": "Piotr",
                        "middle": [],
                        "last": "Doll\u00e1r",
                        "suffix": ""
                    },
                    {
                        "first": "Ross",
                        "middle": [],
                        "last": "Girshick",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the IEEE international conference on computer vision",
                "volume": "",
                "issue": "",
                "pages": "2961--2969",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. 2017. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 2961-2969.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Deep residual learning for image recognition",
                "authors": [
                    {
                        "first": "Kaiming",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Xiangyu",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Shaoqing",
                        "middle": [],
                        "last": "Ren",
                        "suffix": ""
                    },
                    {
                        "first": "Jian",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
                "volume": "",
                "issue": "",
                "pages": "770--778",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recog- nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770- 778.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Locationaware graph convolutional networks for video question answering",
                "authors": [
                    {
                        "first": "Deng",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Peihao",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Runhao",
                        "middle": [],
                        "last": "Zeng",
                        "suffix": ""
                    },
                    {
                        "first": "Qing",
                        "middle": [],
                        "last": "Du",
                        "suffix": ""
                    },
                    {
                        "first": "Mingkui",
                        "middle": [],
                        "last": "Tan",
                        "suffix": ""
                    },
                    {
                        "first": "Chuang",
                        "middle": [],
                        "last": "Gan",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "AAAI",
                "volume": "",
                "issue": "",
                "pages": "11021--11028",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Deng Huang, Peihao Chen, Runhao Zeng, Qing Du, Mingkui Tan, and Chuang Gan. 2020. Location- aware graph convolutional networks for video ques- tion answering. In AAAI, pages 11021-11028.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Tgif-qa: Toward spatiotemporal reasoning in visual question answering",
                "authors": [
                    {
                        "first": "Yunseok",
                        "middle": [],
                        "last": "Jang",
                        "suffix": ""
                    },
                    {
                        "first": "Yale",
                        "middle": [],
                        "last": "Song",
                        "suffix": ""
                    },
                    {
                        "first": "Youngjae",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Youngjin",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Gunhee",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "2758--2766",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. 2017. Tgif-qa: Toward spatio- temporal reasoning in visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2758-2766.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Divide and conquer: Question-guided spatio-temporal contextual attention for video question answering",
                "authors": [
                    {
                        "first": "Jianwen",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    },
                    {
                        "first": "Ziqiang",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Haojie",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Xibin",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Yue",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "AAAI",
                "volume": "",
                "issue": "",
                "pages": "11101--11108",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jianwen Jiang, Ziqiang Chen, Haojie Lin, Xibin Zhao, and Yue Gao. 2020. Divide and conquer: Question-guided spatio-temporal contextual atten- tion for video question answering. In AAAI, pages 11101-11108.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Reasoning with heterogeneous graph alignment for video question answering",
                "authors": [
                    {
                        "first": "Pin",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    },
                    {
                        "first": "Yahong",
                        "middle": [],
                        "last": "Han",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "AAAI",
                "volume": "",
                "issue": "",
                "pages": "11109--11116",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Pin Jiang and Yahong Han. 2020. Reasoning with het- erogeneous graph alignment for video question an- swering. In AAAI, pages 11109-11116.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Dual attention networks for visual reference resolution in visual dialog",
                "authors": [
                    {
                        "first": "Gi-Cheon",
                        "middle": [],
                        "last": "Kang",
                        "suffix": ""
                    },
                    {
                        "first": "Jaeseo",
                        "middle": [],
                        "last": "Lim",
                        "suffix": ""
                    },
                    {
                        "first": "Byoung-Tak",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "2024--2033",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Gi-Cheon Kang, Jaeseo Lim, and Byoung-Tak Zhang. 2019. Dual attention networks for visual reference resolution in visual dialog. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing, pages 2024-2033.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "The kinetics human action video dataset",
                "authors": [
                    {
                        "first": "Will",
                        "middle": [],
                        "last": "Kay",
                        "suffix": ""
                    },
                    {
                        "first": "Joao",
                        "middle": [],
                        "last": "Carreira",
                        "suffix": ""
                    },
                    {
                        "first": "Karen",
                        "middle": [],
                        "last": "Simonyan",
                        "suffix": ""
                    },
                    {
                        "first": "Brian",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Chloe",
                        "middle": [],
                        "last": "Hillier",
                        "suffix": ""
                    },
                    {
                        "first": "Sudheendra",
                        "middle": [],
                        "last": "Vijayanarasimhan",
                        "suffix": ""
                    },
                    {
                        "first": "Fabio",
                        "middle": [],
                        "last": "Viola",
                        "suffix": ""
                    },
                    {
                        "first": "Tim",
                        "middle": [],
                        "last": "Green",
                        "suffix": ""
                    },
                    {
                        "first": "Trevor",
                        "middle": [],
                        "last": "Back",
                        "suffix": ""
                    },
                    {
                        "first": "Paul",
                        "middle": [],
                        "last": "Natsev",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1705.06950"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijaya- narasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. 2017. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Multimodal dual attention memory for video story question answering",
                "authors": [
                    {
                        "first": "Kyung-Min",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Seong-Ho",
                        "middle": [],
                        "last": "Choi",
                        "suffix": ""
                    },
                    {
                        "first": "Jin-Hwa",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Byoung-Tak",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the European Conference on Computer Vision (ECCV)",
                "volume": "",
                "issue": "",
                "pages": "673--688",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kyung-Min Kim, Seong-Ho Choi, Jin-Hwa Kim, and Byoung-Tak Zhang. 2018b. Multimodal dual atten- tion memory for video story question answering. In Proceedings of the European Conference on Com- puter Vision (ECCV), pages 673-688.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Semisupervised classification with graph convolutional networks",
                "authors": [
                    {
                        "first": "N",
                        "middle": [],
                        "last": "Thomas",
                        "suffix": ""
                    },
                    {
                        "first": "Max",
                        "middle": [],
                        "last": "Kipf",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Welling",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1609.02907"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Thomas N Kipf and Max Welling. 2016. Semi- supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
                "authors": [
                    {
                        "first": "Ranjay",
                        "middle": [],
                        "last": "Krishna",
                        "suffix": ""
                    },
                    {
                        "first": "Yuke",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "Oliver",
                        "middle": [],
                        "last": "Groth",
                        "suffix": ""
                    },
                    {
                        "first": "Justin",
                        "middle": [],
                        "last": "Johnson",
                        "suffix": ""
                    },
                    {
                        "first": "Kenji",
                        "middle": [],
                        "last": "Hata",
                        "suffix": ""
                    },
                    {
                        "first": "Joshua",
                        "middle": [],
                        "last": "Kravitz",
                        "suffix": ""
                    },
                    {
                        "first": "Stephanie",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Yannis",
                        "middle": [],
                        "last": "Kalantidis",
                        "suffix": ""
                    },
                    {
                        "first": "Li-Jia",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [
                            "A"
                        ],
                        "last": "Shamma",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "International journal of computer vision",
                "volume": "123",
                "issue": "1",
                "pages": "32--73",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John- son, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. 2017. Visual genome: Connecting language and vi- sion using crowdsourced dense image annotations. International journal of computer vision, 123(1):32- 73.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Hierarchical conditional relation networks for video question answering",
                "authors": [
                    {
                        "first": "Thao",
                        "middle": [],
                        "last": "Minh",
                        "suffix": ""
                    },
                    {
                        "first": "Le",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Vuong",
                        "middle": [],
                        "last": "Le",
                        "suffix": ""
                    },
                    {
                        "first": "Svetha",
                        "middle": [],
                        "last": "Venkatesh",
                        "suffix": ""
                    },
                    {
                        "first": "Truyen",
                        "middle": [],
                        "last": "Tran",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "9972--9981",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Thao Minh Le, Vuong Le, Svetha Venkatesh, and Truyen Tran. 2020. Hierarchical conditional relation networks for video question answering. In Proceed- ings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pages 9972-9981.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Tvqa: Localized, compositional video question answering",
                "authors": [
                    {
                        "first": "Jie",
                        "middle": [],
                        "last": "Lei",
                        "suffix": ""
                    },
                    {
                        "first": "Licheng",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Mohit",
                        "middle": [],
                        "last": "Bansal",
                        "suffix": ""
                    },
                    {
                        "first": "Tamara",
                        "middle": [
                            "L"
                        ],
                        "last": "Berg",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1809.01696"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jie Lei, Licheng Yu, Mohit Bansal, and Tamara L Berg. 2018. Tvqa: Localized, compositional video ques- tion answering. arXiv preprint arXiv:1809.01696.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Beyond rnns: Positional self-attention with co-attention for video question answering",
                "authors": [
                    {
                        "first": "Xiangpeng",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Jingkuan",
                        "middle": [],
                        "last": "Song",
                        "suffix": ""
                    },
                    {
                        "first": "Lianli",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Xianglong",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Wenbing",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Xiangnan",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Chuang",
                        "middle": [],
                        "last": "Gan",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "volume": "33",
                "issue": "",
                "pages": "8658--8665",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xiangpeng Li, Jingkuan Song, Lianli Gao, Xianglong Liu, Wenbing Huang, Xiangnan He, and Chuang Gan. 2019. Beyond rnns: Positional self-attention with co-attention for video question answering. In Proceedings of the AAAI Conference on Artificial In- telligence, volume 33, pages 8658-8665.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "A structured self-attentive sentence embedding",
                "authors": [
                    {
                        "first": "Zhouhan",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Minwei",
                        "middle": [],
                        "last": "Feng",
                        "suffix": ""
                    },
                    {
                        "first": "Cicero",
                        "middle": [],
                        "last": "Nogueira Dos Santos",
                        "suffix": ""
                    },
                    {
                        "first": "Mo",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Bing",
                        "middle": [],
                        "last": "Xiang",
                        "suffix": ""
                    },
                    {
                        "first": "Bowen",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1703.03130"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Zhouhan Lin, Minwei Feng, Cicero Nogueira dos San- tos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. 2017. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Ava: A large-scale database for aesthetic visual analysis",
                "authors": [
                    {
                        "first": "Naila",
                        "middle": [],
                        "last": "Murray",
                        "suffix": ""
                    },
                    {
                        "first": "Luca",
                        "middle": [],
                        "last": "Marchesotti",
                        "suffix": ""
                    },
                    {
                        "first": "Florent",
                        "middle": [],
                        "last": "Perronnin",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "2012 IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "2408--2415",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Naila Murray, Luca Marchesotti, and Florent Perronnin. 2012. Ava: A large-scale database for aesthetic vi- sual analysis. In 2012 IEEE Conference on Com- puter Vision and Pattern Recognition, pages 2408- 2415. IEEE.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Cut-based graph learning networks to discover compositional structure of sequential video data",
                "authors": [
                    {
                        "first": "Eun-Sol",
                        "middle": [],
                        "last": "Kyoung-Woon On",
                        "suffix": ""
                    },
                    {
                        "first": "Yu-Jung",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Byoung-Tak",
                        "middle": [],
                        "last": "Heo",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "volume": "34",
                "issue": "",
                "pages": "5315--5322",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kyoung-Woon On, Eun-Sol Kim, Yu-Jung Heo, and Byoung-Tak Zhang. 2020. Cut-based graph learn- ing networks to discover compositional structure of sequential video data. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 5315-5322.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Glove: Global vectors for word representation",
                "authors": [
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Pennington",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)",
                "volume": "",
                "issue": "",
                "pages": "1532--1543",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word rep- resentation. In Proceedings of the 2014 conference on empirical methods in natural language process- ing (EMNLP), pages 1532-1543.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Faster r-cnn: towards real-time object detection with region proposal networks",
                "authors": [
                    {
                        "first": "Kaiming",
                        "middle": [],
                        "last": "Shaoqing Ren",
                        "suffix": ""
                    },
                    {
                        "first": "Ross",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Jian",
                        "middle": [],
                        "last": "Girshick",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "IEEE transactions on pattern analysis and machine intelligence",
                "volume": "39",
                "issue": "",
                "pages": "1137--1149",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2016. Faster r-cnn: towards real-time object detection with region proposal networks. IEEE transactions on pattern analysis and machine intelli- gence, 39(6):1137-1149.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "Learning spatiotemporal features with 3d convolutional networks",
                "authors": [
                    {
                        "first": "Du",
                        "middle": [],
                        "last": "Tran",
                        "suffix": ""
                    },
                    {
                        "first": "Lubomir",
                        "middle": [],
                        "last": "Bourdev",
                        "suffix": ""
                    },
                    {
                        "first": "Rob",
                        "middle": [],
                        "last": "Fergus",
                        "suffix": ""
                    },
                    {
                        "first": "Lorenzo",
                        "middle": [],
                        "last": "Torresani",
                        "suffix": ""
                    },
                    {
                        "first": "Manohar",
                        "middle": [],
                        "last": "Paluri",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the IEEE international conference on computer vision",
                "volume": "",
                "issue": "",
                "pages": "4489--4497",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Tor- resani, and Manohar Paluri. 2015. Learning spa- tiotemporal features with 3d convolutional networks. In Proceedings of the IEEE international conference on computer vision, pages 4489-4497.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "Multimodal transformer for unaligned multimodal language sequences",
                "authors": [
                    {
                        "first": "Yao-Hung Hubert",
                        "middle": [],
                        "last": "Tsai",
                        "suffix": ""
                    },
                    {
                        "first": "Shaojie",
                        "middle": [],
                        "last": "Bai",
                        "suffix": ""
                    },
                    {
                        "first": "Paul",
                        "middle": [
                            "Pu"
                        ],
                        "last": "Liang",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Zico Kolter",
                        "suffix": ""
                    },
                    {
                        "first": "Louis-Philippe",
                        "middle": [],
                        "last": "Morency",
                        "suffix": ""
                    },
                    {
                        "first": "Ruslan",
                        "middle": [],
                        "last": "Salakhutdinov",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the conference. Association for Computational Linguistics. Meeting",
                "volume": "2019",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, J Zico Kolter, Louis-Philippe Morency, and Ruslan Salakhutdinov. 2019. Multimodal transformer for unaligned multimodal language sequences. In Pro- ceedings of the conference. Association for Com- putational Linguistics. Meeting, volume 2019, page 6558. NIH Public Access.",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "Attention is all you need",
                "authors": [
                    {
                        "first": "Ashish",
                        "middle": [],
                        "last": "Vaswani",
                        "suffix": ""
                    },
                    {
                        "first": "Noam",
                        "middle": [],
                        "last": "Shazeer",
                        "suffix": ""
                    },
                    {
                        "first": "Niki",
                        "middle": [],
                        "last": "Parmar",
                        "suffix": ""
                    },
                    {
                        "first": "Jakob",
                        "middle": [],
                        "last": "Uszkoreit",
                        "suffix": ""
                    },
                    {
                        "first": "Llion",
                        "middle": [],
                        "last": "Jones",
                        "suffix": ""
                    },
                    {
                        "first": "Aidan",
                        "middle": [
                            "N"
                        ],
                        "last": "Gomez",
                        "suffix": ""
                    },
                    {
                        "first": "Lukasz",
                        "middle": [],
                        "last": "Kaiser",
                        "suffix": ""
                    },
                    {
                        "first": "Illia",
                        "middle": [],
                        "last": "Polosukhin",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1706.03762"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. arXiv preprint arXiv:1706.03762.",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "b36",
                "title": "Analyzing multihead self-attention: Specialized heads do the heavy lifting, the rest can be pruned",
                "authors": [
                    {
                        "first": "Elena",
                        "middle": [],
                        "last": "Voita",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Talbot",
                        "suffix": ""
                    },
                    {
                        "first": "Fedor",
                        "middle": [],
                        "last": "Moiseev",
                        "suffix": ""
                    },
                    {
                        "first": "Rico",
                        "middle": [],
                        "last": "Sennrich",
                        "suffix": ""
                    },
                    {
                        "first": "Ivan",
                        "middle": [],
                        "last": "Titov",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1905.09418"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Elena Voita, David Talbot, Fedor Moiseev, Rico Sen- nrich, and Ivan Titov. 2019. Analyzing multi- head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. arXiv preprint arXiv:1905.09418.",
                "links": null
            },
            "BIBREF37": {
                "ref_id": "b37",
                "title": "Video question answering via gradually refined attention over appearance and motion",
                "authors": [
                    {
                        "first": "Dejing",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Zhou",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Jun",
                        "middle": [],
                        "last": "Xiao",
                        "suffix": ""
                    },
                    {
                        "first": "Fei",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Hanwang",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 25th ACM international conference on Multimedia",
                "volume": "",
                "issue": "",
                "pages": "1645--1653",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. 2017. Video question answering via gradually refined at- tention over appearance and motion. In Proceedings of the 25th ACM international conference on Multi- media, pages 1645-1653.",
                "links": null
            },
            "BIBREF38": {
                "ref_id": "b38",
                "title": "Msrvtt: A large video description dataset for bridging video and language",
                "authors": [
                    {
                        "first": "Jun",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Tao",
                        "middle": [],
                        "last": "Mei",
                        "suffix": ""
                    },
                    {
                        "first": "Ting",
                        "middle": [],
                        "last": "Yao",
                        "suffix": ""
                    },
                    {
                        "first": "Yong",
                        "middle": [],
                        "last": "Rui",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
                "volume": "",
                "issue": "",
                "pages": "5288--5296",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jun Xu, Tao Mei, Ting Yao, and Yong Rui. 2016. Msr- vtt: A large video description dataset for bridging video and language. In Proceedings of the IEEE con- ference on computer vision and pattern recognition, pages 5288-5296.",
                "links": null
            },
            "BIBREF39": {
                "ref_id": "b39",
                "title": "Show, attend and tell: neural image caption generation with visual attention",
                "authors": [
                    {
                        "first": "Kelvin",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Jimmy",
                        "middle": [
                            "Lei"
                        ],
                        "last": "Ba",
                        "suffix": ""
                    },
                    {
                        "first": "Ryan",
                        "middle": [],
                        "last": "Kiros",
                        "suffix": ""
                    },
                    {
                        "first": "Kyunghyun",
                        "middle": [],
                        "last": "Cho",
                        "suffix": ""
                    },
                    {
                        "first": "Aaron",
                        "middle": [],
                        "last": "Courville",
                        "suffix": ""
                    },
                    {
                        "first": "Ruslan",
                        "middle": [],
                        "last": "Salakhutdinov",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [
                            "S"
                        ],
                        "last": "Zemel",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 32nd International Conference on International Conference on Machine Learning",
                "volume": "37",
                "issue": "",
                "pages": "2048--2057",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kelvin Xu, Jimmy Lei Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard S Zemel, and Yoshua Bengio. 2015. Show, attend and tell: neural image caption generation with visual attention. In Proceedings of the 32nd In- ternational Conference on International Conference on Machine Learning-Volume 37, pages 2048-2057.",
                "links": null
            },
            "BIBREF40": {
                "ref_id": "b40",
                "title": "Stacked attention networks for image question answering",
                "authors": [
                    {
                        "first": "Zichao",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaodong",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Jianfeng",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Li",
                        "middle": [],
                        "last": "Deng",
                        "suffix": ""
                    },
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Smola",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
                "volume": "",
                "issue": "",
                "pages": "21--29",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola. 2016. Stacked attention networks for image question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 21-29.",
                "links": null
            },
            "BIBREF41": {
                "ref_id": "b41",
                "title": "Activitynet-qa: A dataset for understanding complex web videos via question answering",
                "authors": [
                    {
                        "first": "Zhou",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Dejing",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Jun",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Ting",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Zhou",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Yueting",
                        "middle": [],
                        "last": "Zhuang",
                        "suffix": ""
                    },
                    {
                        "first": "Dacheng",
                        "middle": [],
                        "last": "Tao",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "volume": "33",
                "issue": "",
                "pages": "9127--9134",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. 2019. Activitynet-qa: A dataset for understanding complex web videos via question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 9127-9134.",
                "links": null
            },
            "BIBREF42": {
                "ref_id": "b42",
                "title": "From recognition to cognition: Visual commonsense reasoning",
                "authors": [
                    {
                        "first": "Rowan",
                        "middle": [],
                        "last": "Zellers",
                        "suffix": ""
                    },
                    {
                        "first": "Yonatan",
                        "middle": [],
                        "last": "Bisk",
                        "suffix": ""
                    },
                    {
                        "first": "Ali",
                        "middle": [],
                        "last": "Farhadi",
                        "suffix": ""
                    },
                    {
                        "first": "Yejin",
                        "middle": [],
                        "last": "Choi",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "6720--6731",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. From recognition to cognition: Vi- sual commonsense reasoning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pat- tern Recognition, pages 6720-6731.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: An overview of MASN. Each extracted feature from ResNet and I3D is fed into the Appearance and Motion modules. Both modules have the same structure with a GCN and VQ interaction submodule. The results from each module are then concatenated and fused in the Motion-Appearance Fusion module.The output from the fusion module is used to derive answers. For question features, the word-level representation F Q is integrated with the visual features in the VQ interaction submodule. The last hidden units q from the bi-LSTM are used to combine appearance and motion features.",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 2: Motion-Appearance Fusion module. The blue-colored elements in a matrix denote appearancequestion, and the pink ones indicate motion-question combined features. Matrices above QK represent an attention score maps from each kind of attention. The final output S in the figure is the weighted-sum matrix of all three attended features.",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Figure 3: Qualitative results on TGIF-QA dataset. From the left, Count and FrameQA are shown in 1st row and Action, Trans. in 2nd row. Each visualized attention map is log-scaled. Scores below attention maps represent \u03b1 from the equation 9.",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td colspan=\"5\">Methods Count Action Trans. FrameQA</td></tr><tr><td>ST-VQA</td><td>4.28</td><td>60.8</td><td>67.1</td><td>49.3</td></tr><tr><td colspan=\"2\">Co-Mem 4.10</td><td>68.2</td><td>74.3</td><td>51.5</td></tr><tr><td>PSAC</td><td>4.27</td><td>70.4</td><td>76.9</td><td>55.7</td></tr><tr><td>STA</td><td>4.25</td><td>72.3</td><td>79.0</td><td>56.6</td></tr><tr><td>HME</td><td>4.02</td><td>73.9</td><td>77.8</td><td>53.8</td></tr><tr><td>HGA</td><td>4.09</td><td>75.4</td><td>81.0</td><td>55.1</td></tr><tr><td>L-GCN</td><td>3.95</td><td>74.3</td><td>81.1</td><td>56.3</td></tr><tr><td>QueST</td><td>4.19</td><td>75.9</td><td>81.0</td><td>59.7</td></tr><tr><td>HCRN</td><td>3.82</td><td>75.0</td><td>81.4</td><td>55.9</td></tr><tr><td>MASN</td><td>3.75</td><td>84.4</td><td>87.4</td><td>59.5</td></tr><tr><td colspan=\"5\">Methods MSVD-QA MSRVTT-QA</td></tr><tr><td>ST-VQA</td><td/><td>31.3</td><td>30.9</td><td/></tr><tr><td>GRA</td><td/><td>32.0</td><td>32.5</td><td/></tr><tr><td>Co-Mem</td><td/><td>31.7</td><td>32.0</td><td/></tr><tr><td>HME</td><td/><td>33.7</td><td>33.0</td><td/></tr><tr><td>HGA</td><td/><td>34.7</td><td>35.5</td><td/></tr><tr><td>QuesT</td><td/><td>36.1</td><td>34.6</td><td/></tr><tr><td>HCRN</td><td/><td>36.1</td><td>35.6</td><td/></tr><tr><td>MASN</td><td/><td>38.0</td><td>35.2</td><td/></tr><tr><td colspan=\"5\">Table 2: State-of-the-art comparison on the MSVD-QA</td></tr><tr><td colspan=\"5\">and MSRVTT-QA datasets. All values represent accu-</td></tr><tr><td colspan=\"5\">racy (%). Best results in bold, underlined results denote</td></tr><tr><td colspan=\"2\">the second best.</td><td/><td/><td/></tr></table>",
                "type_str": "table",
                "text": "are automatically generated from video descriptions. It consists of 1,970 video clips and 50K and 243K QA pairs, respectively. The average video lengths are 10 seconds and 15 seconds respectively. Questions belong to five types: what, who, how, when, and where. The task is open-ended with a pre-defined answer sets of size 1,000 and 4,000, respectively. State-of-the-art comparison on the TGIF-QA dataset. Mean 2 loss for Count, and accuracy (%) for others. Best results in bold, underlined results denote the second best.",
                "html": null,
                "num": null
            },
            "TABREF1": {
                "content": "<table><tr><td>Methods</td><td/><td>Count</td><td>Action</td><td>Trans.</td><td>FrameQA</td></tr><tr><td>Appr. Module</td><td/><td>3.94</td><td>82.9</td><td>86.2</td><td>58.6</td></tr><tr><td>Motion Module</td><td/><td>3.84</td><td>82.5</td><td>86.2</td><td>51.0</td></tr><tr><td colspan=\"2\">Appr. Module + Motion Module</td><td>3.82</td><td>83.4</td><td>86.8</td><td>58.6</td></tr><tr><td colspan=\"2\">Appr. Module + Motion Module + Fusion (Ours)</td><td>3.75</td><td>84.4</td><td>87.4</td><td>59.5</td></tr><tr><td/><td>Appr.</td><td>3.78</td><td>82.8</td><td>86.3</td><td>58.9</td></tr><tr><td>Single-Att. Fusion</td><td>Motion</td><td>3.79</td><td>83.1</td><td>87.0</td><td>59.1</td></tr><tr><td/><td>All</td><td>3.78</td><td>83.6</td><td>87.4</td><td>59.3</td></tr><tr><td colspan=\"2\">Appr. + Motion</td><td>3.77</td><td>83.6</td><td>87.4</td><td>59.2</td></tr><tr><td>Dual-Att. Fusion</td><td>Appr. + All</td><td>3.77</td><td>83.6</td><td>87.5</td><td>59.0</td></tr><tr><td/><td>Motion + All</td><td>3.80</td><td>84.1</td><td>86.5</td><td>59.1</td></tr></table>",
                "type_str": "table",
                "text": "Ablation study on the TGIF-QA dataset. Mean 2 loss for Count, and accuracy (%) for others. Appr. and Att. stand for Appearance and Attention. Best results in bold, underlined results denote the second best.",
                "html": null,
                "num": null
            }
        }
    }
}