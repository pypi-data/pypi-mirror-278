{
    "paper_id": "N16-1181",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:04:50.726453Z"
    },
    "title": "Learning to Compose Neural Networks for Question Answering",
    "authors": [
        {
            "first": "Jacob",
            "middle": [],
            "last": "Andreas",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of California",
                "location": {
                    "settlement": "Berkeley"
                }
            },
            "email": ""
        },
        {
            "first": "Marcus",
            "middle": [],
            "last": "Rohrbach",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of California",
                "location": {
                    "settlement": "Berkeley"
                }
            },
            "email": "rohrbach@eecs.berkeley.edu"
        },
        {
            "first": "Trevor",
            "middle": [],
            "last": "Darrell",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of California",
                "location": {
                    "settlement": "Berkeley"
                }
            },
            "email": "trevor@eecs.berkeley.edu"
        },
        {
            "first": "Dan",
            "middle": [],
            "last": "Klein",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of California",
                "location": {
                    "settlement": "Berkeley"
                }
            },
            "email": "klein@eecs.berkeley.edu"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "We describe a question answering model that applies to both images and structured knowledge bases. The model uses natural language strings to automatically assemble neural networks from a collection of composable modules. Parameters for these modules are learned jointly with network-assembly parameters via reinforcement learning, with only (world, question, answer) triples as supervision. Our approach, which we term a dynamic neural module network, achieves state-of-theart results on benchmark datasets in both visual and structured domains.",
    "pdf_parse": {
        "paper_id": "N16-1181",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "We describe a question answering model that applies to both images and structured knowledge bases. The model uses natural language strings to automatically assemble neural networks from a collection of composable modules. Parameters for these modules are learned jointly with network-assembly parameters via reinforcement learning, with only (world, question, answer) triples as supervision. Our approach, which we term a dynamic neural module network, achieves state-of-theart results on benchmark datasets in both visual and structured domains.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "This paper presents a compositional, attentional model for answering questions about a variety of world representations, including images and structured knowledge bases. The model translates from questions to dynamically assembled neural networks, then applies these networks to world representations (images or knowledge bases) to produce answers. We take advantage of two largely independent lines of work: on one hand, an extensive literature on answering questions by mapping from strings to logical representations of meaning; on the other, a series of recent successes in deep neural models for image recognition and captioning. By constructing neural networks instead of logical forms, our model leverages the best aspects of both linguistic compositionality and continuous representations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Our model has two components, trained jointly: first, a collection of neural \"modules\" that can be freely composed (Figure 1b ); second, a network layout predictor that assembles modules into complete deep networks tailored to each question (Figure 1a ). Previous work has used manually-specified modular structures for visual learning (Andreas et al., 2016) .",
                "cite_spans": [
                    {
                        "start": 336,
                        "end": 358,
                        "text": "(Andreas et al., 2016)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 123,
                        "end": 125,
                        "text": "1b",
                        "ref_id": "FIGREF0"
                    },
                    {
                        "start": 249,
                        "end": 251,
                        "text": "1a",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Here we:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "\u2022 learn a network structure predictor jointly with module parameters themselves \u2022 extend visual primitives from previous work to reason over structured world representations",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Training data consists of (world, question, answer) triples: our approach requires no supervision of network layouts. We achieve state-of-the-art performance on two markedly different question answering tasks: one with questions about natural images, and another with more compositional questions about United States geography.1 ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We begin with a high-level discussion of the kinds of composed networks we would like to learn. Andreas et al. (2016) describe a heuristic approach for decomposing visual question answering tasks into sequence of modular sub-problems. For example, the question What color is the bird? might be answered in two steps: first, \"where is the bird?\" (Figure 2a ), second, \"what color is that part of the image?\" (Figure 2c ). This first step, a generic module called find, can be expressed as a fragment of a neural network that maps from image features and a lexical item (here bird) to a distribution over pixels. This operation is commonly referred to as the attention mechanism, and is a standard tool for manipulating images (Xu et al., 2015) and text representations (Hermann et al., 2015) .",
                "cite_spans": [
                    {
                        "start": 96,
                        "end": 117,
                        "text": "Andreas et al. (2016)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 725,
                        "end": 742,
                        "text": "(Xu et al., 2015)",
                        "ref_id": null
                    },
                    {
                        "start": 768,
                        "end": 790,
                        "text": "(Hermann et al., 2015)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 353,
                        "end": 355,
                        "text": "2a",
                        "ref_id": "FIGREF1"
                    },
                    {
                        "start": 415,
                        "end": 417,
                        "text": "2c",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Deep networks as functional programs",
                "sec_num": "2"
            },
            {
                "text": "The first contribution of this paper is an extension and generalization of this mechanism to enable fully-differentiable reasoning about more structured semantic representations. Figure 2b shows how the same module can be used to focus on the entity Georgia in a non-visual grounding domain; more generally, by representing every entity in the universe of discourse as a feature vector, we can obtain a distribution over entities that corresponds roughly to a logical set-valued denotation.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 186,
                        "end": 188,
                        "text": "2b",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Deep networks as functional programs",
                "sec_num": "2"
            },
            {
                "text": "Having obtained such a distribution, existing neural approaches use it to immediately compute a weighted average of image features and project back into a labeling decision-a describe module (Figure 2c ). But the logical perspective suggests a number of novel modules that might operate on attentions: e.g. combining them (by analogy to conjunction or disjunction) or inspecting them directly without a return to feature space (by analogy to quantification, Figure 2d ). These modules are discussed in detail in Section 4. Unlike their formal counterparts, they are differentiable end-to-end, facilitating their integration into learned models. Building on previous work, we learn behavior for a collection of heterogeneous modules from (world, question, answer) triples.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 199,
                        "end": 201,
                        "text": "2c",
                        "ref_id": "FIGREF1"
                    },
                    {
                        "start": 465,
                        "end": 467,
                        "text": "2d",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Deep networks as functional programs",
                "sec_num": "2"
            },
            {
                "text": "The second contribution of this paper is a model for learning to assemble such modules compositionally. Isolated modules are of limited use-to obtain expressive power comparable to either formal approaches or monolithic deep networks, they must be composed into larger structures. Figure 2 shows simple examples of composed structures, but for realistic question-answering tasks, even larger net- works are required. Thus our goal is to automatically induce variable-free, tree-structured computation descriptors. We can use a familiar functional notation from formal semantics (e.g. Liang et al., 2011) to represent these computations. 2 We write the two examples in Figure 2 as respectively. These are network layouts: they specify a structure for arranging modules (and their lexical parameters) into a complete network. Andreas et al. (2016) use hand-written rules to deterministically transform dependency trees into layouts, and are restricted to producing simple structures like the above for non-synthetic data. For full generality, we will need to solve harder problems, like transforming What cities are in Georgia? (Figure 1 ) into",
                "cite_spans": [
                    {
                        "start": 584,
                        "end": 603,
                        "text": "Liang et al., 2011)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 824,
                        "end": 845,
                        "text": "Andreas et al. (2016)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 288,
                        "end": 289,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    },
                    {
                        "start": 675,
                        "end": 676,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    },
                    {
                        "start": 1134,
                        "end": 1135,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Deep networks as functional programs",
                "sec_num": "2"
            },
            {
                "text": "(and find[city] (relate[in] lookup[Georgia]))",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Deep networks as functional programs",
                "sec_num": "2"
            },
            {
                "text": "In this paper, we present a model for learning to select such structures from a set of automatically generated candidates. We call this model a dynamic neural module network.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Deep networks as functional programs",
                "sec_num": "2"
            },
            {
                "text": "There is an extensive literature on database question answering, in which strings are mapped to logical forms, then evaluated by a black-box execution model to produce answers. Supervision may be provided either by annotated logical forms (Wong and Mooney, 2007; Kwiatkowski et al., 2010; Andreas et al., 2013) or from (world, question, answer) triples alone (Liang et al., 2011; Pasupat and Liang, 2015) . In general the set of primitive functions from which these logical forms can be assembled is fixed, but one recent line of work focuses on inducing new predicates functions automatically, either from perceptual features (Krishnamurthy and Kollar, 2013) or the underlying schema (Kwiatkowski et al., 2013) . The model we describe in this paper has a unified framework for handling both the perceptual and schema cases, and differs from existing work primarily in learning a differentiable execution model with continuous evaluation results.",
                "cite_spans": [
                    {
                        "start": 239,
                        "end": 262,
                        "text": "(Wong and Mooney, 2007;",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 263,
                        "end": 288,
                        "text": "Kwiatkowski et al., 2010;",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 289,
                        "end": 310,
                        "text": "Andreas et al., 2013)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 359,
                        "end": 379,
                        "text": "(Liang et al., 2011;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 380,
                        "end": 404,
                        "text": "Pasupat and Liang, 2015)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 627,
                        "end": 659,
                        "text": "(Krishnamurthy and Kollar, 2013)",
                        "ref_id": null
                    },
                    {
                        "start": 685,
                        "end": 711,
                        "text": "(Kwiatkowski et al., 2013)",
                        "ref_id": "BIBREF12"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "3"
            },
            {
                "text": "Neural models for question answering are also a subject of current interest. These include approaches that model the task directly as a multiclass classification problem (Iyyer et al., 2014) , models that attempt to embed questions and answers in a shared vector space (Bordes et al., 2014) and attentional models that select words from documents sources (Hermann et al., 2015) . Such approaches generally require that answers can be retrieved directly based on surface linguistic features, without requiring intermediate computation. A more structured approach described by Yin et al. (2015) learns a query execution model for database tables without any natural language component. Previous efforts toward unifying formal logic and representation learning include those of Grefenstette (2013) and Krishnamurthy and Mitchell (2013) .",
                "cite_spans": [
                    {
                        "start": 170,
                        "end": 190,
                        "text": "(Iyyer et al., 2014)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 269,
                        "end": 290,
                        "text": "(Bordes et al., 2014)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 355,
                        "end": 377,
                        "text": "(Hermann et al., 2015)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 575,
                        "end": 592,
                        "text": "Yin et al. (2015)",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 775,
                        "end": 794,
                        "text": "Grefenstette (2013)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 799,
                        "end": 832,
                        "text": "Krishnamurthy and Mitchell (2013)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "3"
            },
            {
                "text": "The visually-grounded component of this work relies on recent advances in convolutional networks for computer vision (Simonyan and Zisserman, 2014) , and in particular the fact that late convolutional layers in networks trained for image recognition contain rich features useful for other downstream vision tasks, while preserving spatial information. These features have been used for both image captioning (Xu et al., 2015) and visual question answering (Yang et al., 2015) .",
                "cite_spans": [
                    {
                        "start": 117,
                        "end": 147,
                        "text": "(Simonyan and Zisserman, 2014)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 408,
                        "end": 425,
                        "text": "(Xu et al., 2015)",
                        "ref_id": null
                    },
                    {
                        "start": 456,
                        "end": 475,
                        "text": "(Yang et al., 2015)",
                        "ref_id": "BIBREF25"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "3"
            },
            {
                "text": "Most previous approaches to visual question answering either apply a recurrent model to deep representations of both the image and the question (Ren et al., ; Malinowski et al., 2015) , or use the question to compute an attention over the input image, and then answer based on both the question and the image features attended to (Yang et al., 2015; Xu and Saenko, 2015) . Other approaches include the simple classification model described by Zhou et al. (2015) and the dynamic parameter prediction network described by Noh et al. (2015) . All of these models assume that a fixed computation can be performed on the image and question to compute the answer, rather than adapting the structure of the computation to the question.",
                "cite_spans": [
                    {
                        "start": 144,
                        "end": 158,
                        "text": "(Ren et al., ;",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 159,
                        "end": 183,
                        "text": "Malinowski et al., 2015)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 330,
                        "end": 349,
                        "text": "(Yang et al., 2015;",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 350,
                        "end": 370,
                        "text": "Xu and Saenko, 2015)",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 443,
                        "end": 461,
                        "text": "Zhou et al. (2015)",
                        "ref_id": null
                    },
                    {
                        "start": 520,
                        "end": 537,
                        "text": "Noh et al. (2015)",
                        "ref_id": "BIBREF16"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "3"
            },
            {
                "text": "As noted, Andreas et al. (2016) previously considered a simple generalization of these attentional approaches in which small variations in the network structure per-question were permitted, with the structure chosen by (deterministic) syntactic processing of questions. Other approaches in this general family include the \"universal parser\" sketched by Bottou (2014) , and the recursive neural networks of Socher et al. (2013) , which use a fixed tree structure to perform further linguistic analysis without any external world representation. We are unaware of previous work that succeeds in simultaneously learning both the parameters for and structures of instance-specific neural networks.",
                "cite_spans": [
                    {
                        "start": 10,
                        "end": 31,
                        "text": "Andreas et al. (2016)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 353,
                        "end": 366,
                        "text": "Bottou (2014)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 406,
                        "end": 426,
                        "text": "Socher et al. (2013)",
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "3"
            },
            {
                "text": "Recall that our goal is to map from questions and world representations to answers. This process involves the following variables:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model",
                "sec_num": "4"
            },
            {
                "text": "1. w a world representation 2. x a question 3. y an answer 4. z a network layout 5. \u03b8 a collection of model parameters Our model is built around two distributions: a layout model p(z|x; \u03b8 ) which chooses a layout for a sentence, and a execution model p z (y|w; \u03b8 e ) which applies the network specified by z to w.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model",
                "sec_num": "4"
            },
            {
                "text": "For ease of presentation, we introduce these models in reverse order. We first imagine that z is always observed, and in Section 4.1 describe how to evaluate and learn modules parameterized by \u03b8 e within fixed structures. In Section 4.2, we move to the real scenario, where z is unknown. We describe how to predict layouts from questions and learn \u03b8 e and \u03b8 jointly without layout supervision.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model",
                "sec_num": "4"
            },
            {
                "text": "Given a layout z, we assemble the corresponding modules into a full neural network (Figure 1c ), and apply it to the knowledge representation. Intermediate results flow between modules until an answer is produced at the root. We denote the output of the network with layout z on input world w as z w ; when explicitly referencing the substructure of z, we can alternatively write m(h 1 , h 2 ) for a top-level module m with submodule outputs h 1 and h 2 . We then define the execution model:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 91,
                        "end": 93,
                        "text": "1c",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Evaluating modules",
                "sec_num": "4.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "p z (y|w) = ( z w ) y",
                        "eq_num": "(1)"
                    }
                ],
                "section": "Evaluating modules",
                "sec_num": "4.1"
            },
            {
                "text": "(This assumes that the root module of z produces a distribution over labels y.) The set of possible layouts z is restricted by module type constraints: some modules (like find above) operate directly on the input representation, while others (like describe above) also depend on input from specific earlier modules. Two base types are considered in this paper are Attention (a distribution over pixels or entities) and Labels (a distribution over answers). Parameters are tied across multiple instances of the same module, so different instantiated networks may share some parameters but not others. Modules have both parameter arguments (shown in square brackets) and ordinary inputs (shown in parentheses). Parameter arguments, like the running bird example in Section 2, are provided by the layout, and are used to specialize module behavior for particular lexical items. Ordinary inputs are the result of computation lower in the network. In addition to parameter-specific weights, modules have global weights shared across all instances of the module (but not shared with other modules). We write A, a, B, b, . . . for global weights and u i , v i for weights associated with the parameter argument i. \u2295 and denote (possibly broadcasted) elementwise addition and multiplication respectively. The complete set of global weights and parameter-specific weights constitutes \u03b8 e . Every module has access to the world representation, represented as a collection of vectors w 1 , w 2 , . . . (or W expressed as a matrix). The nonlinearity \u03c3 denotes a rectified linear unit.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluating modules",
                "sec_num": "4.1"
            },
            {
                "text": "The modules used in this paper are shown below, with names and type constraints in the first row and a description of the module's computation following.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluating modules",
                "sec_num": "4.1"
            },
            {
                "text": "(\u2192 Attention) lookup[i] produces an attention focused entirely at the index f (i), where the relationship f between words and positions in the input map is known ahead of time (e.g. string matches on database fields).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Lookup",
                "sec_num": null
            },
            {
                "text": "lookup[i] = e f (i) (2)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Lookup",
                "sec_num": null
            },
            {
                "text": "where e i is the basis vector that is 1 in the ith position and 0 elsewhere.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Lookup",
                "sec_num": null
            },
            {
                "text": "Find (\u2192 Attention) find[i]",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Lookup",
                "sec_num": null
            },
            {
                "text": "computes a distribution over indices by concatenating the parameter argument with each position of the input feature map, and passing the concatenated vector through a MLP:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Lookup",
                "sec_num": null
            },
            {
                "text": "find[i] = softmax(a \u03c3(Bv i \u2295 CW \u2295 d)) (3)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Lookup",
                "sec_num": null
            },
            {
                "text": "(Attention \u2192 Attention) relate directs focus from one region of the input to another. It behaves much like the find module, but also conditions its behavior on the current region of attention h. Let w(h) = k h k w k , where h k is the k th element of h. Then,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Relate",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "relate[i](h) = softmax(a \u03c3(Bv i \u2295 CW \u2295 D w(h) \u2295 e))",
                        "eq_num": "(4)"
                    }
                ],
                "section": "Relate",
                "sec_num": null
            },
            {
                "text": "And (Attention* \u2192 Attention) and performs an operation analogous to set intersection for attentions. The analogy to probabilistic logic suggests multiplying probabilities:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Relate",
                "sec_num": null
            },
            {
                "text": "and(h 1 , h 2 , . . .) = h 1 h 2 \u2022 \u2022 \u2022 (5)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Relate",
                "sec_num": null
            },
            {
                "text": "Describe (Attention \u2192 Labels) describe[i] computes a weighted average of w under the input attention. This average is then used to predict an answer representation. With w as above,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Relate",
                "sec_num": null
            },
            {
                "text": "describe[i](h) = softmax(A\u03c3(B w(h) + v i )) (6)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Relate",
                "sec_num": null
            },
            {
                "text": "(Attention \u2192 Labels) exists is the existential quantifier, and inspects the incoming attention directly to produce a label, rather than an intermediate feature vector like describe: ... With z observed, the model we have described so far corresponds largely to that of Andreas et al. (2016) , though the module inventory is differentin particular, our new exists and relate modules do not depend on the two-dimensional spatial structure of the input. This enables generalization to nonvisual world representations.",
                "cite_spans": [
                    {
                        "start": 269,
                        "end": 290,
                        "text": "Andreas et al. (2016)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Exists",
                "sec_num": null
            },
            {
                "text": "exists](h) = softmax max k h k a + b (7)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Exists",
                "sec_num": null
            },
            {
                "text": "Learning in this simplified setting is straightforward. Assuming the top-level module in each layout is a describe or exists module, the fully-instantiated network corresponds to a distribution over labels conditioned on layouts. To train, we maximize (w,y,z) log p z (y|w; \u03b8 e ) directly. This can be understood as a parameter-tying scheme, where the decisions about which parameters to tie are governed by the observed layouts z.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Exists",
                "sec_num": null
            },
            {
                "text": "Next we describe the layout model p(z|x; \u03b8 ). We first use a fixed syntactic parse to generate a small set of candidate layouts, analogously to the way a semantic grammar generates candidate semantic parses in previous work (Berant and Liang, 2014) .",
                "cite_spans": [
                    {
                        "start": 224,
                        "end": 248,
                        "text": "(Berant and Liang, 2014)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Assembling networks",
                "sec_num": "4.2"
            },
            {
                "text": "A semantic parse differs from a syntactic parse in two primary ways. First, lexical items must be mapped onto a (possibly smaller) set of semantic primitives. Second, these semantic primitives must be combined into a structure that closely, but not exactly, parallels the structure provided by syntax. For example, state and province might need to be identified with the same field in a database schema, while all states have a capital might need to be identified with the correct (in situ) quantifier scope.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Assembling networks",
                "sec_num": "4.2"
            },
            {
                "text": "While we cannot avoid the structure selection problem, continuous representations simplify the lexical selection problem. For modules that accept a vector parameter, we associate these parameters with words rather than semantic tokens, and thus turn the combinatorial optimization problem associated with lexicon induction into a continuous one. Now, in order to learn that province and state have the same denotation, it is sufficient to learn that their associated parameters are close in some embedding space-a task amenable to gradient descent. (Note that this is easy only in an optimizability sense, and not an information-theoretic one-we must still learn to associate each independent lexical item with the correct vector.) The remaining combinatorial problem is to arrange the provided lexical items into the right computational structure. In this respect, layout prediction is more like syntactic parsing than ordinary semantic parsing, and we can rely on an off-the-shelf syntactic parser to get most of the way there. In this work, syntactic structure is provided by the Stanford dependency parser (De Marneffe and Manning, 2008) .",
                "cite_spans": [
                    {
                        "start": 1110,
                        "end": 1141,
                        "text": "(De Marneffe and Manning, 2008)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Assembling networks",
                "sec_num": "4.2"
            },
            {
                "text": "The construction of layout candidates is depicted in Figure 3 , and proceeds as follows:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 60,
                        "end": 61,
                        "text": "3",
                        "ref_id": "FIGREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Assembling networks",
                "sec_num": "4.2"
            },
            {
                "text": "1. Represent the input sentence as a dependency tree. 2. Collect all nouns, verbs, and prepositional phrases that are attached directly to a wh-word or copula. 3. Associate each of these with a layout fragment: Ordinary nouns and verbs are mapped to a single find module. Proper nouns to a single lookup module. Prepositional phrases are mapped to a depth-2 fragment, with a relate module for the preposition above a find module for the enclosed head noun. 4. Form subsets of this set of layout fragments.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Assembling networks",
                "sec_num": "4.2"
            },
            {
                "text": "For each subset, construct a layout candidate by joining all fragments with an and module, and inserting either a measure or describe module at the top (each subset thus results in two parse candidates.)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Assembling networks",
                "sec_num": "4.2"
            },
            {
                "text": "All layouts resulting from this process feature a relatively flat tree structure with at most one conjunction and one quantifier. This is a strong simplifying assumption, but appears sufficient to cover most of the examples that appear in both of our tasks. As our approach includes both categories, relations and simple quantification, the range of phenomena considered is generally broader than previous perceptually-grounded QA work (Krishnamurthy and Kollar, 2013; Matuszek et al., 2012) .",
                "cite_spans": [
                    {
                        "start": 436,
                        "end": 468,
                        "text": "(Krishnamurthy and Kollar, 2013;",
                        "ref_id": null
                    },
                    {
                        "start": 469,
                        "end": 491,
                        "text": "Matuszek et al., 2012)",
                        "ref_id": "BIBREF15"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Assembling networks",
                "sec_num": "4.2"
            },
            {
                "text": "Having generated a set of candidate parses, we need to score them. This is a ranking problem; as in the rest of our approach, we solve it using standard neural machinery. In particular, we produce an LSTM representation of the question, a feature-based representation of the query, and pass both representations through a multilayer perceptron (MLP). The query feature vector includes indicators on the number of modules of each type present, as well as their associated parameter arguments. While one can easily imagine a more sophisticated parsescoring model, this simple approach works well for our tasks.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Assembling networks",
                "sec_num": "4.2"
            },
            {
                "text": "Formally, for a question x, let h q (x) be an LSTM encoding of the question (i.e. the last hidden layer of an LSTM applied word-by-word to the input question). Let {z 1 , z 2 , . . .} be the proposed layouts for x, and let f (z i ) be a feature vector representing the ith layout. Then the score s(z i |x) for the layout z i is",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Assembling networks",
                "sec_num": "4.2"
            },
            {
                "text": "s(z i |x) = a \u03c3(Bh q (x) + Cf (z i ) + d) (8)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Assembling networks",
                "sec_num": "4.2"
            },
            {
                "text": "i.e. the output of an MLP with inputs h q (x) and f (z i ), and parameters \u03b8 = {a, B, C, d}. Finally, we normalize these scores to obtain a distribution:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Assembling networks",
                "sec_num": "4.2"
            },
            {
                "text": "p(z i |x; \u03b8 ) = e s(z i |x) n j=1 e s(z j |x) (9)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Assembling networks",
                "sec_num": "4.2"
            },
            {
                "text": "Having defined a layout selection module p(z|x; \u03b8 ) and a network execution model p z (y|w; \u03b8 e ), we are ready to define a model for predicting answers given only (world, question) pairs. The key constraint is that we want to minimize evaluations of p z (y|w; \u03b8 e ) (which involves expensive application of a deep network to a large input representation), but can tractably evaluate p(z|x; \u03b8 ) for all z (which involves application of a shallow network to a relatively small set of candidates). This is the opposite of the situation usually encountered semantic parsing, where calls to the query execution model are fast but the set of candidate parses is too large to score exhaustively.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Assembling networks",
                "sec_num": "4.2"
            },
            {
                "text": "In fact, the problem more closely resembles the scenario faced by agents in the reinforcement learning setting (where it is cheap to score actions, but potentially expensive to execute them and obtain rewards). We adopt a common approach from that literature, and express our model as a stochastic policy. Under this policy, we first sample a layout z from a distribution p(z|x; \u03b8 ), and then apply z to the knowledge source and obtain a distribution over answers p(y|z, w; \u03b8 e ).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Assembling networks",
                "sec_num": "4.2"
            },
            {
                "text": "After z is chosen, we can train the execution model directly by maximizing log p(y|z, w; \u03b8 e ) with respect to \u03b8 e as before (this is ordinary backpropagation). Because the hard selection of z is nondifferentiable, we optimize p(z|x; \u03b8 ) using a policy gradient method. The gradient of the reward surface J with respect to the parameters of the policy is",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Assembling networks",
                "sec_num": "4.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\u2207J(\u03b8 ) = E[\u2207 log p(z|x; \u03b8 ) \u2022 r]",
                        "eq_num": "(10)"
                    }
                ],
                "section": "Assembling networks",
                "sec_num": "4.2"
            },
            {
                "text": "(this is the REINFORCE rule (Williams, 1992)). Here the expectation is taken with respect to rollouts of the policy, and r is the reward. Because our goal is to select the network that makes the most accurate predictions, we take the reward to be identically the negative log-probability from the execution phase, i.e.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Assembling networks",
                "sec_num": "4.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "E[(\u2207 log p(z|x; \u03b8 )) \u2022 log p(y|z, w; \u03b8 e )]",
                        "eq_num": "(11)"
                    }
                ],
                "section": "Assembling networks",
                "sec_num": "4.2"
            },
            {
                "text": "Thus the update to the layout-scoring model at each timestep is simply the gradient of the log-probability of the chosen layout, scaled by the accuracy of that layout's predictions. At training time, we approximate the expectation with a single rollout, so at each step we update \u03b8 in the direction (\u2207 log p(z|x; \u03b8 ))\u2022 log p(y|z, w; \u03b8 e ) for a single z \u223c p(z|x; \u03b8 ). \u03b8 e and \u03b8 are optimized using ADADELTA (Zeiler, 2012) with \u03c1 = 0.95, \u03b5 = 1e-6 and gradient clipping at a norm of 10.",
                "cite_spans": [
                    {
                        "start": 407,
                        "end": 421,
                        "text": "(Zeiler, 2012)",
                        "ref_id": "BIBREF26"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Assembling networks",
                "sec_num": "4.2"
            },
            {
                "text": "What is in the sheep's ear?",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Assembling networks",
                "sec_num": "4.2"
            },
            {
                "text": "What color is she wearing?",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Assembling networks",
                "sec_num": "4.2"
            },
            {
                "text": "What is the man dragging? For the first two examples, the model produces reasonable parses, attends to the correct region of the images (the ear and the woman's clothing), and generates the correct answer. In the third image, the verb is discarded and a wrong answer is produced.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Assembling networks",
                "sec_num": "4.2"
            },
            {
                "text": "The framework described in this paper is general, and we are interested in how well it performs on datasets of varying domain, size and linguistic complexity. To that end, we evaluate our model on tasks at opposite extremes of both these criteria: a large visual question answering dataset, and a small collection of more structured geography questions.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "5"
            },
            {
                "text": "Our first task is the recently-introduced Visual Question Answering challenge (VQA) (Antol et al., 2015) . The VQA dataset consists of more than 200,000 images paired with human-annotated questions and answers, as in Figure 4 . We use the VQA 1.0 release, employing the development set for model selection and hyperparameter tuning, and reporting final results from the evaluation server on the test-standard set. For the experiments described in this section, the input feature representations w i are computed by the the fifth convolutional layer of a 16-layer VGGNet after pooling (Simonyan and Zisserman, 2014) . Input images are scaled to 448\u00d7448 before computing their representations. We found that performance on this task was best if the candidate layouts were relatively simple: only describe, and and find modules are used, and layouts contain at most two conjuncts. One weakness of this basic framework is a difficulty modeling prior knowledge about answers (of the form bears are brown). This kinds of linguistic \"prior\" is essential for the VQA task, and easily incorporated. We simply introduce an extra hidden layer for recombining the final module network output with the input sentence representation h q (x) (see Equation 8), replacing Equation 1 with:",
                "cite_spans": [
                    {
                        "start": 84,
                        "end": 104,
                        "text": "(Antol et al., 2015)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 584,
                        "end": 614,
                        "text": "(Simonyan and Zisserman, 2014)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 224,
                        "end": 225,
                        "text": "4",
                        "ref_id": "FIGREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Questions about images",
                "sec_num": "5.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "log p z (y|w, x) = (Ah q (x) + B z w ) y",
                        "eq_num": "(12)"
                    }
                ],
                "section": "Questions about images",
                "sec_num": "5.1"
            },
            {
                "text": "(Now modules with output type Labels should be understood as producing an answer embedding rather than a distribution over answers.) This allows the question to influence the answer directly.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Questions about images",
                "sec_num": "5.1"
            },
            {
                "text": "Results are shown in Table 1 . The use of dynamic networks provides a small gain, most noticeably on yes/no questions. We achieve state-of-theart results on this task, outperforming a highly effective visual bag-of-words model (Zhou et al., 2015) , a model with dynamic network parameter prediction (but fixed network structure) (Noh et al., 2015) , and a previous approach using neural module networks with no structure prediction (Andreas et al., 2016) . For this last model, we report both the numbers from the original paper, and a reimplementation of the model that uses the same image preprocessing as the dynamic module network experiments in this paper. A more conventional attentional model has also been applied to this task (Yang et al., 2015) ; while we also outperform their reported performance, the evaluation uses different train/test split, so results are not directly comparable. Some examples are shown in Figure 4 . In general, the model learns to focus on the correct region of the image, and tends to consider a broad window around the region. This facilitates answering questions like Where is the cat?, which requires knowledge of the surroundings as well as the object in question.",
                "cite_spans": [
                    {
                        "start": 227,
                        "end": 246,
                        "text": "(Zhou et al., 2015)",
                        "ref_id": null
                    },
                    {
                        "start": 329,
                        "end": 347,
                        "text": "(Noh et al., 2015)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 432,
                        "end": 454,
                        "text": "(Andreas et al., 2016)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 735,
                        "end": 754,
                        "text": "(Yang et al., 2015)",
                        "ref_id": "BIBREF25"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 27,
                        "end": 28,
                        "text": "1",
                        "ref_id": "TABREF1"
                    },
                    {
                        "start": 932,
                        "end": 933,
                        "text": "4",
                        "ref_id": "FIGREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Questions about images",
                "sec_num": "5.1"
            },
            {
                "text": "The next set of experiments we consider focuses on GeoQA, a geographical question-answering task first introduced by Krishnamurthy and Kollar (2013) . This task was originally paired with a visual question answering task much simpler than the one just discussed, and is appealing for a number of reasons. In contrast to the VQA dataset, GeoQA is quite small, containing only 263 examples. Two baselines are available: one using a classical semantic parser backed by a database, and another which induces logical predicates using linear classifiers over both spatial and distributional features. This allows us to evaluate the quality of our model relative to other perceptually grounded logical semantics, as well as strictly logical approaches.",
                "cite_spans": [
                    {
                        "start": 117,
                        "end": 148,
                        "text": "Krishnamurthy and Kollar (2013)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Questions about geography",
                "sec_num": "5.2"
            },
            {
                "text": "The GeoQA domain consists of a set of entities (e.g. states, cities, parks) which participate in various relations (e.g. north-of, capital-of). Here we take the world representation to consist of two pieces: a set of category features (used by the find module) and a different set of relational features (used by the relate module). For our experiments, we use a subset of the features originally used by Krishnamurthy et al. The original dataset includes no quantifiers, and treats the questions What cities are in Texas? and Are there any cities in Texas? identically. Because we are interested in testing the parser's ability to predict a variety of different structures, we intro- [none] (daytona-beach): wrong module behavior duce a new version of the dataset, GeoQA+Q, which distinguishes these two cases, and expects a Boolean answer to questions of the second kind.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Questions about geography",
                "sec_num": "5.2"
            },
            {
                "text": "Results are shown in Table 2 . As in the original work, we report the results of leave-oneenvironment-out cross-validation on the set of 10 environments. Our dynamic model (D-NMN) outperforms both the logical (LSP-F) and perceptual models (LSP-W) described by (Krishnamurthy and Kollar, 2013) , as well as a fixed-structure neural module net (NMN). This improvement is particularly notable on the dataset with quantifiers, where dynamic structure prediction produces a 20% relative improvement over the fixed baseline. A variety of predicted layouts are shown in Figure 5 .",
                "cite_spans": [
                    {
                        "start": 260,
                        "end": 292,
                        "text": "(Krishnamurthy and Kollar, 2013)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 27,
                        "end": 28,
                        "text": "2",
                        "ref_id": null
                    },
                    {
                        "start": 570,
                        "end": 571,
                        "text": "5",
                        "ref_id": "FIGREF6"
                    }
                ],
                "eq_spans": [],
                "section": "Questions about geography",
                "sec_num": "5.2"
            },
            {
                "text": "We have introduced a new model, the dynamic neural module network, for answering queries about both structured and unstructured sources of information. Given only (question, world, answer) triples as training data, the model learns to assemble neural networks on the fly from an inventory of neural models, and simultaneously learns weights for these modules so that they can be composed into novel structures. Our approach achieves state-of-the-art results on two tasks. We believe that the success of this work derives from two factors:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "6"
            },
            {
                "text": "Continuous representations improve the expressiveness and learnability of semantic parsers: by replacing discrete predicates with differentiable neural network fragments, we bypass the challenging combinatorial optimization problem associated with induction of a semantic lexicon. In structured world representations, neural predicate representations allow the model to invent reusable attributes and relations not expressed in the schema. Perhaps more importantly, we can extend compositional questionanswering machinery to complex, continuous world representations like images.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "6"
            },
            {
                "text": "Semantic structure prediction improves generalization in deep networks: by replacing a fixed network topology with a dynamic one, we can tailor the computation performed to each problem instance, using deeper networks for more complex questions and representing combinatorially many queries with comparatively few parameters. In practice, this results in considerable gains in speed and sample efficiency, even with very little training data.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "6"
            },
            {
                "text": "These observations are not limited to the question answering domain, and we expect that they can be applied similarly to tasks like instruction following, game playing, and language generation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "6"
            },
            {
                "text": "We have released our code at http://github.com/ jacobandreas/nmn2",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "But note that unlike formal semantics, the behavior of the primitive functions here is itself unknown.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "JA is supported by a National Science Foundation Graduate Fellowship. MR is supported by a fellowship within the FIT weltweit-Program of the German Academic Exchange Service (DAAD). This work was additionally supported by DARPA, AFRL, DoD MURI award N000141110688, NSF awards IIS-1427425 and IIS-1212798, and the Berkeley Vision and Learning Center.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Semantic parsing as machine translation",
                "authors": [
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Andreas",
                        "suffix": ""
                    },
                    {
                        "first": "Andreas",
                        "middle": [],
                        "last": "Vlachos",
                        "suffix": ""
                    },
                    {
                        "first": "Stephen",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jacob Andreas, Andreas Vlachos, and Stephen Clark. 2013. Semantic parsing as machine translation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, Sofia, Bulgaria.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Neural module networks",
                "authors": [
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Andreas",
                        "suffix": ""
                    },
                    {
                        "first": "Marcus",
                        "middle": [],
                        "last": "Rohrbach",
                        "suffix": ""
                    },
                    {
                        "first": "Trevor",
                        "middle": [],
                        "last": "Darrell",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Klein",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. 2016. Neural module networks. In Pro- ceedings of the Conference on Computer Vision and Pattern Recognition.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "VQA: Visual question answering",
                "authors": [
                    {
                        "first": "Stanislaw",
                        "middle": [],
                        "last": "Antol",
                        "suffix": ""
                    },
                    {
                        "first": "Aishwarya",
                        "middle": [],
                        "last": "Agrawal",
                        "suffix": ""
                    },
                    {
                        "first": "Jiasen",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    },
                    {
                        "first": "Margaret",
                        "middle": [],
                        "last": "Mitchell",
                        "suffix": ""
                    },
                    {
                        "first": "Dhruv",
                        "middle": [],
                        "last": "Batra",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Lawrence Zitnick",
                        "suffix": ""
                    },
                    {
                        "first": "Devi",
                        "middle": [],
                        "last": "Parikh",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the International Conference on Computer Vision",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar- garet Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. 2015. VQA: Visual question answer- ing. In Proceedings of the International Conference on Computer Vision.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Semantic parsing via paraphrasing",
                "authors": [
                    {
                        "first": "Jonathan",
                        "middle": [],
                        "last": "Berant",
                        "suffix": ""
                    },
                    {
                        "first": "Percy",
                        "middle": [],
                        "last": "Liang",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics",
                "volume": "7",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jonathan Berant and Percy Liang. 2014. Semantic pars- ing via paraphrasing. In Proceedings of the Annual Meeting of the Association for Computational Linguis- tics, volume 7, page 92.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Question answering with subgraph embeddings",
                "authors": [
                    {
                        "first": "Antoine",
                        "middle": [],
                        "last": "Bordes",
                        "suffix": ""
                    },
                    {
                        "first": "Sumit",
                        "middle": [],
                        "last": "Chopra",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Weston",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Antoine Bordes, Sumit Chopra, and Jason Weston. 2014. Question answering with subgraph embeddings. Pro- ceedings of the Conference on Empirical Methods in Natural Language Processing.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "From machine learning to machine reasoning",
                "authors": [
                    {
                        "first": "L\u00e9on",
                        "middle": [],
                        "last": "Bottou",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Machine learning",
                "volume": "94",
                "issue": "2",
                "pages": "133--149",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "L\u00e9on Bottou. 2014. From machine learning to machine reasoning. Machine learning, 94(2):133-149.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "The Stanford typed dependencies representation",
                "authors": [
                    {
                        "first": "Marie-Catherine",
                        "middle": [],
                        "last": "De",
                        "suffix": ""
                    },
                    {
                        "first": "Marneffe",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proceedings of the International Conference on Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "1--8",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Marie-Catherine De Marneffe and Christopher D Man- ning. 2008. The Stanford typed dependencies repre- sentation. In Proceedings of the International Confer- ence on Computational Linguistics, pages 1-8.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Towards a formal distributional semantics: Simulating logical calculi with tensors",
                "authors": [
                    {
                        "first": "Edward",
                        "middle": [],
                        "last": "Grefenstette",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Joint Conference on Lexical and Computational Semantics",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Edward Grefenstette. 2013. Towards a formal distribu- tional semantics: Simulating logical calculi with ten- sors. Joint Conference on Lexical and Computational Semantics.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Teaching machines to read and comprehend",
                "authors": [
                    {
                        "first": "Karl",
                        "middle": [],
                        "last": "Moritz Hermann",
                        "suffix": ""
                    },
                    {
                        "first": "Tomas",
                        "middle": [],
                        "last": "Kocisky",
                        "suffix": ""
                    },
                    {
                        "first": "Edward",
                        "middle": [],
                        "last": "Grefenstette",
                        "suffix": ""
                    },
                    {
                        "first": "Lasse",
                        "middle": [],
                        "last": "Espeholt",
                        "suffix": ""
                    },
                    {
                        "first": "Will",
                        "middle": [],
                        "last": "Kay",
                        "suffix": ""
                    },
                    {
                        "first": "Mustafa",
                        "middle": [],
                        "last": "Suleyman",
                        "suffix": ""
                    },
                    {
                        "first": "Phil",
                        "middle": [],
                        "last": "Blunsom",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "1684--1692",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Karl Moritz Hermann, Tomas Kocisky, Edward Grefen- stette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems, pages 1684-1692.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Jointly learning to parse and perceive: connecting natural language to the physical world",
                "authors": [
                    {
                        "first": "Mohit",
                        "middle": [],
                        "last": "Iyyer",
                        "suffix": ""
                    },
                    {
                        "first": "Jordan",
                        "middle": [],
                        "last": "Boyd-Graber",
                        "suffix": ""
                    },
                    {
                        "first": "Leonardo",
                        "middle": [],
                        "last": "Claudino",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Hal",
                        "middle": [],
                        "last": "Daum\u00e9",
                        "suffix": ""
                    },
                    {
                        "first": "Iii",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. Jayant Krishnamurthy and Thomas Kollar",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mohit Iyyer, Jordan Boyd-Graber, Leonardo Claudino, Richard Socher, and Hal Daum\u00e9 III. 2014. A neu- ral network for factoid question answering over para- graphs. In Proceedings of the Conference on Empiri- cal Methods in Natural Language Processing. Jayant Krishnamurthy and Thomas Kollar. 2013. Jointly learning to parse and perceive: connecting natural lan- guage to the physical world. Transactions of the Asso- ciation for Computational Linguistics.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Vector space semantic parsing: A framework for compositional vector space models",
                "authors": [
                    {
                        "first": "Jayant",
                        "middle": [],
                        "last": "Krishnamurthy",
                        "suffix": ""
                    },
                    {
                        "first": "Tom",
                        "middle": [],
                        "last": "Mitchell",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of the ACL Workshop on Continuous Vector Space Models and their Compositionality",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jayant Krishnamurthy and Tom Mitchell. 2013. Vec- tor space semantic parsing: A framework for compo- sitional vector space models. In Proceedings of the ACL Workshop on Continuous Vector Space Models and their Compositionality.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Inducing probabilistic CCG grammars from logical form with higherorder unification",
                "authors": [
                    {
                        "first": "Tom",
                        "middle": [],
                        "last": "Kwiatkowski",
                        "suffix": ""
                    },
                    {
                        "first": "Luke",
                        "middle": [],
                        "last": "Zettlemoyer",
                        "suffix": ""
                    },
                    {
                        "first": "Sharon",
                        "middle": [],
                        "last": "Goldwater",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Steedman",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "1223--1233",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa- ter, and Mark Steedman. 2010. Inducing probabilis- tic CCG grammars from logical form with higher- order unification. In Proceedings of the Conference on Empirical Methods in Natural Language Process- ing, pages 1223-1233, Cambridge, Massachusetts.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Scaling semantic parsers with onthe-fly ontology matching",
                "authors": [
                    {
                        "first": "Tom",
                        "middle": [],
                        "last": "Kwiatkowski",
                        "suffix": ""
                    },
                    {
                        "first": "Eunsol",
                        "middle": [],
                        "last": "Choi",
                        "suffix": ""
                    },
                    {
                        "first": "Yoav",
                        "middle": [],
                        "last": "Artzi",
                        "suffix": ""
                    },
                    {
                        "first": "Luke",
                        "middle": [],
                        "last": "Zettlemoyer",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke Zettlemoyer. 2013. Scaling semantic parsers with on- the-fly ontology matching. In Proceedings of the Con- ference on Empirical Methods in Natural Language Processing.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Learning dependency-based compositional semantics",
                "authors": [
                    {
                        "first": "Percy",
                        "middle": [],
                        "last": "Liang",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Jordan",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Klein",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of the Human Language Technology Conference of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "590--599",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Percy Liang, Michael Jordan, and Dan Klein. 2011. Learning dependency-based compositional semantics. In Proceedings of the Human Language Technology Conference of the Association for Computational Lin- guistics, pages 590-599, Portland, Oregon.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Ask your neurons: A neural-based approach to answering questions about images",
                "authors": [
                    {
                        "first": "Mateusz",
                        "middle": [],
                        "last": "Malinowski",
                        "suffix": ""
                    },
                    {
                        "first": "Marcus",
                        "middle": [],
                        "last": "Rohrbach",
                        "suffix": ""
                    },
                    {
                        "first": "Mario",
                        "middle": [],
                        "last": "Fritz",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the International Conference on Computer Vision",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mateusz Malinowski, Marcus Rohrbach, and Mario Fritz. 2015. Ask your neurons: A neural-based approach to answering questions about images. In Proceedings of the International Conference on Computer Vision.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "A joint model of language and perception for grounded attribute learning",
                "authors": [
                    {
                        "first": "Cynthia",
                        "middle": [],
                        "last": "Matuszek",
                        "suffix": ""
                    },
                    {
                        "first": "Nicholas",
                        "middle": [],
                        "last": "Fitzgerald",
                        "suffix": ""
                    },
                    {
                        "first": "Luke",
                        "middle": [],
                        "last": "Zettlemoyer",
                        "suffix": ""
                    },
                    {
                        "first": "Liefeng",
                        "middle": [],
                        "last": "Bo",
                        "suffix": ""
                    },
                    {
                        "first": "Dieter",
                        "middle": [],
                        "last": "Fox",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "International Conference on Machine Learning",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Cynthia Matuszek, Nicholas FitzGerald, Luke Zettle- moyer, Liefeng Bo, and Dieter Fox. 2012. A joint model of language and perception for grounded at- tribute learning. In International Conference on Ma- chine Learning.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Image question answering using convolutional neural network with dynamic parameter prediction",
                "authors": [
                    {
                        "first": "Hyeonwoo",
                        "middle": [],
                        "last": "Noh",
                        "suffix": ""
                    },
                    {
                        "first": "Paul",
                        "middle": [],
                        "last": "Hongsuck Seo",
                        "suffix": ""
                    },
                    {
                        "first": "Bohyung",
                        "middle": [],
                        "last": "Han",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1511.05756"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Hyeonwoo Noh, Paul Hongsuck Seo, and Bohyung Han. 2015. Image question answering using convolutional neural network with dynamic parameter prediction. arXiv preprint arXiv:1511.05756.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Compositional semantic parsing on semi-structured tables",
                "authors": [
                    {
                        "first": "Panupong",
                        "middle": [],
                        "last": "Pasupat",
                        "suffix": ""
                    },
                    {
                        "first": "Percy",
                        "middle": [],
                        "last": "Liang",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Panupong Pasupat and Percy Liang. 2015. Composi- tional semantic parsing on semi-structured tables. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Exploring models and data for image question answering",
                "authors": [
                    {
                        "first": "Mengye",
                        "middle": [],
                        "last": "Ren",
                        "suffix": ""
                    },
                    {
                        "first": "Ryan",
                        "middle": [],
                        "last": "Kiros",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Zemel",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mengye Ren, Ryan Kiros, and Richard Zemel. Explor- ing models and data for image question answering. In Advances in Neural Information Processing Systems.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Very deep convolutional networks for large-scale image recognition",
                "authors": [
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Simonyan",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Zisserman",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1409.1556"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "K Simonyan and A Zisserman. 2014. Very deep con- volutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Parsing with compositional vector grammars",
                "authors": [
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Bauer",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [
                            "Y"
                        ],
                        "last": "Ng",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of the Annual Meeting of the",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Richard Socher, John Bauer, Christopher D. Manning, and Andrew Y. Ng. 2013. Parsing with compositional vector grammars. In Proceedings of the Annual Meet- ing of the Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning",
                "authors": [
                    {
                        "first": "Williams",
                        "middle": [],
                        "last": "Ronald",
                        "suffix": ""
                    }
                ],
                "year": 1992,
                "venue": "Machine learning",
                "volume": "8",
                "issue": "3-4",
                "pages": "229--256",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ronald J Williams. 1992. Simple statistical gradient- following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229-256.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Learning synchronous grammars for semantic parsing with lambda calculus",
                "authors": [
                    {
                        "first": "Yuk",
                        "middle": [
                            "Wah"
                        ],
                        "last": "Wong",
                        "suffix": ""
                    },
                    {
                        "first": "Raymond",
                        "middle": [
                            "J"
                        ],
                        "last": "Mooney",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics",
                "volume": "45",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yuk Wah Wong and Raymond J. Mooney. 2007. Learn- ing synchronous grammars for semantic parsing with lambda calculus. In Proceedings of the Annual Meet- ing of the Association for Computational Linguistics, volume 45, page 960.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Ask, attend and answer: Exploring question-guided spatial attention for visual question answering",
                "authors": [
                    {
                        "first": "Huijuan",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Kate",
                        "middle": [],
                        "last": "Saenko",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1511.05234"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Huijuan Xu and Kate Saenko. 2015. Ask, attend and answer: Exploring question-guided spatial atten- tion for visual question answering. arXiv preprint arXiv:1511.05234.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Show, attend and tell: Neural image caption generation with visual attention",
                "authors": [
                    {
                        "first": "Kelvin",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Jimmy",
                        "middle": [],
                        "last": "Ba",
                        "suffix": ""
                    },
                    {
                        "first": "Ryan",
                        "middle": [],
                        "last": "Kiros",
                        "suffix": ""
                    },
                    {
                        "first": "Kyunghyun",
                        "middle": [],
                        "last": "Cho",
                        "suffix": ""
                    },
                    {
                        "first": "Aaron",
                        "middle": [],
                        "last": "Courville",
                        "suffix": ""
                    },
                    {
                        "first": "Ruslan",
                        "middle": [],
                        "last": "Salakhutdinov",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Zemel",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "International Conference on Machine Learning",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, and Yoshua Bengio. 2015. Show, attend and tell: Neural image caption generation with visual attention. In International Conference on Machine Learning.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Stacked attention networks for image question answering",
                "authors": [
                    {
                        "first": "Zichao",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaodong",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Jianfeng",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Li",
                        "middle": [],
                        "last": "Deng",
                        "suffix": ""
                    },
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Smola",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1511.02274"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola. 2015. Stacked attention net- works for image question answering. arXiv preprint arXiv:1511.02274.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "ADADELTA: An adaptive learning rate method",
                "authors": [
                    {
                        "first": "Pengcheng",
                        "middle": [],
                        "last": "Yin",
                        "suffix": ""
                    },
                    {
                        "first": "Zhengdong",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    },
                    {
                        "first": "Hang",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Ben",
                        "middle": [],
                        "last": "Kao",
                        "suffix": ""
                    },
                    {
                        "first": ";",
                        "middle": [],
                        "last": "Matthew",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Zeiler",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Simple baseline for visual question answering",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1512.00965"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Pengcheng Yin, Zhengdong Lu, Hang Li, and Ben Kao. 2015. Neural enquirer: Learning to query tables. arXiv preprint arXiv:1512.00965. Matthew D Zeiler. 2012. ADADELTA: An adaptive learning rate method. arXiv preprint arXiv:1212.5701. Bolei Zhou, Yuandong Tian, Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus. 2015. Simple base- line for visual question answering. arXiv preprint arXiv:1512.02167.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: A learned syntactic analysis (a) is used to assemble a collection of neural modules (b) into a deep neural network (c), and applied to a world representation (d) to produce an answer.",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 2: Simple neural module networks, corresponding to the questions What color is the bird? and Are there any states? (a) A neural find module for computing an attention over pixels. (b) The same operation applied to a knowledge base. (c) Using an attention produced by a lower module to identify the color of the region of the image attended to. (d) Performing quantification by evaluating an attention directly.",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF4": {
                "num": null,
                "text": "Figure 3: Generation of layout candidates. The input sentence (a) is represented as a dependency parse (b). Fragments of this dependency parse are then associated with appropriate modules (c), and these fragments are assembled into full layouts (d).",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "FIGREF5": {
                "num": null,
                "text": "Figure4: Sample outputs for the visual question answering task. The second row shows the final attention provided as input to the top-level describe module. For the first two examples, the model produces reasonable parses, attends to the correct region of the images (the ear and the woman's clothing), and generates the correct answer. In the third image, the verb is discarded and a wrong answer is produced.",
                "uris": null,
                "fig_num": "4",
                "type_str": "figure"
            },
            "FIGREF6": {
                "num": null,
                "text": "Figure 5: Example layouts and answers selected by the model on the GeoQA dataset. For incorrect predictions, the correct answer is shown in parentheses.",
                "uris": null,
                "fig_num": "5",
                "type_str": "figure"
            },
            "TABREF1": {
                "content": "<table><tr><td/><td/><td>test-dev</td><td/><td/><td>test-std</td></tr><tr><td/><td colspan=\"3\">Yes/No Number Other</td><td>All</td><td>All</td></tr><tr><td colspan=\"2\">Zhou (2015) 76.6</td><td>35.0</td><td>42.6</td><td>55.7</td><td>55.9</td></tr><tr><td colspan=\"2\">Noh (2015) 80.7</td><td>37.2</td><td>41.7</td><td>57.2</td><td>57.4</td></tr><tr><td>NMN</td><td>77.7</td><td>37.2</td><td>39.3</td><td>54.8</td><td>55.1</td></tr><tr><td>NMN*</td><td>79.7</td><td>37.1</td><td>42.8</td><td>57.3</td><td>-</td></tr><tr><td>D-NMN</td><td>80.5</td><td>37.4</td><td>43.1</td><td>57.9</td><td>58.0</td></tr></table>",
                "type_str": "table",
                "text": "Results on the VQA test server. NMN is the parameter-tying model fromAndreas et al. (2015), while NMN* is a reimplementation using the same image processing pipeline as D-NMN. The model with dynamic network structure prediction achieves the best published results on this task.",
                "html": null,
                "num": null
            }
        }
    }
}