{
    "paper_id": "D17-1135",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:00:25.786348Z"
    },
    "title": "Chinese Zero Pronoun Resolution with Deep Memory Network",
    "authors": [
        {
            "first": "Qingyu",
            "middle": [],
            "last": "Yin",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Harbin Institute of Technology",
                "location": {
                    "country": "China"
                }
            },
            "email": "qyyin@ir.hit.edu.cn"
        },
        {
            "first": "Yu",
            "middle": [],
            "last": "Zhang",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Harbin Institute of Technology",
                "location": {
                    "country": "China"
                }
            },
            "email": "yzhang@ir.hit.edu.cn"
        },
        {
            "first": "Weinan",
            "middle": [],
            "last": "Zhang",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Harbin Institute of Technology",
                "location": {
                    "country": "China"
                }
            },
            "email": "wnzhang@ir.hit.edu.cn"
        },
        {
            "first": "Ting",
            "middle": [],
            "last": "Liu",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Harbin Institute of Technology",
                "location": {
                    "country": "China"
                }
            },
            "email": "tliu@ir.hit.edu.cn"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Existing approaches for Chinese zero pronoun resolution typically utilize only syntactical and lexical features while ignoring semantic information. The fundamental reason is that zero pronouns have no descriptive information, which brings difficulty in explicitly capturing their semantic similarities with antecedents. Meanwhile, representing zero pronouns is challenging since they are merely gaps that convey no actual content. In this paper, we address this issue by building a deep memory network that is capable of encoding zero pronouns into vector representations with information obtained from their contexts and potential antecedents. Consequently, our resolver takes advantage of semantic information by using these continuous distributed representations. Experiments on the OntoNotes 5.0 dataset show that the proposed memory network could substantially outperform the state-of-the-art systems in various experimental settings.",
    "pdf_parse": {
        "paper_id": "D17-1135",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Existing approaches for Chinese zero pronoun resolution typically utilize only syntactical and lexical features while ignoring semantic information. The fundamental reason is that zero pronouns have no descriptive information, which brings difficulty in explicitly capturing their semantic similarities with antecedents. Meanwhile, representing zero pronouns is challenging since they are merely gaps that convey no actual content. In this paper, we address this issue by building a deep memory network that is capable of encoding zero pronouns into vector representations with information obtained from their contexts and potential antecedents. Consequently, our resolver takes advantage of semantic information by using these continuous distributed representations. Experiments on the OntoNotes 5.0 dataset show that the proposed memory network could substantially outperform the state-of-the-art systems in various experimental settings.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "A zero pronoun (ZP) is a gap in a sentence, which refers to an entity that supplies the necessary information for interpreting the gap (Zhao and Ng, 2007) . A ZP can be either anaphoric if it corefers to one or more preceding noun phrases (antecedents) in the associated text, or non-anaphoric if there are no such noun phrases. Below is an example of ZPs and their antecedents, where \"\u03c6\" denotes the ZP.",
                "cite_spans": [
                    {
                        "start": 135,
                        "end": 154,
                        "text": "(Zhao and Ng, 2007)",
                        "ref_id": "BIBREF41"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "[\u8b66\u65b9] \u8868\u793a \u4ed6\u4eec \u81ea\u6740 \u7684 \u53ef\u80fd\u6027 \u5f88\u9ad8\uff0c \u4e0d \u8fc7 \u03c6 1 \u4e5f \u4e0d \u6392\u9664 \u03c6 2 \u6709 \u4ed6\u6740 \u7684 \u53ef\u80fd\u3002 * Email corresponding.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "([The police] said that they are more likely to commit suicide, but \u03c6 1 could not rule out \u03c6 2 the possibility of homicide.)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this example, the ZP \"\u03c6 1 \" is an anaphoric ZP that refers to the antecedent \"\u8b66\u65b9/The police\" while the ZP \"\u03c6 2 \" is non-anaphoric. Unlike overt pronouns, ZPs lack grammatical attributes such as gender and number that have been proven to be essential in pronoun resolution (Chen and Ng, 2014a) , which makes ZP resolution a more challenging task than overt pronoun resolution.",
                "cite_spans": [
                    {
                        "start": 275,
                        "end": 295,
                        "text": "(Chen and Ng, 2014a)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Automatic Chinese ZP resolution is typically composed of two steps, i.e., anaphoric zero pronoun (AZP) identification that identifies whether a ZP is anaphoric; and AZP resolution, which determines antecedents for AZPs. For AZP identification, state-of-the-art resolvers use machine learning algorithms to build AZP classifiers in a supervised manner (Chen and Ng, 2013, 2016) . For AZP resolution, literature approaches include unsupervised methods (Chen and Ng, 2014b, 2015) , feature-based supervised models (Zhao and Ng, 2007; Kong and Zhou, 2010) , and neural network models (Chen and Ng, 2016) . Neural network models for AZP resolution are of growing interest for their capacity to learn task-specific representations without extensive feature engineering and to effectively exploit lexical information for ZPs and their candidate antecedents in a more scalable manner than feature-based models.",
                "cite_spans": [
                    {
                        "start": 351,
                        "end": 360,
                        "text": "(Chen and",
                        "ref_id": null
                    },
                    {
                        "start": 361,
                        "end": 376,
                        "text": "Ng, 2013, 2016)",
                        "ref_id": null
                    },
                    {
                        "start": 450,
                        "end": 459,
                        "text": "(Chen and",
                        "ref_id": null
                    },
                    {
                        "start": 460,
                        "end": 476,
                        "text": "Ng, 2014b, 2015)",
                        "ref_id": null
                    },
                    {
                        "start": 511,
                        "end": 530,
                        "text": "(Zhao and Ng, 2007;",
                        "ref_id": "BIBREF41"
                    },
                    {
                        "start": 531,
                        "end": 551,
                        "text": "Kong and Zhou, 2010)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 580,
                        "end": 599,
                        "text": "(Chen and Ng, 2016)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Despite these advantages, existing supervised approaches (Zhao and Ng, 2007; Chen and Ng, 2013, 2016) for AZP resolution typically utilize only syntactical and lexical information through features. They overlook semantic information that is regarded as an important factor in the resolution of common noun phrases (Ng, 2007) . The fundamental reason is that ZPs have no descriptive information, which results in difficulty in calculating semantic similarities and relatedness scores between the ZPs and their antecedents. Therefore, the proper representations of ZPs are required so as to take advantage of semantic information when resolving ZPs. However, representing ZPs is challenging because they are merely gaps that convey no actual content.",
                "cite_spans": [
                    {
                        "start": 57,
                        "end": 76,
                        "text": "(Zhao and Ng, 2007;",
                        "ref_id": "BIBREF41"
                    },
                    {
                        "start": 77,
                        "end": 85,
                        "text": "Chen and",
                        "ref_id": null
                    },
                    {
                        "start": 86,
                        "end": 101,
                        "text": "Ng, 2013, 2016)",
                        "ref_id": null
                    },
                    {
                        "start": 314,
                        "end": 324,
                        "text": "(Ng, 2007)",
                        "ref_id": "BIBREF26"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "One straightforward method to address this issue is to represent ZPs with supplemental information provided by some available components, such as contexts and candidate antecedents. Motivated by Chen and Ng (2016) who encode a ZP's lexical contexts by utilizing its preceding word and governing verb, we notice that a ZP's context can help to describe the ZP itself. As an example of its usefulness, given the sentence \"\u03c6 taste spicy\", people may resolve the ZP \"\u03c6\" to the candidate antecedent \"red peppers\", but can hardly regard \"my shoes\" as its antecedent, because they naturally look at the ZP's context \"taste spicy\" to resolve it (\"my shoes\" cannot \"taste spicy\"). Meanwhile, considering that the antecedents of a ZP provide the necessary information for interpreting the gap (ZP), it is a natural way to express a ZP by its potential antecedents. However, only some subsets of candidate antecedents are needed to represent a ZP1 . To achieve this goal, a desirable solution should be capable of explicitly capturing the importance of each candidate antecedent and using them to build up the representation for the ZP.",
                "cite_spans": [
                    {
                        "start": 195,
                        "end": 213,
                        "text": "Chen and Ng (2016)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this paper, inspired by the recent success of computational models with attention mechanism and explicit memory (Sukhbaatar et al., 2015; Tang et al., 2016; Kumar et al., 2015) , we focus on AZP resolution, proposing the zero pronounspecific memory network (ZPMN) that is competent for representing a ZP with information obtained from its contexts and candidate antecedents. These representations provide our system with an ability to take advantage of semantic information when resolving ZPs. Our ZPMN consists of multiple computational layers with shared parameters. With the underlying intuition that not all candidate antecedents are equally relevant for representing the ZP, we develop each computational layer as an attention-based model, which first learns the importance of each candidate antecedent and then utilizes this information to calculate the continu-ous distributed representation of the ZP. The attention weights over candidate antecedents with respect to the ZP's representation obtained by the last layer are regarded as the ZP coreference classification result. Given that every component is differentiable, the entire model could be efficiently trained end-to-end with gradient descent.",
                "cite_spans": [
                    {
                        "start": 115,
                        "end": 140,
                        "text": "(Sukhbaatar et al., 2015;",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 141,
                        "end": 159,
                        "text": "Tang et al., 2016;",
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 160,
                        "end": 179,
                        "text": "Kumar et al., 2015)",
                        "ref_id": "BIBREF22"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We evaluate our method on the Chinese portions of the OntoNotes 5.0 corpus by comparing with the baseline systems in different experimental settings. Results show that our approach significantly outperforms the baseline algorithms and achieves state-of-the-art performance.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We describe our deep memory network approach for AZP resolution in this section. We first give an overview of our model and then describe its components. Finally, we present the training and initialization details.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Zero Pronoun-specific Memory Network",
                "sec_num": "2"
            },
            {
                "text": "In this part, we present an overview of the zero pronoun-specific memory network (ZPMN) for AZP resolution. Given an AZP zp, we first extract a set of candidate antecedents. Following Chen and Ng (2016) , we regard all and only those maximal or modifier noun phrases (NPs) that precede zp in the associated text and are at most two sentences away from it, to be its candidate antecedents. Suppose k candidate antecedents are extracted, our task is to determine the correct antecedent of zp from its candidate antecedent set A(zp) = {c 1 , c 2 , ..., c k }. Specifically, these candidate antecedents are represented in form of vectors {v c 1 , v c 2 , ..., v c k }, which are stacked and regarded as the external memory mem \u2208 R l\u00d7k , where l is the dimension of v c . Meanwhile, we represent each word as a continuous and real-valued vector, which is known as word embedding (Bengio et al., 2003) . These word vectors can be randomly initialized, or be pre-trained from text corpus with learning algorithms (Mikolov et al., 2013; Pennington et al., 2014) . In this work, we adopt the latter strategy since it can better exploit the semantics of words. All the word vectors are stacked in a word embedding matrix L w \u2208 R d\u00d7|V | , where d is the dimension of the word vector and |V | is the size of the word vocabulary. The embedding of word w is Figure 1 : Illustration of the zero pronoun-specific memory network with three computational layers (hops). v zp and v c denote the vector representation of an AZP and its candidate antecedents. The left part in dashed box shows the details of the first hop.",
                "cite_spans": [
                    {
                        "start": 184,
                        "end": 202,
                        "text": "Chen and Ng (2016)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 874,
                        "end": 895,
                        "text": "(Bengio et al., 2003)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 1006,
                        "end": 1028,
                        "text": "(Mikolov et al., 2013;",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 1029,
                        "end": 1053,
                        "text": "Pennington et al., 2014)",
                        "ref_id": "BIBREF27"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 1351,
                        "end": 1352,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "An Overview of the Method",
                "sec_num": "2.1"
            },
            {
                "text": "notated as e \u2208 R d\u00d71 , which is the column in L w .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "An Overview of the Method",
                "sec_num": "2.1"
            },
            {
                "text": "An illustration of ZPMN is given in Figure 1 , which is inspired by the memory network utilized in question answering (Sukhbaatar et al., 2015) . Our model consists of multiple computational layers, each of which contains an attention layer and a linear layer. First, we represent the AZP zp by utilizing its contextual information, that is, proposing the ZP-centered LSTM that encodes zp into its distributed vector representation (i.e. v zp in Figure 1 ). We then regard v zp as the initial representation of zp, and feed it as the input to the first computational layer (hop 1). In the first computational layer, we calculate the attention weight across the AZP for each candidate antecedent, by which our model adaptively selects important information from the external memory (candidate antecedents). The output of the attention layer and the linear transformation of v zp are summed together as the input of to the next layer (hop 2).",
                "cite_spans": [
                    {
                        "start": 118,
                        "end": 143,
                        "text": "(Sukhbaatar et al., 2015)",
                        "ref_id": "BIBREF33"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 43,
                        "end": 44,
                        "text": "1",
                        "ref_id": null
                    },
                    {
                        "start": 453,
                        "end": 454,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "An Overview of the Method",
                "sec_num": "2.1"
            },
            {
                "text": "We stack multiple hops by repeating the same process for multiple times in a similar manner. We call the abstractive information obtained from the external memory the \"key extension\" of the AZP. Note that the attention and linear layer parameters are shared in different hops. Regardless of the number of hops the model employs, they utilize the same number of parameters. Finally, after going through all the hops, we regard the attention weight of each candidate antecedent with respect to the AZP representation generated by the last hop as the probability that the candidate antecedent is the correct antecedent, and predict the highest-scoring (most probable) one to be the antecedent of the given AZP.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "An Overview of the Method",
                "sec_num": "2.1"
            },
            {
                "text": "A vector representation of AZP is required when computing the ZPMN. As aforementioned, a ZP contains no actual content, it is therefore needed to employ some supplemental information to generate its initial representation. To achieve this goal, we develop the ZP-centered LSTM that encodes an AZP into a vector representation by utilizing its contextual information.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Modeling Zero Pronouns by Contexts",
                "sec_num": "2.2"
            },
            {
                "text": "Admittedly, one efficient method to model a variable-length sequence of words (context words) is to utilize a recurrent neural network (Elman, 1991) . A recurrent neural network (RNN) stores the sequence history in a real-valued history vector, which captures information of the whole sequence. LSTM (Hochreiter and Schmidhuber, 1997) is one of the classical variations of RNN that mitigate the gradient vanish problem of RNN. Assuming x = {x 1 , x 2 , ..., x n } is an input sequence, each time step t has an input x t and a hidden state h t . The internal mechanics of the LSTM is defined by:",
                "cite_spans": [
                    {
                        "start": 135,
                        "end": 148,
                        "text": "(Elman, 1991)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 300,
                        "end": 334,
                        "text": "(Hochreiter and Schmidhuber, 1997)",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Modeling Zero Pronouns by Contexts",
                "sec_num": "2.2"
            },
            {
                "text": "i t = \u03c3(W (i) \u2022 [x t ; h t-1 ] + b (i) )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Modeling Zero Pronouns by Contexts",
                "sec_num": "2.2"
            },
            {
                "text": "(1)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Modeling Zero Pronouns by Contexts",
                "sec_num": "2.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "f t = \u03c3(W (f ) \u2022 [x t ; h t-1 ] + b (f ) )",
                        "eq_num": "(2)"
                    }
                ],
                "section": "Modeling Zero Pronouns by Contexts",
                "sec_num": "2.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "o t = \u03c3(W (o) \u2022 [x t ; h t-1 ] + b (o) ) (3) Ct = tanh(W (c) \u2022 [x t ; h t-1 ] + b (c) ) (4) C t = i t Ct + f t C t-1 (5) h t = o t tanh(C t )",
                        "eq_num": "(6)"
                    }
                ],
                "section": "Modeling Zero Pronouns by Contexts",
                "sec_num": "2.2"
            },
            {
                "text": "where is an element-wise product and c) , and b (c) are the parameters of the LSTM network.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Modeling Zero Pronouns by Contexts",
                "sec_num": "2.2"
            },
            {
                "text": "W (i) , b (i) , W (f ) , b (f ) , W (o) , b (o) , W (",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Modeling Zero Pronouns by Contexts",
                "sec_num": "2.2"
            },
            {
                "text": "Intuitively, the words near an AZP generally contain richer information to express it. To bet-LSTMp LSTMp LSTMp ......",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Modeling Zero Pronouns by Contexts",
                "sec_num": "2.2"
            },
            {
                "text": "1 w 2 w 1 zp w \uf02d 1 h \uf075 \uf072 2 h \uf075\uf075 \uf072 1 zp h \uf02d \uf075\uf075\uf075\uf075\uf072 LSTMf LSTMf LSTMf ...... n w 1 n w \uf02d 1 zp w \uf02b n h \uf073\uf075 \uf075 1 n h \uf02d \uf073\uf075\uf075\uf075 1 zp h \uf02b \uf073\uf075\uf075\uf075\uf075 zp v ZP Figure 2: ZP-centered LSTM",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Modeling Zero Pronouns by Contexts",
                "sec_num": "2.2"
            },
            {
                "text": "for encoding the AZP by its context words. w i means the i-th word in the sentence, w zp-i is the i-th last word before the ZP and w zp+i is the i-th word behind the ZP.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Modeling Zero Pronouns by Contexts",
                "sec_num": "2.2"
            },
            {
                "text": "ter utilize the information of words surrounding the AZP, on the basis of the traditional LSTM, we propose the ZP-centered LSTM to encode the AZPs. A graphical representation of this model is displayed in Figure 2 . Specifically, the ZPcentered LSTM contains two standard LSTM neural networks, i.e., the LSTM p that encodes the preceding context of the AZP in a left-to-right manner, and the LSTM f that models the following context in the reverse direction. Ideally, the ZPcentered LSTM models the preceding and following contexts of the AZP separately, so that the words near the AZP are regarded as the last hidden units and could contribute more in representing the AZP. Afterward, we obtain the representation of the AZP by concatenating the last hidden vectors of LSTM p and LSTM f , which summarizes the useful contextual information centered around the AZP. Averaging or summing the last hidden vectors of LSTM p and LSTM f could also be attempted as alternatives. We regard it as the initial vector representation of the AZP and feed it to the first computational layer to go through the remaining procedures of our system.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 212,
                        "end": 213,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Modeling Zero Pronouns by Contexts",
                "sec_num": "2.2"
            },
            {
                "text": "We describe our method for generating the external memory in this subsection. For a given AZP, a set of noun phrases (NPs) is extracted as its candidate antecedents. Specifically, we generate the external memory by utilizing these candidate antecedents. One way to encode an NP candidate is to utilize its head word embedding (Chen and Ng, 2016) . However, this method has a major drawback of not utilizing contextual information that is essential for representing a phrase. Besides, some approaches (Socher et al., 2013; Sun et al., 2015) encode a phrase by utilizing the average word embedding it contains. We argue that such an averaging operation simply treats all the words in a phrase equally, which is inaccurate because some words might be more informative than others.",
                "cite_spans": [
                    {
                        "start": 326,
                        "end": 345,
                        "text": "(Chen and Ng, 2016)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 500,
                        "end": 521,
                        "text": "(Socher et al., 2013;",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 522,
                        "end": 539,
                        "text": "Sun et al., 2015)",
                        "ref_id": "BIBREF34"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generating the External Memory",
                "sec_num": "2.3"
            },
            {
                "text": "A helpful property of LSTM is that it could keep useful history information in the memory cell by exploiting input, output and forget gates to decide how to utilize and update the memory of previous information. Given a sequence of words {w 1 , w 2 , ..., w n }, previous research (Sutskever et al., 2014) utilizes the last hidden vector of LSTM to represent the information of the whole sequence. For word w t in a sequence, its corresponding hidden vector h t can capture useful information before and including w t . Candi represents the candidate antecedent. Suppose the candidate antecedent contains m words, w c[j] denotes its j-th word. w i is the i-th word in the sentence, and w c+1(-1) is the word appears immediately after (before) the candidate antecedent.",
                "cite_spans": [
                    {
                        "start": 281,
                        "end": 305,
                        "text": "(Sutskever et al., 2014)",
                        "ref_id": "BIBREF35"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generating the External Memory",
                "sec_num": "2.3"
            },
            {
                "text": "1 c w \uf02d [ ] c m w 1 c w \uf02b n w 1 w [ ] 1 c c m c v h h \uf02d \uf03d \uf02d \uf075\uf075 \uf072 ' ' [1] 1 c c c v h h \uf02b \uf03d \uf02d \uf073\uf075 \uf075 ... ... ... [1] c w Candi 1 h 1 c h \uf02d [1] c h [ ] c m h [1] c h\uf0a2 n h\uf0a2 1 c h \uf02b \uf0a2 [ ] c m h\uf0a2",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generating the External Memory",
                "sec_num": "2.3"
            },
            {
                "text": "Inspired by this, we propose a novel method to produce representations of the candidate antecedents by utilizing both their contexts and content words. Specifically, we use the subtraction between LSTM hidden vectors to encode the candi-date antecedents, as illustrated in Figure 3 . Given a candidate antecedent c with m words, two standard LSTM neural networks are employed for encoding c in the forward and backward direction, respectively. For the forward LSTM, we extract a sequence of words related with c in a left-to-right manner, i.e., {w 1 , w 2 , ..., w c-1 , w c[1] , ..., w c[m] }. Subsequently, the forward vector representation of c can be calculated as -",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 280,
                        "end": 281,
                        "text": "3",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Generating the External Memory",
                "sec_num": "2.3"
            },
            {
                "text": "\u2192 v c = h c[m] -h c-1 ,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generating the External Memory",
                "sec_num": "2.3"
            },
            {
                "text": "where h c[m] and h c-1 indicate the hidden vectors of the forward LSTM corresponding to w c[m] and w c-1 , respectively. Meanwhile, the backward LSTM models a sequence of words that are extracted in the reverse direction, that is,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generating the External Memory",
                "sec_num": "2.3"
            },
            {
                "text": "{w n , w n-1 , ..., w c+1 , w c[m] , ..., w c[1] }.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generating the External Memory",
                "sec_num": "2.3"
            },
            {
                "text": "We then perform the similar operation, computing the backward representation of c as \u2190 -",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generating the External Memory",
                "sec_num": "2.3"
            },
            {
                "text": "v c = h c[1] -h c+1 ,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generating the External Memory",
                "sec_num": "2.3"
            },
            {
                "text": "where h c[1] and h c+1 indicate the hidden vectors of the backward LSTM corresponding to w c[1] and w c+1 . Finally, we concatenate these two vectors together as the ultimate vector representation of c,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generating the External Memory",
                "sec_num": "2.3"
            },
            {
                "text": "v c = - \u2192 v c || \u2190 - v c .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generating the External Memory",
                "sec_num": "2.3"
            },
            {
                "text": "This method enables our model to encode a candidate antecedent by the information both outside and inside the phrase, which provides our model a strong ability to access to sentence-level information when modeling the candidate antecedents. In this manner, we generate the vector representations of the candidate antecedents, and regard them as the external memory, i.e., mem = {v c 1 , v c 2 , ..., v c k }.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generating the External Memory",
                "sec_num": "2.3"
            },
            {
                "text": "In this part, we introduce our attention mechanism. This strategy has been widely used in many nature language processing tasks, such as factoid question answering (Hermann et al., 2015) , entailment (Rockt\u00e4schel et al., 2015) and disfluency detection (Wang et al., 2016) . The basic idea of attention mechanism is that it assigns a weight/importance to each lower position when computing an upper-level representation (Bahdanau et al., 2015) . With the underlying intuition that not all candidate antecedents are equally relevant for representing the AZP, we employ the attention mechanism as to dynamically align the more informative candidate antecedents from the external memory, mem = {v c 1 , v c 2 , ..., v c k } with regard to the given AZP, and use them to build up the representation of the AZP.",
                "cite_spans": [
                    {
                        "start": 164,
                        "end": 186,
                        "text": "(Hermann et al., 2015)",
                        "ref_id": null
                    },
                    {
                        "start": 200,
                        "end": 226,
                        "text": "(Rockt\u00e4schel et al., 2015)",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 252,
                        "end": 271,
                        "text": "(Wang et al., 2016)",
                        "ref_id": "BIBREF37"
                    },
                    {
                        "start": 419,
                        "end": 442,
                        "text": "(Bahdanau et al., 2015)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Attention Mechanism",
                "sec_num": "2.4"
            },
            {
                "text": "As shown in Chen and Ng (2016) , traditional hand-crafted features are crucial for the resolver's success since they capture the syntactic, positional and other relationships between an AZP and its candidate antecedents. Therefore, to evaluate the importance of each candidate antecedent in a comprehensive manner, following Chen and Ng (2016) who encode hand-crafted features as inputs to their network, we integrate a set of features that are utilized in Chen and Ng (2016) , in the form of vector (v (f eature) ) into our attention model. For each multi-valued feature, we convert it into a corresponding set of binary-valued features2 . Specifically, for the t-th candidate antecedent in the memory, v ct , taking the vector representation of the AZP v zp and the corresponding feature vector v (f eature) t as inputs, we compute the attention score as",
                "cite_spans": [
                    {
                        "start": 12,
                        "end": 30,
                        "text": "Chen and Ng (2016)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 457,
                        "end": 475,
                        "text": "Chen and Ng (2016)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Attention Mechanism",
                "sec_num": "2.4"
            },
            {
                "text": "\u03b1 t = G(v ct , v zp , v (f eature) t",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Attention Mechanism",
                "sec_num": "2.4"
            },
            {
                "text": "). The scoring function G is defined by:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Attention Mechanism",
                "sec_num": "2.4"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "s t = tanh(W (att) \u2022 [v ct ; v zp ; v (f eature) t ] + b (att) ) (7) \u03b1 t = exp(s t ) k t =1 exp(s t )",
                        "eq_num": "(8)"
                    }
                ],
                "section": "Attention Mechanism",
                "sec_num": "2.4"
            },
            {
                "text": "where W (att) and b (att) are the attention parameters and k indicates the number of candidate antecedents. After obtaining the attention scores for all the candidate antecedents {a 1 , a 2 , ..., a k }, our attention layer outputs a continuous vector vec that is computed as the weighted sum of each piece of memory in mem:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Attention Mechanism",
                "sec_num": "2.4"
            },
            {
                "text": "vec = k i=1 \u03b1 i v c i (9)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Attention Mechanism",
                "sec_num": "2.4"
            },
            {
                "text": "We initialize our word embeddings with 100 dimensional ones produced by the word2vec toolkit (Mikolov et al., 2013) on the Chinese portion of the training data from the OntoNotes 5.0 corpus. We randomly initialize the parameters from a uniform distribution U (-0.03, 0.03) and minimize the training objective using stochastic gradient descent with learning rate equals to 0.01. In addition, to regularize the network, we apply L2 regularization to the network weights and dropout with a rate of 0.5 on the output of each hidden layer.",
                "cite_spans": [
                    {
                        "start": 93,
                        "end": 115,
                        "text": "(Mikolov et al., 2013)",
                        "ref_id": "BIBREF25"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training Details",
                "sec_num": "2.5"
            },
            {
                "text": "The model is trained in a supervised manner by minimizing the cross-entropy error of ZP coreference classification. Suppose the training set contains N AZPs {zp 1 , zp 2 , ..., zp N }. Let A(zp i ) denote the set of candidate antecedents of an AZP zp i , and P (c|zp i ) represents the probability of predicting candidate c as the antecedent of zp i (i.e., the attention weight of candidate antecedent c with respect to the AZP representation generated by the last hop), the loss is given by:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training Details",
                "sec_num": "2.5"
            },
            {
                "text": "loss = - N i=1 c\u2208A(zp i ) \u03b4(zp i , c)log(P (c|zp i ))",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training Details",
                "sec_num": "2.5"
            },
            {
                "text": "(10) where \u03b4(zp, c) is 1 or 0, indicating whether zp and c are coreferent.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training Details",
                "sec_num": "2.5"
            },
            {
                "text": "Datasets: Following Chen and Ng (2016, 2015) , we run experiments on the Chinese portion of the OntoNotes Release 5.0 dataset 3 used in the CoNLL 2012 Shared Task (Pradhan et al., 2012) . The dataset consists of three parts, i.e., a training set, a development set and a test set. Since only the training set and the development set contain ZP coreference annotations, we train our model on the training set and utilize the development set for testing purposes. Meanwhile, we reserve 20% of the training set as a held-out development set for tuning the hyperparameters of our network. The same experimental data setting is utilized in the baseline system (Chen and Ng, 2016) . Table 1 shows the statistics of our corpus. Besides, documents in the datasets come from six sources, i.e., broadcast news (BN), newswires (NW), broadcast conversations (BC), telephone conversations (TC), web blogs (WB) and magazines (MZ). Evaluation metrics: Same as previous studies on Chinese ZP resolution (Zhao and Ng, 2007; Chen and Ng, 2016) , we use three metrics to evaluate the quality of our model: recall, precision and F-score (denoted as R, P and F, respectively).",
                "cite_spans": [
                    {
                        "start": 20,
                        "end": 28,
                        "text": "Chen and",
                        "ref_id": null
                    },
                    {
                        "start": 29,
                        "end": 44,
                        "text": "Ng (2016, 2015)",
                        "ref_id": null
                    },
                    {
                        "start": 163,
                        "end": 185,
                        "text": "(Pradhan et al., 2012)",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 655,
                        "end": 674,
                        "text": "(Chen and Ng, 2016)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 987,
                        "end": 1006,
                        "text": "(Zhao and Ng, 2007;",
                        "ref_id": "BIBREF41"
                    },
                    {
                        "start": 1007,
                        "end": 1025,
                        "text": "Chen and Ng, 2016)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 683,
                        "end": 684,
                        "text": "1",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Experimental Setup",
                "sec_num": "3.1"
            },
            {
                "text": "3 http://catalog.ldc.upenn.edu/LDC2013T19",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Setup",
                "sec_num": "3.1"
            },
            {
                "text": "Experimental settings: We employ three Chinese ZP resolution systems as our baselines, i.e., Zhao and Ng (2007) ; Chen and Ng (2015, 2016) . Consistent with Chen and Ng (2015, 2016) , three experimental settings are designed to evaluate our approach. In Setting 1, we directly employ the gold syntactic parse trees and gold AZPs that are obtained from the OntoNotes dataset. In Setting 2, we utilize gold syntactic parse trees and system (automatically identified) AZPs4 . In Setting 3, we employ system AZP and system syntactic parse trees that obtained through the Berkeley parser5 , which is the state-of-the-art parsing model.",
                "cite_spans": [
                    {
                        "start": 93,
                        "end": 111,
                        "text": "Zhao and Ng (2007)",
                        "ref_id": "BIBREF41"
                    },
                    {
                        "start": 114,
                        "end": 122,
                        "text": "Chen and",
                        "ref_id": null
                    },
                    {
                        "start": 123,
                        "end": 138,
                        "text": "Ng (2015, 2016)",
                        "ref_id": null
                    },
                    {
                        "start": 157,
                        "end": 165,
                        "text": "Chen and",
                        "ref_id": null
                    },
                    {
                        "start": 166,
                        "end": 181,
                        "text": "Ng (2015, 2016)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Setup",
                "sec_num": "3.1"
            },
            {
                "text": "Table 2 shows the experimental results of the baseline systems and our model on entire test set. Our approach is abbreviated to ZPMN (k), where k indicates the number of hops. The best methods in each of the three experimental settings are in bold text. From Table 2 , we can observe that our approach outperforms all previous baseline systems by a substantial margin. Meanwhile, among all our models from single hop to six hops, using more computational layers could generally lead to better performance. The best performance is achieved by the model with six hops under experimental Setting 1 and 2, and with four hops in experimental Setting 3. Furthermore, the ZPMN (with six hops) significantly outperforms the state-of-the-art baseline system (Chen and Ng, 2016) under three experimental settings by 2.7%, 2.7%, and 3.9% in terms of overall F-score6 , respectively. In all words, our model is an extremely strong performer and substantially outperforms baseline methods, which demonstrate the efficiency of the proposed zero pronoun-specific memory network.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "2",
                        "ref_id": "TABREF1"
                    },
                    {
                        "start": 265,
                        "end": 266,
                        "text": "2",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Experimental Results",
                "sec_num": "3.2"
            },
            {
                "text": "It is well accepted that computational models that are composed of multiple processing layers could learn representations of data with multiple levels of abstraction (LeCun et al., 2015) . In our approach, multiple computation layers allow the model to learn representations of AZPs with multiple levels of abstraction generated by candidate antecedents. Each layer/hop retrieves important candidate antecedents, and transforms the repre- sentation at previous level into a representation at a higher, slightly more abstract level. We regard this representation as the \"key extension\" of the AZP, by which our model learns to encode the AZP in an efficient manner.",
                "cite_spans": [
                    {
                        "start": 166,
                        "end": 186,
                        "text": "(LeCun et al., 2015)",
                        "ref_id": "BIBREF23"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Results",
                "sec_num": "3.2"
            },
            {
                "text": "For per-source results, we conduct experiments by comparing the ZPMN (with six hops) with the state-of-the-art baseline system (Chen and Ng, 2016) on six sources of test data, as shown in Table 3 . The rows in Table 3 are the experimental results from different sources under the three experimental settings. In experimental Settings 1 and 3, ZPMN improves results further across all the six sources of data. Under experimental Setting 2, our model outperforms the baseline system in five of the six sources of data, only slightly underperforms in source TC. All these prove that our approach achieves a considerable improvement in Chinese ZP resolution.",
                "cite_spans": [
                    {
                        "start": 127,
                        "end": 146,
                        "text": "(Chen and Ng, 2016)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 194,
                        "end": 195,
                        "text": "3",
                        "ref_id": "TABREF2"
                    },
                    {
                        "start": 216,
                        "end": 217,
                        "text": "3",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Experimental Results",
                "sec_num": "3.2"
            },
            {
                "text": "Moreover, to evaluate the effectiveness of our methods for modeling the AZP and candidate antecedents proposed in Section 2.2 and 2.3, we compare with three models that are all simplified versions of the ZPMN, namely, ZPCon-textFree where an AZP is initially represented by its governing verb and preceding word; AntCon-tentAvg where the candidate antecedents are encoded by their averaged content word embeddings; and AntContentHead where each candi-date antecedent is represented by the embedding of its head word. To make comparison as fair as possible, we keep the other parts of these models unchanged from the ZPMN with six computational layers (hop 6). To minimize the external influence, we run experiments under experimental Setting 1 (gold parse and gold AZPs). With an intuition that contexts of an AZP provide more sufficient information than only a few specific of words in expressing the AZP, the performance of ZPContextFree is unsurprisingly worse than that of the ZPMN, which reflects the effects of the ZP-centered LSTM proposed to generate the initial representation for the AZP. In addition, the performance of AntContentAvg is relatively low. We attribute this to the model assigning the same importance to all the content words in a phrase, which causes difficulty for the model to capture informative words in a candidate antecedent. Meanwhile, AntContent-Head only models limited information when encoding candidate antecedents, thereby underperforms the ZPMN whose external memory contains sentence-level information both outside and inside the candidate antecedents. These demonstrate the utility of the method for modeling candidate antecedents.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Results",
                "sec_num": "3.2"
            },
            {
                "text": "\u8fd9\u6b21 \u8fd1 50 \u5e74 \u6765 \u5370\u5ea6 \u53d1\u751f \u7684 T \u5f3a\u70c8 \u5730b b\u7ea7 \u5f3a\uff0c\u03c6 \u6ce2\u53ca \u8303\u56f4 N\uff0c\u5370\u5ea6 a\u56fdI \u5c3c\u6cca\u5c14 \u4e5f \u53d7\u5230 \u4e86 \u5f71\u54cd c",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Attention Model Visualization",
                "sec_num": "3.3"
            },
            {
                "text": "The earthqua-e that ,5 the 5tro0ge5t o0e occur5 ,0 I0d,a w,th,0 rece0t 50 year5 ha5 a h,gh mag0,tude, \u03c6 ,0f.ue0ce5 a .arge ra0ge of area5, a0d the 0e,ghbour,0g cou0try of I0d,a .,-e Nepa. ,5 a.5o affected. To obtain a better understanding of our deep memory network, we visualize the attention weights of the ZPMN, as is shown in Figure 4 . We can observe that in the first three hops, the fourth candidate \"\u4e2d\u56fd\u7ea2\u5341\u5b57\u4f1a/Red Cross Society of China\" gains a higher attention weight than the others. Nevertheless, in hop 5 and 6, the attention weight of \"\u8fd9\u6b21...\u5f3a\u70c8\u7684\u5730\u9707/the earthquake that ... in India\" increases and the model finally predicts it correctly as the antecedent. This case illustrates the effects of multiple hops.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 337,
                        "end": 338,
                        "text": "4",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Attention Model Visualization",
                "sec_num": "3.3"
            },
            {
                "text": "4 Related Work",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Attention Model Visualization",
                "sec_num": "3.3"
            },
            {
                "text": "Chinese zero pronoun resolution. Early studies utilize heuristic rules to resolve ZPs in Chinese (Converse, 2006; Yeh and Chen, 2007) . More recently, supervised approaches have been vastly explored. Zhao and Ng (2007) first present a machine learning approach to identify and resolve ZPs. By employing the J48 decision tree algorithm, various kinds of features are integrated into their model. Kong and Zhou (2010) develop a kernel-based approach, employing context-sensitive convolution tree kernels to model syntactic information. Chen and Ng (2013) further extend the study of Zhao and Ng (2007) by proposing several novel features and introducing the coreference links between ZPs. Despite the effectiveness of feature engineering, it is labor intensive and highly relies on annotated corpus. To handle these weaknesses, Chen and Ng (2014b) propose an unsupervised method. They first recover each ZP into ten overt pronouns and then apply a ranking model to rank the antecedents. Chen and Ng (2015) propose an end-to-end unsupervised probabilistic model, utilizing a salience model to capture discourse information. In recent years, Chen and Ng (2016) develop a deep neural network approach to learn useful task-specific representations and effectively exploit lexical features through word embeddings. Different from previous studies, in this work, we propose a novel memory network to perform the task. By encoding ZPs and candidate antecedents through the composition of texts based on the representation of words, our model benefits from the semantic information when resolving the ZPs.",
                "cite_spans": [
                    {
                        "start": 97,
                        "end": 113,
                        "text": "(Converse, 2006;",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 114,
                        "end": 133,
                        "text": "Yeh and Chen, 2007)",
                        "ref_id": "BIBREF40"
                    },
                    {
                        "start": 200,
                        "end": 218,
                        "text": "Zhao and Ng (2007)",
                        "ref_id": "BIBREF41"
                    },
                    {
                        "start": 395,
                        "end": 415,
                        "text": "Kong and Zhou (2010)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 534,
                        "end": 552,
                        "text": "Chen and Ng (2013)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 581,
                        "end": 599,
                        "text": "Zhao and Ng (2007)",
                        "ref_id": "BIBREF41"
                    },
                    {
                        "start": 826,
                        "end": 845,
                        "text": "Chen and Ng (2014b)",
                        "ref_id": null
                    },
                    {
                        "start": 985,
                        "end": 1003,
                        "text": "Chen and Ng (2015)",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Zero Pronoun Resolution",
                "sec_num": "4.1"
            },
            {
                "text": "Zero pronoun resolution for other languages. There have been various studies on ZP resolution for other languages besides Chinese. Ferr\u00e1ndez and Peral (2000) propose a set of hand-crafted rules for resolving ZPs in Spanish texts. Recently, supervised approaches have been widely exploited for ZP resolution in Korean (Han, 2006) , Italian (Iida and Poesio, 2011) and Japanese (Isozaki and Hirao, 2003; Iida et al., 2006 Iida et al., , 2007;; Imamura et al., 2009; Sasano and Kurohashi, 2011; Iida and Poesio, 2011; Iida et al., 2015) . Iida et al. (2016) propose a multi-column convolutional neural network for Japanese intra-sentential subject zero anaphora resolution, where both the surface word sequence and dependency tree of a target sentence are exploited as clues in their model.",
                "cite_spans": [
                    {
                        "start": 131,
                        "end": 157,
                        "text": "Ferr\u00e1ndez and Peral (2000)",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 317,
                        "end": 328,
                        "text": "(Han, 2006)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 339,
                        "end": 362,
                        "text": "(Iida and Poesio, 2011)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 376,
                        "end": 401,
                        "text": "(Isozaki and Hirao, 2003;",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 402,
                        "end": 419,
                        "text": "Iida et al., 2006",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 420,
                        "end": 441,
                        "text": "Iida et al., , 2007;;",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 442,
                        "end": 463,
                        "text": "Imamura et al., 2009;",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 464,
                        "end": 491,
                        "text": "Sasano and Kurohashi, 2011;",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 492,
                        "end": 514,
                        "text": "Iida and Poesio, 2011;",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 515,
                        "end": 533,
                        "text": "Iida et al., 2015)",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 536,
                        "end": 554,
                        "text": "Iida et al. (2016)",
                        "ref_id": "BIBREF18"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Zero Pronoun Resolution",
                "sec_num": "4.1"
            },
            {
                "text": "Attention mechanisms have been widely used in many studies and have achieved promising performances on a variety of NLP tasks (Rockt\u00e4schel et al., 2015; Rush et al., 2015; Liu et al., 2017) . Recently, the memory network has been proposed and applied to question answering task (Weston et al., 2014) , which is defined to have four compo-nents: input (I), generalization (G), output (O) and response (R). After then, memory networks have been adopted in many other NLP tasks, such as aspect sentiment classification (Tang et al., 2016) , dialog systems (Dodge et al., 2015) , and information extraction (Xiaocheng et al., 2017) .",
                "cite_spans": [
                    {
                        "start": 126,
                        "end": 152,
                        "text": "(Rockt\u00e4schel et al., 2015;",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 153,
                        "end": 171,
                        "text": "Rush et al., 2015;",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 172,
                        "end": 189,
                        "text": "Liu et al., 2017)",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 278,
                        "end": 299,
                        "text": "(Weston et al., 2014)",
                        "ref_id": null
                    },
                    {
                        "start": 516,
                        "end": 535,
                        "text": "(Tang et al., 2016)",
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 553,
                        "end": 573,
                        "text": "(Dodge et al., 2015)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 603,
                        "end": 627,
                        "text": "(Xiaocheng et al., 2017)",
                        "ref_id": "BIBREF39"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Attention and Memory Network",
                "sec_num": "4.2"
            },
            {
                "text": "In this study, we propose a novel zero pronounspecific memory network that is capable of encoding zero pronouns into the vector representations with supplemental information obtained from their contexts and candidate antecedents. Consequently, these continuous distributed vectors provide our model with an ability to take advantage of the semantic information when resolving zero pronouns. We evaluate our method on the Chinese portion of OntoNotes 5.0 dataset and report substantial improvements over the state-ofthe-art systems in various experimental settings.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "5"
            },
            {
                "text": "A common way to do this task is to first extract a set of candidate antecedents, and then select antecedents from the candidate set. Therefore, only those candidates who are possibly the correct antecedent of the given ZP are suitable for interpreting it.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "If one feature has k different values, we will convert it into k binary features.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "In this study, we adopt the learning-based method utilized in(Chen and Ng, 2016) to identify system AZPs, including the location and identification of",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "AZPs. 5 https://github.com/slavpetrov/",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "berkeleyparser 6 All significance tests are paired t-tests, with p < 0.05.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "We greatly thank Yiming Cui and Xuxiang Wang for their tremendously helpful discussions. We also thank the anonymous reviewers for their valuable comments. This work was supported by the National High Technology Development 863 Program of China (No.2015AA015407), National Natural Science Foundation of China (No.61472105 and No.61472107).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Neural machine translation by jointly learning to align and translate",
                "authors": [
                    {
                        "first": "Dzmitry",
                        "middle": [],
                        "last": "Bahdanau",
                        "suffix": ""
                    },
                    {
                        "first": "Kyunghyun",
                        "middle": [],
                        "last": "Cho",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "International Conference on Learning Representations (ICLR)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2015. Neural machine translation by jointly learning to align and translate. In International Con- ference on Learning Representations (ICLR).",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "A neural probabilistic language model",
                "authors": [
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    },
                    {
                        "first": "Rejean",
                        "middle": [],
                        "last": "Ducharme",
                        "suffix": ""
                    },
                    {
                        "first": "Pascal",
                        "middle": [],
                        "last": "Vincent",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Journal of Machine Learning Research",
                "volume": "3",
                "issue": "",
                "pages": "1137--1155",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yoshua Bengio, Rejean Ducharme, and Pascal Vincent. 2003. A neural probabilistic language model. Jour- nal of Machine Learning Research, 3:1137-1155.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Chinese zero pronoun resolution: Some recent advances",
                "authors": [
                    {
                        "first": "Chen",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Vincent",
                        "middle": [],
                        "last": "Ng",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "EMNLP",
                "volume": "",
                "issue": "",
                "pages": "1360--1365",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chen Chen and Vincent Ng. 2013. Chinese zero pro- noun resolution: Some recent advances. In EMNLP, pages 1360-1365.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Chinese overt pronoun resolution: A bilingual approach",
                "authors": [
                    {
                        "first": "Chen",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Vincent",
                        "middle": [],
                        "last": "Ng",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "AAAI",
                "volume": "",
                "issue": "",
                "pages": "1615--1621",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chen Chen and Vincent Ng. 2014a. Chinese overt pro- noun resolution: A bilingual approach. In AAAI, pages 1615-1621.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Chinese zero pronoun resolution: An unsupervised approach combining ranking and integer linear programming",
                "authors": [
                    {
                        "first": "Chen",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Vincent",
                        "middle": [],
                        "last": "Ng",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Twenty-Eighth AAAI Conference on Artificial Intelligence",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chen Chen and Vincent Ng. 2014b. Chinese zero pro- noun resolution: An unsupervised approach com- bining ranking and integer linear programming. In Twenty-Eighth AAAI Conference on Artificial Intel- ligence.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Chinese zero pronoun resolution: A joint unsupervised discourseaware model rivaling state-of-the-art resolvers",
                "authors": [
                    {
                        "first": "Chen",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Vincent",
                        "middle": [],
                        "last": "Ng",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 53rd Annual Meeting of the ACL and the 7th International Joint Conference on Natural Language Processing",
                "volume": "2",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chen Chen and Vincent Ng. 2015. Chinese zero pro- noun resolution: A joint unsupervised discourse- aware model rivaling state-of-the-art resolvers. In Proceedings of the 53rd Annual Meeting of the ACL and the 7th International Joint Conference on Natu- ral Language Processing (Volume 2: Short Papers), page 320.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Chinese zero pronoun resolution with deep neural networks",
                "authors": [
                    {
                        "first": "Chen",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Vincent",
                        "middle": [],
                        "last": "Ng",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the 54rd Annual Meeting of the ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chen Chen and Vincent Ng. 2016. Chinese zero pro- noun resolution with deep neural networks. In Pro- ceedings of the 54rd Annual Meeting of the ACL.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Pronominal anaphora resolution in chinese",
                "authors": [
                    {
                        "first": "Susan",
                        "middle": [
                            "P"
                        ],
                        "last": "Converse",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Susan P Converse. 2006. Pronominal anaphora resolu- tion in chinese.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Evaluating prerequisite qualities for learning end-to-end dialog systems",
                "authors": [
                    {
                        "first": "Jesse",
                        "middle": [],
                        "last": "Dodge",
                        "suffix": ""
                    },
                    {
                        "first": "Andreea",
                        "middle": [],
                        "last": "Gane",
                        "suffix": ""
                    },
                    {
                        "first": "Xiang",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Antoine",
                        "middle": [],
                        "last": "Bordes",
                        "suffix": ""
                    },
                    {
                        "first": "Sumit",
                        "middle": [],
                        "last": "Chopra",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [],
                        "last": "Miller",
                        "suffix": ""
                    },
                    {
                        "first": "Arthur",
                        "middle": [],
                        "last": "Szlam",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Weston",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1511.06931"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jesse Dodge, Andreea Gane, Xiang Zhang, Antoine Bordes, Sumit Chopra, Alexander Miller, Arthur Szlam, and Jason Weston. 2015. Evaluating prereq- uisite qualities for learning end-to-end dialog sys- tems. arXiv preprint arXiv:1511.06931.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Distributed representations, simple recurrent networks, and grammatical structure",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Jeffrey L Elman",
                        "suffix": ""
                    }
                ],
                "year": 1991,
                "venue": "Machine learning",
                "volume": "7",
                "issue": "2-3",
                "pages": "195--225",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jeffrey L Elman. 1991. Distributed representations, simple recurrent networks, and grammatical struc- ture. Machine learning, 7(2-3):195-225.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "A computational approach to zero-pronouns in spanish",
                "authors": [
                    {
                        "first": "Antonio",
                        "middle": [],
                        "last": "Ferr\u00e1ndez",
                        "suffix": ""
                    },
                    {
                        "first": "Jes\u00fas",
                        "middle": [],
                        "last": "Peral",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "Proceedings of the 38th Annual Meeting on Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "166--172",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Antonio Ferr\u00e1ndez and Jes\u00fas Peral. 2000. A computa- tional approach to zero-pronouns in spanish. In Pro- ceedings of the 38th Annual Meeting on Association for Computational Linguistics, pages 166-172. As- sociation for Computational Linguistics.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Korean zero pronouns: analysis and resolution",
                "authors": [
                    {
                        "first": "Na-Rae",
                        "middle": [],
                        "last": "Han",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Na-Rae Han. 2006. Korean zero pronouns: analysis and resolution. Ph.D. thesis, Citeseer.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Teaching machines to read and comprehend",
                "authors": [
                    {
                        "first": "Karl",
                        "middle": [],
                        "last": "Moritz Hermann",
                        "suffix": ""
                    },
                    {
                        "first": "Tomas",
                        "middle": [],
                        "last": "Kocisky",
                        "suffix": ""
                    },
                    {
                        "first": "Edward",
                        "middle": [],
                        "last": "Grefenstette",
                        "suffix": ""
                    },
                    {
                        "first": "Lasse",
                        "middle": [],
                        "last": "Espeholt",
                        "suffix": ""
                    },
                    {
                        "first": "Will",
                        "middle": [],
                        "last": "Kay",
                        "suffix": ""
                    },
                    {
                        "first": "Mustafa",
                        "middle": [],
                        "last": "Suleyman",
                        "suffix": ""
                    },
                    {
                        "first": "Phil",
                        "middle": [],
                        "last": "Blunsom",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "1693--1701",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Su- leyman, and Phil Blunsom. 2015. Teaching ma- chines to read and comprehend. In Advances in Neu- ral Information Processing Systems, pages 1693- 1701.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Long short-term memory",
                "authors": [
                    {
                        "first": "Sepp",
                        "middle": [],
                        "last": "Hochreiter",
                        "suffix": ""
                    },
                    {
                        "first": "J\u00fcrgen",
                        "middle": [],
                        "last": "Schmidhuber",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "Neural computation",
                "volume": "9",
                "issue": "8",
                "pages": "1735--1780",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735-1780.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Exploiting syntactic patterns as clues in zero-anaphora resolution",
                "authors": [
                    {
                        "first": "Ryu",
                        "middle": [],
                        "last": "Iida",
                        "suffix": ""
                    },
                    {
                        "first": "Kentaro",
                        "middle": [],
                        "last": "Inui",
                        "suffix": ""
                    },
                    {
                        "first": "Yuji",
                        "middle": [],
                        "last": "Matsumoto",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "625--632",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ryu Iida, Kentaro Inui, and Yuji Matsumoto. 2006. Ex- ploiting syntactic patterns as clues in zero-anaphora resolution. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Compu- tational Linguistics, pages 625-632. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Zero-anaphora resolution by learning rich syntactic pattern features",
                "authors": [
                    {
                        "first": "Ryu",
                        "middle": [],
                        "last": "Iida",
                        "suffix": ""
                    },
                    {
                        "first": "Kentaro",
                        "middle": [],
                        "last": "Inui",
                        "suffix": ""
                    },
                    {
                        "first": "Yuji",
                        "middle": [],
                        "last": "Matsumoto",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "ACM Transactions on Asian Language Information Processing (TALIP)",
                "volume": "6",
                "issue": "4",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ryu Iida, Kentaro Inui, and Yuji Matsumoto. 2007. Zero-anaphora resolution by learning rich syntactic pattern features. ACM Transactions on Asian Lan- guage Information Processing (TALIP), 6(4):1.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "A cross-lingual ilp solution to zero anaphora resolution",
                "authors": [
                    {
                        "first": "Ryu",
                        "middle": [],
                        "last": "Iida",
                        "suffix": ""
                    },
                    {
                        "first": "Massimo",
                        "middle": [],
                        "last": "Poesio",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "1",
                "issue": "",
                "pages": "804--813",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ryu Iida and Massimo Poesio. 2011. A cross-lingual ilp solution to zero anaphora resolution. In Proceed- ings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 804-813. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Intrasentential zero anaphora resolution using subject sharing recognition",
                "authors": [
                    {
                        "first": "Ryu",
                        "middle": [],
                        "last": "Iida",
                        "suffix": ""
                    },
                    {
                        "first": "Kentaro",
                        "middle": [],
                        "last": "Torisawa",
                        "suffix": ""
                    },
                    {
                        "first": "Chikara",
                        "middle": [],
                        "last": "Hashimoto",
                        "suffix": ""
                    },
                    {
                        "first": "Jong-Hoon",
                        "middle": [],
                        "last": "Oh",
                        "suffix": ""
                    },
                    {
                        "first": "Julien",
                        "middle": [],
                        "last": "Kloetzer",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of EMNLP'15",
                "volume": "",
                "issue": "",
                "pages": "2179--2189",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ryu Iida, Kentaro Torisawa, Chikara Hashimoto, Jong- Hoon Oh, and Julien Kloetzer. 2015. Intra- sentential zero anaphora resolution using subject sharing recognition. Proceedings of EMNLP'15, pages 2179-2189.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Intrasentential subject zero anaphora resolution using multi-column convolutional neural network",
                "authors": [
                    {
                        "first": "Ryu",
                        "middle": [],
                        "last": "Iida",
                        "suffix": ""
                    },
                    {
                        "first": "Kentaro",
                        "middle": [],
                        "last": "Torisawa",
                        "suffix": ""
                    },
                    {
                        "first": "Jong-Hoon",
                        "middle": [],
                        "last": "Oh",
                        "suffix": ""
                    },
                    {
                        "first": "Canasai",
                        "middle": [],
                        "last": "Kruengkrai",
                        "suffix": ""
                    },
                    {
                        "first": "Julien",
                        "middle": [],
                        "last": "Kloetzer",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ryu Iida, Kentaro Torisawa, Jong-Hoon Oh, Cana- sai Kruengkrai, and Julien Kloetzer. 2016. Intra- sentential subject zero anaphora resolution using multi-column convolutional neural network. In Pro- ceedings of EMNLP.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Discriminative approach to predicateargument structure analysis with zero-anaphora resolution",
                "authors": [
                    {
                        "first": "Kenji",
                        "middle": [],
                        "last": "Imamura",
                        "suffix": ""
                    },
                    {
                        "first": "Kuniko",
                        "middle": [],
                        "last": "Saito",
                        "suffix": ""
                    },
                    {
                        "first": "Tomoko",
                        "middle": [],
                        "last": "Izumi",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the ACL-IJCNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kenji Imamura, Kuniko Saito, and Tomoko Izumi. 2009. Discriminative approach to predicate- argument structure analysis with zero-anaphora res- olution. In Proceedings of the ACL-IJCNLP 2009",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Japanese zero pronoun resolution based on ranking rules and machine learning",
                "authors": [
                    {
                        "first": "Hideki",
                        "middle": [],
                        "last": "Isozaki",
                        "suffix": ""
                    },
                    {
                        "first": "Tsutomu",
                        "middle": [],
                        "last": "Hirao",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Proceedings of the 2003 conference on Empirical methods in natural language processing",
                "volume": "",
                "issue": "",
                "pages": "184--191",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hideki Isozaki and Tsutomu Hirao. 2003. Japanese zero pronoun resolution based on ranking rules and machine learning. In Proceedings of the 2003 con- ference on Empirical methods in natural language processing, pages 184-191. Association for Compu- tational Linguistics.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "A tree kernelbased unified framework for chinese zero anaphora resolution",
                "authors": [
                    {
                        "first": "Fang",
                        "middle": [],
                        "last": "Kong",
                        "suffix": ""
                    },
                    {
                        "first": "Guodong",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "882--891",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Fang Kong and Guodong Zhou. 2010. A tree kernel- based unified framework for chinese zero anaphora resolution. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Pro- cessing, pages 882-891. Association for Computa- tional Linguistics.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Ask me anything: Dynamic memory networks for natural language processing",
                "authors": [
                    {
                        "first": "Ankit",
                        "middle": [],
                        "last": "Kumar",
                        "suffix": ""
                    },
                    {
                        "first": "Ozan",
                        "middle": [],
                        "last": "Irsoy",
                        "suffix": ""
                    },
                    {
                        "first": "Jonathan",
                        "middle": [],
                        "last": "Su",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [],
                        "last": "Bradbury",
                        "suffix": ""
                    },
                    {
                        "first": "Robert",
                        "middle": [],
                        "last": "English",
                        "suffix": ""
                    },
                    {
                        "first": "Brian",
                        "middle": [],
                        "last": "Pierce",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [],
                        "last": "Ondruska",
                        "suffix": ""
                    },
                    {
                        "first": "Ishaan",
                        "middle": [],
                        "last": "Gulrajani",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1506.07285"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ankit Kumar, Ozan Irsoy, Jonathan Su, James Brad- bury, Robert English, Brian Pierce, Peter On- druska, Ishaan Gulrajani, and Richard Socher. 2015. Ask me anything: Dynamic memory networks for natural language processing. arXiv preprint arXiv:1506.07285.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Deep learning",
                "authors": [
                    {
                        "first": "Yann",
                        "middle": [],
                        "last": "Lecun",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    },
                    {
                        "first": "Geoffrey",
                        "middle": [],
                        "last": "Hinton",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Nature",
                "volume": "521",
                "issue": "7553",
                "pages": "436--444",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep learning. Nature, 521(7553):436-444.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Effective deep memory networks for distant supervised relation extraction",
                "authors": [
                    {
                        "first": "Ting",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Yiming",
                        "middle": [],
                        "last": "Cui",
                        "suffix": ""
                    },
                    {
                        "first": "Qingyu",
                        "middle": [],
                        "last": "Yin",
                        "suffix": ""
                    },
                    {
                        "first": "Shijin",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Weinan",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Guoping",
                        "middle": [],
                        "last": "Hu",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ting Liu, Yiming Cui, Qingyu Yin, Shijin Wang, Weinan Zhang, and Guoping Hu. 2017. Effective deep memory networks for distant supervised rela- tion extraction. In ACL.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Distributed representations of words and phrases and their compositionality",
                "authors": [
                    {
                        "first": "Tomas",
                        "middle": [],
                        "last": "Mikolov",
                        "suffix": ""
                    },
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    },
                    {
                        "first": "Kai",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Greg",
                        "middle": [
                            "S"
                        ],
                        "last": "Corrado",
                        "suffix": ""
                    },
                    {
                        "first": "Jeff",
                        "middle": [],
                        "last": "Dean",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Advances in neural information processing systems",
                "volume": "",
                "issue": "",
                "pages": "3111--3119",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor- rado, and Jeff Dean. 2013. Distributed representa- tions of words and phrases and their compositional- ity. In Advances in neural information processing systems, pages 3111-3119.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Semantic class induction and coreference resolution",
                "authors": [
                    {
                        "first": "Vincent",
                        "middle": [],
                        "last": "Ng",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "AcL",
                "volume": "",
                "issue": "",
                "pages": "536--543",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Vincent Ng. 2007. Semantic class induction and coref- erence resolution. In AcL, pages 536-543.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Glove: Global vectors for word representation",
                "authors": [
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Pennington",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "EMNLP",
                "volume": "14",
                "issue": "",
                "pages": "1532--1543",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. In EMNLP, volume 14, pages 1532- 1543.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Conll-2012 shared task: Modeling multilingual unrestricted coreference in ontonotes",
                "authors": [
                    {
                        "first": "Alessandro",
                        "middle": [],
                        "last": "Sameer Pradhan",
                        "suffix": ""
                    },
                    {
                        "first": "Nianwen",
                        "middle": [],
                        "last": "Moschitti",
                        "suffix": ""
                    },
                    {
                        "first": "Olga",
                        "middle": [],
                        "last": "Xue",
                        "suffix": ""
                    },
                    {
                        "first": "Yuchen",
                        "middle": [],
                        "last": "Uryupina",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Joint Conference on EMNLP and CoNLL-Shared Task",
                "volume": "",
                "issue": "",
                "pages": "1--40",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Olga Uryupina, and Yuchen Zhang. 2012. Conll- 2012 shared task: Modeling multilingual unre- stricted coreference in ontonotes. In Joint Confer- ence on EMNLP and CoNLL-Shared Task, pages 1- 40. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Reasoning about entailment with neural attention",
                "authors": [
                    {
                        "first": "Tim",
                        "middle": [],
                        "last": "Rockt\u00e4schel",
                        "suffix": ""
                    },
                    {
                        "first": "Edward",
                        "middle": [],
                        "last": "Grefenstette",
                        "suffix": ""
                    },
                    {
                        "first": "Karl",
                        "middle": [
                            "Moritz"
                        ],
                        "last": "Hermann",
                        "suffix": ""
                    },
                    {
                        "first": "Tom\u00e1\u0161",
                        "middle": [],
                        "last": "Ko\u010disk\u1ef3",
                        "suffix": ""
                    },
                    {
                        "first": "Phil",
                        "middle": [],
                        "last": "Blunsom",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1509.06664"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Tim Rockt\u00e4schel, Edward Grefenstette, Karl Moritz Hermann, Tom\u00e1\u0161 Ko\u010disk\u1ef3, and Phil Blunsom. 2015. Reasoning about entailment with neural attention. arXiv preprint arXiv:1509.06664.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Sumit Chopra, and Jason Weston",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Alexander",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Rush",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "A neural attention model for abstractive sentence summarization",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1509.00685"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Alexander M Rush, Sumit Chopra, and Jason We- ston. 2015. A neural attention model for ab- stractive sentence summarization. arXiv preprint arXiv:1509.00685.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "A discriminative approach to japanese zero anaphora resolution with large-scale lexicalized case frames",
                "authors": [
                    {
                        "first": "Ryohei",
                        "middle": [],
                        "last": "Sasano",
                        "suffix": ""
                    },
                    {
                        "first": "Sadao",
                        "middle": [],
                        "last": "Kurohashi",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "IJCNLP",
                "volume": "",
                "issue": "",
                "pages": "758--766",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ryohei Sasano and Sadao Kurohashi. 2011. A dis- criminative approach to japanese zero anaphora res- olution with large-scale lexicalized case frames. In IJCNLP, pages 758-766.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Reasoning with neural tensor networks for knowledge base completion",
                "authors": [
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Danqi",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Ng",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Advances in neural information processing systems",
                "volume": "",
                "issue": "",
                "pages": "926--934",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng. 2013. Reasoning with neural ten- sor networks for knowledge base completion. In Advances in neural information processing systems, pages 926-934.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "End-to-end memory networks",
                "authors": [
                    {
                        "first": "Sainbayar",
                        "middle": [],
                        "last": "Sukhbaatar",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Weston",
                        "suffix": ""
                    },
                    {
                        "first": "Rob",
                        "middle": [],
                        "last": "Fergus",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Advances in neural information processing systems",
                "volume": "",
                "issue": "",
                "pages": "2440--2448",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. 2015. End-to-end memory networks. In Advances in neural information processing systems, pages 2440-2448.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "Modeling mention, context and entity with neural networks for entity disambiguation",
                "authors": [
                    {
                        "first": "Yaming",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Lei",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Duyu",
                        "middle": [],
                        "last": "Tang",
                        "suffix": ""
                    },
                    {
                        "first": "Nan",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Zhenzhou",
                        "middle": [],
                        "last": "Ji",
                        "suffix": ""
                    },
                    {
                        "first": "Xiaolong",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "IJCAI",
                "volume": "",
                "issue": "",
                "pages": "1333--1339",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yaming Sun, Lei Lin, Duyu Tang, Nan Yang, Zhenzhou Ji, and Xiaolong Wang. 2015. Modeling mention, context and entity with neural networks for entity disambiguation. In IJCAI, pages 1333-1339.",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "Sequence to sequence learning with neural networks",
                "authors": [
                    {
                        "first": "Ilya",
                        "middle": [],
                        "last": "Sutskever",
                        "suffix": ""
                    },
                    {
                        "first": "Oriol",
                        "middle": [],
                        "last": "Vinyals",
                        "suffix": ""
                    },
                    {
                        "first": "Quoc V",
                        "middle": [],
                        "last": "Le",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Advances in neural information processing systems",
                "volume": "",
                "issue": "",
                "pages": "3104--3112",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural net- works. In Advances in neural information process- ing systems, pages 3104-3112.",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "b36",
                "title": "Aspect level sentiment classification with deep memory network",
                "authors": [
                    {
                        "first": "Duyu",
                        "middle": [],
                        "last": "Tang",
                        "suffix": ""
                    },
                    {
                        "first": "Bing",
                        "middle": [],
                        "last": "Qin",
                        "suffix": ""
                    },
                    {
                        "first": "Ting",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Duyu Tang, Bing Qin, and Ting Liu. 2016. Aspect level sentiment classification with deep memory net- work. In EMNLP.",
                "links": null
            },
            "BIBREF37": {
                "ref_id": "b37",
                "title": "A neural attention model for disfluency detection",
                "authors": [
                    {
                        "first": "Shaolei",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Wanxiang",
                        "middle": [],
                        "last": "Che",
                        "suffix": ""
                    },
                    {
                        "first": "Ting",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers",
                "volume": "",
                "issue": "",
                "pages": "278--287",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Shaolei Wang, Wanxiang Che, and Ting Liu. 2016. A neural attention model for disfluency detection. In Proceedings of COLING 2016, the 26th Inter- national Conference on Computational Linguistics: Technical Papers, pages 278-287, Osaka, Japan. The COLING 2016 Organizing Committee.",
                "links": null
            },
            "BIBREF38": {
                "ref_id": "b38",
                "title": "Sumit Chopra, and Antoine Bordes. 2014. Memory networks",
                "authors": [
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Weston",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1410.3916"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jason Weston, Sumit Chopra, and Antoine Bor- des. 2014. Memory networks. arXiv preprint arXiv:1410.3916.",
                "links": null
            },
            "BIBREF39": {
                "ref_id": "b39",
                "title": "Effective deep memory networks for distant supervised relation extraction",
                "authors": [
                    {
                        "first": "Feng",
                        "middle": [],
                        "last": "Xiaocheng",
                        "suffix": ""
                    },
                    {
                        "first": "Guo",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    },
                    {
                        "first": "Qin",
                        "middle": [],
                        "last": "Bing",
                        "suffix": ""
                    },
                    {
                        "first": "Liu",
                        "middle": [],
                        "last": "Ting",
                        "suffix": ""
                    },
                    {
                        "first": "Liu",
                        "middle": [],
                        "last": "Yongjie",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "IJCAI",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Feng Xiaocheng, Guo Jiang, Qin Bing, Liu Ting, and Liu Yongjie. 2017. Effective deep memory networks for distant supervised relation extraction. In IJCAI.",
                "links": null
            },
            "BIBREF40": {
                "ref_id": "b40",
                "title": "Zero anaphora resolution in chinese with shallow parsing",
                "authors": [
                    {
                        "first": "Ching-Long",
                        "middle": [],
                        "last": "Yeh",
                        "suffix": ""
                    },
                    {
                        "first": "Yi-Chun",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Journal of Chinese Language and Computing",
                "volume": "17",
                "issue": "1",
                "pages": "41--56",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ching-Long Yeh and Yi-Chun Chen. 2007. Zero anaphora resolution in chinese with shallow pars- ing. Journal of Chinese Language and Computing, 17(1):41-56.",
                "links": null
            },
            "BIBREF41": {
                "ref_id": "b41",
                "title": "Identification and resolution of chinese zero pronouns: A machine learning approach",
                "authors": [
                    {
                        "first": "Shanheng",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Hwee Tou",
                        "middle": [],
                        "last": "Ng",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "EMNLP-CoNLL",
                "volume": "",
                "issue": "",
                "pages": "541--550",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Shanheng Zhao and Hwee Tou Ng. 2007. Identifica- tion and resolution of chinese zero pronouns: A ma- chine learning approach. In EMNLP-CoNLL, vol- ume 2007, pages 541-550.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF1": {
                "num": null,
                "text": "Figure 3: Illustration for modeling a candidate antecedent through its context and content words.Candi represents the candidate antecedent. Suppose the candidate antecedent contains m words, w c[j] denotes its j-th word. w i is the i-th word in the sentence, and w c+1(-1) is the word appears immediately after (before) the candidate antecedent.",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Figure 4: Example of attention weights in different hops. ZP is denoted as \u03c6. The rows show the attention weights of candidates in each hop. Darker color means higher weight.",
                "uris": null,
                "fig_num": "4",
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td/><td colspan=\"3\">Documents Sentences Words</td><td>AZPs</td></tr><tr><td>Training</td><td>1,391</td><td>36,487</td><td>756K</td><td>12,111</td></tr><tr><td>Test</td><td>172</td><td>6,083</td><td>110K</td><td>1,713</td></tr></table>",
                "type_str": "table",
                "text": "Statistics on the training and test corpus.",
                "html": null,
                "num": null
            },
            "TABREF1": {
                "content": "<table><tr><td/><td/><td/><td/><td/><td/><td/><td>Setting 1</td><td/><td/><td colspan=\"2\">Setting 2</td><td/><td/><td colspan=\"2\">Setting 3</td><td/><td/></tr><tr><td/><td/><td/><td/><td/><td/><td colspan=\"3\">Gold Parse + Gold AZP</td><td colspan=\"4\">Gold Parse + System AZP</td><td colspan=\"4\">System Parse + System AZP</td><td/></tr><tr><td/><td/><td/><td/><td/><td/><td>R</td><td>P</td><td>F</td><td>R</td><td>P</td><td/><td>F</td><td>R</td><td>P</td><td>F</td><td/><td/></tr><tr><td/><td/><td colspan=\"4\">Zhao and Ng (2007)</td><td colspan=\"9\">41.5 41.5 41.5 22.4 24.4 23.3 12.7 14.2</td><td colspan=\"2\">13.4</td><td/></tr><tr><td/><td/><td colspan=\"4\">Chen and Ng (2015)</td><td colspan=\"9\">50.0 50.4 50.2 35.7 26.2 30.3 19.6 15.5</td><td colspan=\"2\">17.3</td><td/></tr><tr><td/><td/><td colspan=\"4\">Chen and Ng (2016)</td><td colspan=\"9\">51.8 52.5 52.2 39.6 27.0 32.1 21.9 15.8</td><td colspan=\"2\">18.4</td><td/></tr><tr><td/><td/><td colspan=\"3\">ZPMN (1)</td><td/><td colspan=\"9\">53.0 53.3 53.1 37.9 30.0 33.4 27.8 17.4</td><td colspan=\"2\">21.4</td><td/></tr><tr><td/><td/><td colspan=\"3\">ZPMN (2)</td><td/><td colspan=\"9\">53.7 54.0 53.9 38.8 30.6 34.0 28.1 18.2</td><td colspan=\"2\">22.1</td><td/></tr><tr><td/><td/><td colspan=\"3\">ZPMN (3)</td><td/><td colspan=\"9\">53.9 54.2 54.1 38.6 30.4 34.2 28.2 17.7</td><td colspan=\"2\">21.7</td><td/></tr><tr><td/><td/><td colspan=\"3\">ZPMN (4)</td><td/><td colspan=\"9\">54.4 54.7 54.5 39.0 30.7 34.3 29.3 18.5</td><td colspan=\"2\">22.7</td><td/></tr><tr><td/><td/><td colspan=\"3\">ZPMN (5)</td><td/><td colspan=\"9\">54.1 54.4 54.3 38.8 30.6 34.2 28.6 17.8</td><td colspan=\"2\">22.0</td><td/></tr><tr><td/><td/><td colspan=\"3\">ZPMN (6)</td><td/><td colspan=\"9\">54.8 55.1 54.9 39.4 31.1 34.8 28.9 18.2</td><td colspan=\"2\">22.3</td><td/></tr><tr><td/><td/><td colspan=\"5\">Setting 1: Gold Parse + Gold AZP</td><td colspan=\"6\">Setting 2: Gold Parse + System AZP</td><td/><td colspan=\"5\">Setting 3: System Parse + System AZP</td></tr><tr><td/><td/><td>Baseline</td><td/><td/><td colspan=\"2\">ZPMN</td><td colspan=\"2\">Baseline</td><td/><td colspan=\"2\">ZPMN</td><td/><td/><td>Baseline</td><td/><td/><td>ZPMN</td></tr><tr><td/><td>R</td><td>P</td><td>F</td><td>R</td><td>P</td><td>F</td><td>R</td><td>P</td><td>F</td><td>R</td><td>P</td><td>F</td><td>R</td><td>P</td><td>F</td><td>R</td><td>P</td><td>F</td></tr><tr><td>NW</td><td colspan=\"18\">48.8 48.8 48.8 48.8 48.8 48.8 34.5 26.4 29.9 39.5 34.3 36.7 11.9 12.8 12.3 21.0 19.9 20.5</td></tr><tr><td>MZ</td><td colspan=\"15\">41.4 41.6 41.5 46.3 46.3 46.3 34.0 22.4 27.0 34.6 35.0 34.8 9.3 7.3 8.2</td><td colspan=\"3\">17.1 15.7 16.4</td></tr><tr><td>WB</td><td colspan=\"18\">56.3 56.3 56.3 59.8 59.8 59.8 44.7 25.1 32.2 41.2 28.7 33.8 23.9 16.1 19.2 31.3 17.6 22.6</td></tr><tr><td>BN</td><td colspan=\"18\">55.4 55.4 55.4 58.2 58.6 58.4 36.9 31.9 34.2 43.8 30.0 35.6 22.1 23.2 22.6 35.1 20.7 26.1</td></tr><tr><td>BC</td><td colspan=\"18\">50.4 51.3 50.8 52.9 53.6 53.2 37.6 25.6 30.5 35.6 29.4 32.2 21.2 14.6 17.3 25.6 15.6 19.4</td></tr><tr><td>TC</td><td colspan=\"18\">51.9 54.2 53.1 54.8 54.8 54.8 46.3 29.0 35.6 36.9 32.9 34.8 31.4 15.9 21.1 33.2 21.0 25.8</td></tr></table>",
                "type_str": "table",
                "text": "Experimental results on the test data. ZPMN represents the proposed zero pronoun-specific memory network model, and the number beside ZPMN in each row denotes the number of hops.",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Experimental results on each source of test data. The strongest F-score in each row is in bold.",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td>shows the results.</td><td/><td/></tr><tr><td/><td>R</td><td>P</td><td>F</td></tr><tr><td>ZPContextFree</td><td colspan=\"3\">53.5 53.8 53.6</td></tr><tr><td>AntContentAvg</td><td colspan=\"3\">52.6 52.9 52.7</td></tr><tr><td>AntContentHead</td><td colspan=\"3\">53.8 54.1 53.9</td></tr><tr><td>ZPMN (hop 6)</td><td colspan=\"3\">54.8 55.1 54.9</td></tr></table>",
                "type_str": "table",
                "text": "",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Experimental results of different models.",
                "html": null,
                "num": null
            }
        }
    }
}