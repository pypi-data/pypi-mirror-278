{
    "paper_id": "N09-1062",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:06:27.068423Z"
    },
    "title": "Inducing Compact but Accurate Tree-Substitution Grammars",
    "authors": [
        {
            "first": "Trevor",
            "middle": [],
            "last": "Cohn",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Edinburgh",
                "location": {
                    "addrLine": "10 Crichton Street",
                    "postCode": "EH8 9AB",
                    "settlement": "Edinburgh, Scotland",
                    "country": "United Kingdom"
                }
            },
            "email": "tcohn@inf.ed.ac.uk"
        },
        {
            "first": "Sharon",
            "middle": [],
            "last": "Goldwater",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Edinburgh",
                "location": {
                    "addrLine": "10 Crichton Street",
                    "postCode": "EH8 9AB",
                    "settlement": "Edinburgh, Scotland",
                    "country": "United Kingdom"
                }
            },
            "email": ""
        },
        {
            "first": "Phil",
            "middle": [],
            "last": "Blunsom",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Edinburgh",
                "location": {
                    "addrLine": "10 Crichton Street",
                    "postCode": "EH8 9AB",
                    "settlement": "Edinburgh, Scotland",
                    "country": "United Kingdom"
                }
            },
            "email": "pblunsom@inf.ed.ac.uk"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Tree substitution grammars (TSGs) are a compelling alternative to context-free grammars for modelling syntax. However, many popular techniques for estimating weighted TSGs (under the moniker of Data Oriented Parsing) suffer from the problems of inconsistency and over-fitting. We present a theoretically principled model which solves these problems using a Bayesian non-parametric formulation. Our model learns compact and simple grammars, uncovering latent linguistic structures (e.g., verb subcategorisation), and in doing so far out-performs a standard PCFG.",
    "pdf_parse": {
        "paper_id": "N09-1062",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Tree substitution grammars (TSGs) are a compelling alternative to context-free grammars for modelling syntax. However, many popular techniques for estimating weighted TSGs (under the moniker of Data Oriented Parsing) suffer from the problems of inconsistency and over-fitting. We present a theoretically principled model which solves these problems using a Bayesian non-parametric formulation. Our model learns compact and simple grammars, uncovering latent linguistic structures (e.g., verb subcategorisation), and in doing so far out-performs a standard PCFG.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Many successful models of syntax are based on Probabilistic Context Free Grammars (PCFGs) (e.g., Collins (1999) ). However, directly learning a PCFG from a treebank results in poor parsing performance, due largely to the unrealistic independence assumptions imposed by the context-free assumption. Considerable effort is required to coax good results from a PCFG, in the form of grammar engineering, feature selection and clever smoothing (Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Johnson, 1998) . This effort must be repeated when moving to different languages, grammar formalisms or treebanks. We propose that much of this hand-coded knowledge can be obtained automatically as an emergent property of the treebanked data, thereby reducing the need for human input in crafting the grammar.",
                "cite_spans": [
                    {
                        "start": 97,
                        "end": 111,
                        "text": "Collins (1999)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 439,
                        "end": 454,
                        "text": "(Collins, 1999;",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 455,
                        "end": 470,
                        "text": "Charniak, 2000;",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 471,
                        "end": 498,
                        "text": "Charniak and Johnson, 2005;",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 499,
                        "end": 513,
                        "text": "Johnson, 1998)",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "We present a model for automatically learning a Probabilistic Tree Substitution Grammar (PTSG), an extension to the PCFG in which non-terminals can rewrite as entire tree fragments (elementary trees), not just immediate children. These large fragments can be used to encode non-local context, such as head-lexicalisation and verb sub-categorisation. Since no annotated data is available providing TSG derivations we must induce the PTSG productions and their probabilities in an unsupervised way from an ordinary treebank. This is the same problem addressed by Data Oriented Parsing (DOP, Bod et al. (2003) ), a method which uses as productions all subtrees of the training corpus. However, many of the DOP estimation methods have serious shortcomings (Johnson, 2002) , namely inconsistency for DOP1 (Bod, 2003) and overfitting of the maximum likelihood estimate (Prescher et al., 2004) .",
                "cite_spans": [
                    {
                        "start": 589,
                        "end": 606,
                        "text": "Bod et al. (2003)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 752,
                        "end": 767,
                        "text": "(Johnson, 2002)",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 800,
                        "end": 811,
                        "text": "(Bod, 2003)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 863,
                        "end": 886,
                        "text": "(Prescher et al., 2004)",
                        "ref_id": "BIBREF22"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this paper we develop an alternative means of learning a PTSG from a treebanked corpus, with the twin objectives of a) finding a grammar which accurately models the data and b) keeping the grammar as simple as possible, with few, compact, elementary trees. This is achieved using a prior to encourage sparsity and simplicity in a Bayesian nonparametric formulation. The framework allows us to perform inference over an infinite space of grammar productions in an elegant and efficient manner. The net result is a grammar which only uses the increased context afforded by the TSG when necessary to model the data, and otherwise uses context-free rules. 1 That is, our model learns to use larger rules when the CFG's independence assumptions do not hold. This contrasts with DOP, which seeks to use all elementary trees from the training set. While our model is able, in theory, to use all such trees, in practice the data does not justify such a large grammar. Grammars that are only about twice the size of a treebank PCFG provide large gains in accuracy. We obtain additional improvements with grammars that are somewhat larger, but still much smaller than the DOP all-subtrees grammar. The rules in these grammars are intuitive, potentially offering insights into grammatical structure which could be used in, e.g., the development of syntactic ontologies and guidelines for future treebanking projects.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "A Tree Substitution Grammar2 (TSG) is a 4-tuple, G = (T, N, S, R), where T is a set of terminal symbols, N is a set of non-terminal symbols, S \u2208 N is the distinguished root non-terminal and R is a set of productions (a.k.a. rules). The productions take the form of elementary trees -tree fragments of depth \u2265 2, where each internal node is labelled with a non-terminal and each leaf is labelled with either a terminal or a non-terminal. Non-terminal leaves are called frontier non-terminals and form the substitution sites in the generative process of creating trees with the grammar.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Background and related work",
                "sec_num": "2"
            },
            {
                "text": "A derivation creates a tree by starting with the root symbol and rewriting (substituting) it with an elementary tree, then continuing to rewrite frontier non-terminals with elementary trees until there are no remaining frontier non-terminals. Unlike Context Free Grammars (CFGs) a syntax tree may not uniquely specify the derivation, as illustrated in Figure 1 which shows two derivations using different elementary trees to produce the same tree.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 359,
                        "end": 360,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Background and related work",
                "sec_num": "2"
            },
            {
                "text": "A Probabilistic Tree Substitution Grammar (PTSG), like a PCFG, assigns a probability to each rule in the grammar. The probability of a derivation is the product of the probabilities of its component rules, and the probability of a tree is the sum of the probabilities of its derivations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Background and related work",
                "sec_num": "2"
            },
            {
                "text": "As we mentioned in the introduction, work within the DOP framework seeks to induce PTSGs from treebanks by using all possible subtrees as rules, and one of a variety of methods for estimating rule probabilities. 3 Our aim of inducing compact grammars contrasts with that of DOP; moreover, we develop a probabilistic estimator which avoids the shortcomings of DOP1 and the maximum likelihood esti-mate (Bod, 2000; Bod, 2003; Johnson, 2002) . Recent work on DOP estimation also seeks to address these problems, drawing from estimation theory to solve the consistency problem (Prescher et al., 2004; Zollmann and Sima'an, 2005) , or incorporating a grammar brevity term into the learning objective (Zuidema, 2007) . Our work differs from these previous approaches in that we explicitly model a prior over grammars within a Bayesian framework. 4Models of grammar refinement (Petrov et al., 2006; Liang et al., 2007; Finkel et al., 2007 ) also aim to automatically learn latent structure underlying treebanked data. These models allow each nonterminal to be split into a number of subcategories. Theoretically the grammar space of our model is a sub-space of theirs (projecting the TSG's elementary trees into CFG rules). However, the number of nonterminals required to recreate our TSG grammars in a PCFG would be exorbitant. Consequently, our model should be better able to learn specific lexical patterns, such as full noun-phrases and verbs with their sub-categorisation frames, while theirs are better suited to learning subcategories with larger membership, such as the terminals for days of the week and noun-adjective agreement. The approaches are orthogonal, and we expect that combining a category refinement model with our TSG model would provide better performance than either approach alone.",
                "cite_spans": [
                    {
                        "start": 401,
                        "end": 412,
                        "text": "(Bod, 2000;",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 413,
                        "end": 423,
                        "text": "Bod, 2003;",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 424,
                        "end": 438,
                        "text": "Johnson, 2002)",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 573,
                        "end": 596,
                        "text": "(Prescher et al., 2004;",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 597,
                        "end": 624,
                        "text": "Zollmann and Sima'an, 2005)",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 695,
                        "end": 710,
                        "text": "(Zuidema, 2007)",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 870,
                        "end": 891,
                        "text": "(Petrov et al., 2006;",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 892,
                        "end": 911,
                        "text": "Liang et al., 2007;",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 912,
                        "end": 931,
                        "text": "Finkel et al., 2007",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Background and related work",
                "sec_num": "2"
            },
            {
                "text": "Our model is similar to the Adaptor Grammar model of Johnson et al. (2007b) , which is also a kind of Bayesian nonparametric tree-substitution grammar. However, Adaptor Grammars require that each sub-tree expands completely, with only terminal symbols as leaves, while our own model permits non-terminal frontier nodes. In addition, they disallow recursive containment of adapted non-terminals; we impose no such constraint.",
                "cite_spans": [
                    {
                        "start": 53,
                        "end": 75,
                        "text": "Johnson et al. (2007b)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Background and related work",
                "sec_num": "2"
            },
            {
                "text": "Recall the nature of our task: we are given a corpus of parse trees t and wish to infer a tree-substitution grammar G that we can use to parse new data. Rather than inferring a grammar directly, we go through an intermediate step of inferring a distribution over the derivations used to produce t, i.e., a distribution over sequences of elementary trees e that compose to form t. We will then essentially read the grammar off the elementary trees, as described in Section 5. Our problem therefore becomes one of identifying the posterior distribution of e given t, which we can do using Bayes' Rule:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model",
                "sec_num": "3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "P (e|t) \u221d P (t|e)P (e)",
                        "eq_num": "(1)"
                    }
                ],
                "section": "Model",
                "sec_num": "3"
            },
            {
                "text": "Since the sequence of elementary trees can be split into derivations, each of which completely specifies a tree, P (t|e) is either equal to 1 (when t and e are consistent) or 0 (otherwise). Therefore, the work in our model is done by the prior distribution over elementary trees. Note that this is analogous to the Bayesian model of word segmentation presented by Goldwater et al. (2006) ; indeed, the problem of inferring e from t can be viewed as a segmentation problem, where each full tree must be segmented into one or more elementary trees. As in Goldwater et al. (2006) , we wish to favour solutions employing a relatively small number of elementary units (here, elementary trees). This can be done using a Dirichlet process (DP) prior. Specifically, we define the distribution of elementary tree e with root non-terminal symbol c as",
                "cite_spans": [
                    {
                        "start": 364,
                        "end": 387,
                        "text": "Goldwater et al. (2006)",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 553,
                        "end": 576,
                        "text": "Goldwater et al. (2006)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model",
                "sec_num": "3"
            },
            {
                "text": "G c |\u03b1 c , P 0 \u223c DP(\u03b1 c , P 0 (\u2022|c)) e|c \u223c G c",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model",
                "sec_num": "3"
            },
            {
                "text": "where P 0 (\u2022|c) (the base distribution) is a distribution over the infinite space of trees rooted with c, and \u03b1 c (the concentration parameter) controls the model's tendency towards either reusing elementary trees or creating novel ones as each training instance is encountered (and consequently, the tendency to infer larger or smaller sets of elementary trees from the observed data). We discuss the base distribution in more detail below.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model",
                "sec_num": "3"
            },
            {
                "text": "Rather than representing the distribution G c explicitly, we integrate over all possible values of G c . The resulting distribution over e i , conditioned on e <i = e 1 . . . e i-1 and the root category c is:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model",
                "sec_num": "3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "p(e i |e <i , c, \u03b1 c , P 0 ) = n <i e i ,c + \u03b1 c P 0 (e i |c) n <i \u2022,c + \u03b1 c",
                        "eq_num": "(2)"
                    }
                ],
                "section": "Model",
                "sec_num": "3"
            },
            {
                "text": "where n <i e i ,c is the number number of times e i has been used to rewrite c in e <i , and n <i",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model",
                "sec_num": "3"
            },
            {
                "text": "\u2022,c = e n <i e,c is the total count of rewriting c.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model",
                "sec_num": "3"
            },
            {
                "text": "As with other DP models, ours can be viewed as a cache model, where e i can be generated in one of two ways: by drawing from the base distribution, where the probability of any particular tree is proportional to \u03b1 c P 0 (e i |c), or by drawing from a cache of previous expansions of c, where the probability of any particular expansion is proportional to the number of times that expansion has been used before. This view makes it clear that the model embodies a \"rich-get-richer\" dynamic in which a few expansions will occur with high probability, but many will occur only once or twice, as is typical of natural language. Our model is similar in this way to the Adaptor Grammar model of Johnson et al. (2007a) .",
                "cite_spans": [
                    {
                        "start": 689,
                        "end": 711,
                        "text": "Johnson et al. (2007a)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model",
                "sec_num": "3"
            },
            {
                "text": "We still need to define P 0 , the base distribution over tree fragments. We use two such distributions. The first, P M 0 generates each elementary tree by a series of random decisions: whether to expand a non-terminal, how many children to produce and their identities. The probability of expanding a nonterminal node labelled c is parameterised via a binomial distribution, Bin(\u03b2 c ), while all other decisions are chosen uniformly at random. The second base distribution, P C 0 , has a similar generative process but draws non-terminal expansions from a treebanktrained PCFG instead of a uniform distribution.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model",
                "sec_num": "3"
            },
            {
                "text": "Both choices of P 0 have the effect of biasing the model towards simple rules with a small number of internal nodes. The geometric increase in cost discourages the model from using larger rules; for this to occur these rules must yield a large increase in the data likelihood. As P C 0 incorporates PCFG probabil- ities, it assigns higher relative probability to larger rules, compared to the more draconian P M 0 .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Model",
                "sec_num": "3"
            },
            {
                "text": "To train our model we use Gibbs sampling (Geman and Geman, 1984) , a Markov chain Monte Carlo method in which variables are repeatedly sampled conditioned on the values of all other variables in the model. After a period of burn-in, each sampler state (set of variable assignments) is a sample from the posterior distribution of the model. In our case, we wish to sample from P (e|t, \u03b1, \u03b2), where (\u03b1, \u03b2) = {\u03b1 c , \u03b2 c } for all categories c. To do so, we associate a binary variable with each non-root internal node of each tree in the training set, indicating whether that node is a substitution point or not. Each substitution point forms the root of some elementary tree, as well as a frontier non-terminal of an ancestor node's elementary tree. Collectively, the training trees and substitution variables specify the sequence of elementary trees e that is the current state of the sampler. Figure 2 shows an example tree with its substitution variables, corresponding to the TSG derivation in Figure 1a . Our Gibbs sampler works by sampling the value of each substitution variable, one at a time, in random order. If d is the node associated with the substitution variable s under consideration, then the two possible values of s define two options for e: one in which d is internal to some elementary tree e M , and one in which d is the substitution site connecting two smaller trees, e A and e B . In the example in Figure 2 , when sampling the VP node, e M = (S NP (VP (V hates) NP)), e A = (S NP VP), and e B = (VP (V hates) NP). To sample a value for s, we compute the probabilities of e M and (e A , e B ), conditioned on e -: all other elementary trees in the training set that share at most a root or frontier non-terminal with e M , e A , or e B . This is easy to do because the DP is exchangeable, meaning that the probability of a set of outcomes does not depend on their ordering. Therefore, we can treat the elementary trees under consideration as the last ones to be sampled, and apply Equation 2, giving us",
                "cite_spans": [
                    {
                        "start": 41,
                        "end": 64,
                        "text": "(Geman and Geman, 1984)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 900,
                        "end": 901,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    },
                    {
                        "start": 1003,
                        "end": 1005,
                        "text": "1a",
                        "ref_id": "FIGREF0"
                    },
                    {
                        "start": 1429,
                        "end": 1430,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "4"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "P (e M |c M )= n - e M ,c M + \u03b1 c M P 0 (e M |c M ) n - \u2022,c M + \u03b1 c M",
                        "eq_num": "(3)"
                    }
                ],
                "section": "Training",
                "sec_num": "4"
            },
            {
                "text": "P (e A , e B |c A )= n - e A ,c A + \u03b1 c A P 0 (e A |c A ) n - \u2022,c A + \u03b1 c A (4) \u00d7 n - e B ,c B + \u03b4(e A , e B ) + \u03b1 c B P 0 (e B |c B ) n - \u2022,c B + \u03b4(c A , c B ) + \u03b1 c B",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "4"
            },
            {
                "text": "where c x is the root label of e x , x \u2208 {A, B, M }, the counts n -are with respect to e -, and \u03b4(\u2022, \u2022) is the Kronecker delta function, which returns 1 when its arguments are identical and 0 otherwise. We have omitted e -, t, \u03b1 and \u03b2 from the conditioning context. The \u03b4 terms in the second factor of (4) account the changes to n -that would occur after observing e A , which forms part of the conditioning context for e B . If the trees e A and e B are identical, then the count n - e B ,c B would increase by one, and if the trees share the same root non-terminal, then n - \u2022,c B would increase by one.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "4"
            },
            {
                "text": "In the previous discussion, we have assumed that the model hyperparameters, (\u03b1, \u03b2), are known. However, selecting their values by hand is extremely difficult and fitting their values on heldout data is often very time consuming. For this reason we treat the hyper-parameters as variables in our model and infer their values during training. We choose vague priors for each hyper-parameter, encoding our lack of information about their values. We treat the concentration parameters, \u03b1, as being generated by a vague gamma prior, \u03b1 c \u223c Gamma(0.001, 1000). We sample a new value \u03b1 c using a log-normal distribution with mean \u03b1 c and variance 0.3, which is then accepted into the distribution p(\u03b1 c |e, t, \u03b1 -, \u03b2) using the Metropolis-Hastings algorithm. We use a Beta prior for the binomial specification parameters, \u03b2 c \u223c Beta(1, 1). As the Beta distribution is conjugate to the binomial, we can directly resample the \u03b2 parameters from the posterior, p(\u03b2 c |e, t, \u03b1, \u03b2 -). Both the concentration and substitution parameters are resampled after every full Gibbs sampling iteration over the training trees.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Training",
                "sec_num": "4"
            },
            {
                "text": "We now turn to the problem of using the model to parse novel sentences. This requires finding the maximiser of p(t|w, t) = p(t|w, e, \u03b1, \u03b2) p(e, \u03b1, \u03b2|t) de d\u03b1 d\u03b2",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Parsing",
                "sec_num": "5"
            },
            {
                "text": "(5) where w is the sequence of words being parsed and t the resulting tree, t are the training trees and e their segmentation into elementary trees.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Parsing",
                "sec_num": "5"
            },
            {
                "text": "Unfortunately solving for the maximising parse tree in (5) is intractable. However, it can approximated using Monte Carlo techniques. Given a sample of (e, \u03b1, \u03b2)5 we can reason over the space of possible trees using a Metropolis-Hastings sampler (Johnson et al., 2007a) coupled with a Monte Carlo integral (Bod, 2003) . The first step is to sample from the posterior over derivations, p(d|w, e, \u03b1, \u03b2). This is achieved by drawing samples from an approximation grammar, p(d|w), which are then accepted to the true distribution using the Metropolis-Hastings algorithm. The second step records for each sampled derivation the CFG tree. The counts of trees constitute an approximation to p(t|w, e, \u03b1, \u03b2), from which we can recover the maximum probability tree.",
                "cite_spans": [
                    {
                        "start": 246,
                        "end": 269,
                        "text": "(Johnson et al., 2007a)",
                        "ref_id": null
                    },
                    {
                        "start": 306,
                        "end": 317,
                        "text": "(Bod, 2003)",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Parsing",
                "sec_num": "5"
            },
            {
                "text": "A natural proposal distribution, p(d|w), is the maximum a posterior (MAP) grammar given the elementary tree analysis of our training set (analogous to the PCFG approximation used in Johnson et al. (2007a) ). This is not practical because the approximation grammar is infinite: elementary trees with zero count in e still have some residual probability under P 0 . In the absence of a better alternative, we discard (most of) the zero-count rules from MAP grammar. This results in a tractable grammar representing the majority of the probability mass, from which we can sample derivations. We specifically retain all zero-count PCFG productions observed in the training set in order to provide greater robustness on unseen data.",
                "cite_spans": [
                    {
                        "start": 182,
                        "end": 204,
                        "text": "Johnson et al. (2007a)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Parsing",
                "sec_num": "5"
            },
            {
                "text": "In addition to finding the maximum probability parse (MPP), we also report results using the maximum probability derivation (MPD). While this could be calculated in the manner as described above, we",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Parsing",
                "sec_num": "5"
            },
            {
                "text": "S \u2192 A | B A \u2192 A A | B B | (A a) (A a) | (B a) (B a) B \u2192 A A | B B | (A b) (A b) | (B b) (B b)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Parsing",
                "sec_num": "5"
            },
            {
                "text": "Figure 3 : TSG used to generate synthetic data. All production probabilities are uniform.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Parsing",
                "sec_num": "5"
            },
            {
                "text": "found that using the CYK algorithm (Cocke, 1969) to find the Viterbi derivation for p yielded consistently better results. This algorithm maximises an approximated model, as opposed to approximately optimising the true model. We also present results using the tree with the maximum expected count of CFG rules (MER). This uses counts of the CFG rules applied at each span (compiled from the derivation samples) followed by a maximisation step to find the best tree. This is similar to the MAX-RULE-SUM algorithm of Petrov and Klein (2007) and maximum expected recall parsing (Goodman, 2003) .",
                "cite_spans": [
                    {
                        "start": 35,
                        "end": 48,
                        "text": "(Cocke, 1969)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 515,
                        "end": 538,
                        "text": "Petrov and Klein (2007)",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 575,
                        "end": 590,
                        "text": "(Goodman, 2003)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Parsing",
                "sec_num": "5"
            },
            {
                "text": "Before applying the model to natural language, we first create a synthetic problem to confirm that the model is capable of recovering a known tree-substitution grammar. We created 50 random trees from the TSG shown in Figure 3 . This produces binary trees with A and B internal nodes and 'a' and 'b' as terminals, such that the terminals correspond to their grand-parent non-terminal (A and a or B and b). These trees cannot be modelled accurately with a CFG because expanding A and B nodes into terminal strings requires knowing their parent's non-terminal.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 225,
                        "end": 226,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Synthetic data",
                "sec_num": null
            },
            {
                "text": "We train the model for 100 iterations of Gibbs sampling using annealing to speed convergence. Annealing amounts to smoothing the distributions in ( 3) and ( 4) by raising them to the power of 1 T . Our annealing schedule begins at T = 3 and linearly decreases to reach T = 1 in the final iteration. The sampler converges to the correct grammar, with the 10 rules from Figure 3 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 375,
                        "end": 376,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Synthetic data",
                "sec_num": null
            },
            {
                "text": "We ran our natural language experiments on the Penn treebank, using the standard data splits (sections 2-21 for training, 22 for development and 23 for testing). As our model is parameter free (the \u03b1 and \u03b2 parameters are learnt in training), we do not use the development set for pa-rameter tuning. We expect that fitting these parameters to maximise performance on the development set would lead to a small increase in generalisation performance, but at a significant cost in runtime. We replace tokens with count \u2264 1 in the training sample with one of roughly 50 generic unknown word markers which convey the token's lexical features and position in the sentence, following Petrov et al. (2006) . We also right-binarise the trees to reduce the branching factor in the same manner as Petrov et al. (2006) . The predicted trees are evaluated using EVALB 6 and we report the F1 score over labelled constituents and exact match accuracy over all sentences in the testing sets.",
                "cite_spans": [
                    {
                        "start": 676,
                        "end": 696,
                        "text": "Petrov et al. (2006)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 785,
                        "end": 805,
                        "text": "Petrov et al. (2006)",
                        "ref_id": "BIBREF21"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Penn-treebank parsing",
                "sec_num": null
            },
            {
                "text": "In our experiments, we initialised the sampler by setting all substitution variables to 0, thus treating every full tree in the training set as an elementary tree. Starting with all the variables set to 1 (corresponding to CFG expansions) or a random mix of 0s and 1s considerably increases time until convergence. We hypothesise that this is due to the sampler getting stuck in modes, from which a series of locally bad decisions are required to escape. The CFG solution seems to be a mode and therefore starting the sampler with maximal trees helps the model to avoid this mode.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Penn-treebank parsing",
                "sec_num": null
            },
            {
                "text": "For our first treebank experiments, we train on a small data sample by using only section 2 of the treebank. Bayesian methods tend to do well with small data samples, while for larger samples the benefits diminish relative to point estimates. The models were trained using Gibbs sampling for 4000 iterations with annealing linearly decreasing from T = 5 to T = 1, after which the model performed another 1000 iterations with T = 1. The final training sample was used in the parsing algorithm, which used 1000 derivation samples for each test sentence. All results are the average of five independent runs.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Small data sample",
                "sec_num": null
            },
            {
                "text": "Table 1 presents the prediction results on the development set. The baseline is a maximum likelihood PCFG. The TSG model significantly outperforms the baseline with either base distribution P M 0 or P C 0 . This confirms our hypothesis that CFGs are not sufficiently powerful to model syntax, but that the increased context afforded to the TSG can make a large difference. This result is even more impressive when considering the difference in the sizes of grammar in the PCFG versus TSG models. The TSG using P M 0 achieves its improvements with only double as many rules, as a consequence of the prior which encourages sparse solutions. The TSG results with the CFG base distribution, P C 0 , are more accurate but with larger grammars. 7 This base distribution assigns proportionally higher probability to larger rules than P M 0 , and consequently the model uses these additional rules in a larger grammar.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "1",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Small data sample",
                "sec_num": null
            },
            {
                "text": "Surprisingly, the MPP technique is not systematically better than the MPD approach, with mixed results under the F1 metric. We conjecture that this is due to sampling variance for long sentences, where repeated samples of the same tree are exceedingly rare. The MER technique results in considerably better F1 scores than either MPD or MPP, with a margin of 1.5 to 3 points. This method is less affected by sampling variance due to its use of smaller tree fragments (PCFG productions at each span).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Small data sample",
                "sec_num": null
            },
            {
                "text": "For comparison, we trained the Berkeley splitmerge (SM) parser (Petrov et al., 2006) on the same data and decoded using the Viterbi algorithm (MPD) and expected rule count (MER a.k.a. MAX-RULE-SUM). We ran two iterations of split-merge training, after which the development F1 dropped substantially (in contrast, our model is not fit to the development data). The result is an accuracy slightly below that of our model (SM \u03c4 =2 ). To be fairer to their model, we adjusted the unknown word threshold to their default setting, i.e., to apply to word types oc- curring fewer than five times (SM \u03c4 =5 ). We expect that tuning the treatment of unknown words in our model would also yield further gains. The grammar sizes are not strictly comparable, as the Berkeley binarised grammars prohibit non-binary rules, and are therefore forced to decompose each of these rules into many child rules. But the trend is clear -our model produces similar results to a state-of-the-art parser, and can do so using a small grammar. With additional rounds of split-merge training, the Berkeley grammar grows exponentially larger (200K rules after six iterations).",
                "cite_spans": [
                    {
                        "start": 63,
                        "end": 84,
                        "text": "(Petrov et al., 2006)",
                        "ref_id": "BIBREF21"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Small data sample",
                "sec_num": null
            },
            {
                "text": "We now train the model using P M 0 on the full training partition of the Penn treebank, using sections 2-21. We run the Gibbs sampler for 15,000 iterations while annealing from T = 5 to T = 1, after which we finish with 5,000 iterations at T = 1. We repeat this three times, giving an average F1 of 84.0% on the testing partition using the maximum expected rule algorithm and 83.0% using the Viterbi algorithm. This far surpasses the ML-PCFG (F1 of 70.7%), and is similar to Zuidema's (2007) DOP result of 83.8%. However, it still well below state-of-the art parsers (e.g., the Berkeley parser trained using the same data representation scores 87.7%). But we must bear in mind that these parsers have benefited from years of tuning to the Penn-treebank, where our model is much simpler and is largely untuned. We anticipate that careful data preparation and model tuning could greatly improve our model's performance. ",
                "cite_spans": [
                    {
                        "start": 475,
                        "end": 491,
                        "text": "Zuidema's (2007)",
                        "ref_id": "BIBREF25"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Full treebank",
                "sec_num": null
            },
            {
                "text": "So what kinds of non-CFG rules is the model learning? Figure 4 shows the grammar statistics for a TSG model trained on the small data sample. This model has 5611 CFG rules and 1008 TSG rules. The TSG rules vary in depth from two to nine levels with the majority between two and four. Most rules combine a small degree of lexicalisation and a variable or two. This confirms that the model is learning local structures to encode, e.g., multi-word units, subcategorisation frames and lexical agreement. The few very large rules specify full parses for sentences which were repeated in the training corpus. These complete trees are also evident in the long tail of node counts (up to 27; not shown in the figure) and counts for highly lexicalised rules (up to 8).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 61,
                        "end": 62,
                        "text": "4",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Discussion",
                "sec_num": "7"
            },
            {
                "text": "To get a better feel for the types of rules being learnt, it is instructive to examine the rules in the re- sultant grammar. Table 2 shows the top fifteen rules for three phrasal categories for the model trained on the full Penn treebank. We can see that many of these rules are larger than CFG rules, showing that the CFG rules alone are inadequate to model the treebank. Two of the NP rules encode the prevalence of preposition phrases headed by 'of' within a noun phrase, as opposed to other prepositions. Also noteworthy is the lexicalisation of the determiner, which can affect the type of NP expansion. For instance, the indefinite article is more likely to have an adjectival modifier, while the definite article appears more frequently unmodified. Highly specific tokens are also incorporated into lexicalised rules. Many of the verb phrase expansions have been lexicalised, encoding the verb's subcategorisation, as shown in Table 3 . Notice that each verb here accepts only one or a small set of argument frames, indicating that by lexicalising the verb in the VP expansion the model can find a less ambiguous and more parsimonious grammar.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 131,
                        "end": 132,
                        "text": "2",
                        "ref_id": "TABREF4"
                    },
                    {
                        "start": 940,
                        "end": 941,
                        "text": "3",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Discussion",
                "sec_num": "7"
            },
            {
                "text": "The model also learns to use large rules to describe the majority of root node expansions (we add a distinguished TOP node to all trees). These rules mostly describe cases when the S category is used for a full sentence, which most often include punctuation such as the full stop and quotation marks. In contrast, the majority of expansions for the S category do not include any punctuation. The model has learnt to differentiate between the two different classes of S -full sentence versus internal clausedue to their different expansions.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion",
                "sec_num": "7"
            },
            {
                "text": "In this work we have presented a non-parametric Bayesian model for inducing tree substitution grammars. By incorporating a structured prior over elementary rules our model is able to reason over the infinite space of all such rules, producing compact and simple grammars. In doing so our model learns local structures for latent linguistic phenomena, such as verb subcategorisation and lexical agreement. Our experimental results show that the induced grammars strongly out-perform standard PCFGs, and are comparable to a state-of-the-art parser on small data samples. While our results on the full treebank are well shy of the best available parsers, we have proposed a number of improvements to the model and the parsing algorithm that could lead to state-of-theart performance in the future.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "8"
            },
            {
                "text": "While TSGs and CFGs describe the same string languages, TSGs can describe context-sensitive tree-languages, which CFGs cannot.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "A TSG is a Tree Adjoining Grammar (TAG;Joshi (2003)) without the adjunction operator.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "TAG induction(Chiang and Bikel, 2002;Xia, 2002) also tackles a similar learning problem.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "A similar Bayesian model of TSG induction has been developed independently to this work(O'Donnell et al., 2009b; O'Donnell et al., 2009a).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Using many samples of (e, \u03b1, \u03b2) in a Monte Carlo integral is a straight-forward extension to our parsing algorithm. We did not observe a significant improvement in parsing accuracy when using a multiple samples compared to a single sample, and therefore just present results for a single sample.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "The grammar is nevertheless far smaller than the full DOP grammar on this data set, which has 700K rules.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Data-oriented parsing. Center for the Study of Language and Information -Studies in Computational Linguistics",
                "authors": [],
                "year": 2003,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Rens Bod, Remko Scha, and Khalil Sima'an, editors. 2003. Data-oriented parsing. Center for the Study of Language and Information -Studies in Computational Linguistics. University of Chicago Press.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Combining semantic and syntactic structure for language modeling",
                "authors": [
                    {
                        "first": "Rens",
                        "middle": [],
                        "last": "Bod",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "Proceedings of the 6th International Conference on Spoken Language Processing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Rens Bod. 2000. Combining semantic and syntactic structure for language modeling. In Proceedings of the 6th International Conference on Spoken Language Processing, Beijing, China.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "An efficient implementation of a new DOP model",
                "authors": [
                    {
                        "first": "Rens",
                        "middle": [],
                        "last": "Bod",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Proceedings of the 10th Conference of the European Chapter of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Rens Bod. 2003. An efficient implementation of a new DOP model. In Proceedings of the 10th Conference of the European Chapter of the Association for Compu- tational Linguistics, Budapest, Hungary, April.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Coarse-tofine n-best parsing and maxent discriminative reranking",
                "authors": [
                    {
                        "first": "Eugene",
                        "middle": [],
                        "last": "Charniak",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Johnson",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "173--180",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Eugene Charniak and Mark Johnson. 2005. Coarse-to- fine n-best parsing and maxent discriminative rerank- ing. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 173-180, Ann Arbor, Michigan, June.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "A maximum-entropy-inspired parser",
                "authors": [
                    {
                        "first": "Eugene",
                        "middle": [],
                        "last": "Charniak",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "Proceedings of 1st Meeting of the North American Chapter of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "132--139",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of 1st Meeting of the North American Chapter of the Association for Computa- tional Linguistics, pages 132-139.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Recovering latent information in treebanks",
                "authors": [
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Chiang",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [
                            "M"
                        ],
                        "last": "Bikel",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of the 19th International Conference on Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "183--189",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "David Chiang and Daniel M. Bikel. 2002. Recovering latent information in treebanks. In Proceedings of the 19th International Conference on Computational Lin- guistics, pages 183-189, Taipei, Taiwan.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Programming languages and their compilers: Preliminary notes",
                "authors": [
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Cocke",
                        "suffix": ""
                    }
                ],
                "year": 1969,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "John Cocke. 1969. Programming languages and their compilers: Preliminary notes. Courant Institute of Mathematical Sciences, New York University.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Head-driven statistical models for natural language parsing",
                "authors": [
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "John",
                        "suffix": ""
                    },
                    {
                        "first": "Collins",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Michael John Collins. 1999. Head-driven statistical models for natural language parsing. Ph.D. thesis, University of Pennsylvania, Philadelphia, PA.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "The infinite tree",
                "authors": [
                    {
                        "first": "Jenny",
                        "middle": [
                            "Rose"
                        ],
                        "last": "Finkel",
                        "suffix": ""
                    },
                    {
                        "first": "Trond",
                        "middle": [],
                        "last": "Grenager",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "272--279",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jenny Rose Finkel, Trond Grenager, and Christopher D. Manning. 2007. The infinite tree. In Proceedings of the 45th Annual Meeting of the Association of Com- putational Linguistics, pages 272-279, Prague, Czech Republic, June.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images",
                "authors": [
                    {
                        "first": "Stuart",
                        "middle": [],
                        "last": "Geman",
                        "suffix": ""
                    },
                    {
                        "first": "Donald",
                        "middle": [],
                        "last": "Geman",
                        "suffix": ""
                    }
                ],
                "year": 1984,
                "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "volume": "6",
                "issue": "",
                "pages": "721--741",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Stuart Geman and Donald Geman. 1984. Stochastic re- laxation, Gibbs distributions and the Bayesian restora- tion of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 6:721-741.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Contextual dependencies in unsupervised word segmentation",
                "authors": [
                    {
                        "first": "Sharon",
                        "middle": [],
                        "last": "Goldwater",
                        "suffix": ""
                    },
                    {
                        "first": "Thomas",
                        "middle": [
                            "L"
                        ],
                        "last": "Griffiths",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Johnson",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proceedings of COLING/ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sharon Goldwater, Thomas L. Griffiths, and Mark John- son. 2006. Contextual dependencies in unsupervised word segmentation. In Proceedings of COLING/ACL, Sydney.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Efficient parsing of DOP with PCFG-reductions",
                "authors": [
                    {
                        "first": "Joshua",
                        "middle": [],
                        "last": "Goodman",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Joshua Goodman. 2003. Efficient parsing of DOP with PCFG-reductions. In Bod et al. (Bod et al., 2003), chapter 8.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Bayesian inference for PCFGs via Markov chain Monte Carlo",
                "authors": [
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Johnson",
                        "suffix": ""
                    },
                    {
                        "first": "Thomas",
                        "middle": [],
                        "last": "Griffiths",
                        "suffix": ""
                    },
                    {
                        "first": "Sharon",
                        "middle": [],
                        "last": "Goldwater",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of Human Language Technologies 2007: The Conference of the North American Chapter",
                "volume": "",
                "issue": "",
                "pages": "139--146",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mark Johnson, Thomas Griffiths, and Sharon Goldwa- ter. 2007a. Bayesian inference for PCFGs via Markov chain Monte Carlo. In Proceedings of Hu- man Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics, pages 139-146, Rochester, New York, April.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Adaptor grammars: A framework for specifying compositional nonparametric Bayesian models",
                "authors": [
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Johnson",
                        "suffix": ""
                    },
                    {
                        "first": "Thomas",
                        "middle": [
                            "L"
                        ],
                        "last": "Griffiths",
                        "suffix": ""
                    },
                    {
                        "first": "Sharon",
                        "middle": [],
                        "last": "Goldwater",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Advances in Neural Information Processing Systems 19",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa- ter. 2007b. Adaptor grammars: A framework for spec- ifying compositional nonparametric Bayesian models. In Advances in Neural Information Processing Sys- tems 19.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "PCFG models of linguistic tree representations",
                "authors": [
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Johnson",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "Computational Linguistics",
                "volume": "24",
                "issue": "4",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mark Johnson. 1998. PCFG models of linguistic tree representations. Computational Linguistics, 24(4), December.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "The DOP estimation method is biased and inconsistent",
                "authors": [
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Johnson",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Computational Lingusitics",
                "volume": "28",
                "issue": "1",
                "pages": "71--76",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mark Johnson. 2002. The DOP estimation method is biased and inconsistent. Computational Lingusitics, 28(1):71-76, March.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Tree adjoining grammars",
                "authors": [
                    {
                        "first": "Aravind",
                        "middle": [],
                        "last": "Joshi",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "The Oxford Handbook of Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "483--501",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Aravind Joshi. 2003. Tree adjoining grammars. In Rus- lan Mikkov, editor, The Oxford Handbook of Computa- tional Linguistics, pages 483-501. Oxford University Press, Oxford, England.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "The infinite PCFG using hierarchical Dirichlet processes",
                "authors": [
                    {
                        "first": "Percy",
                        "middle": [],
                        "last": "Liang",
                        "suffix": ""
                    },
                    {
                        "first": "Slav",
                        "middle": [],
                        "last": "Petrov",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Jordan",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Klein",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)",
                "volume": "",
                "issue": "",
                "pages": "688--697",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Percy Liang, Slav Petrov, Michael Jordan, and Dan Klein. 2007. The infinite PCFG using hierarchical Dirichlet processes. In Proceedings of the 2007 Joint Confer- ence on Empirical Methods in Natural Language Pro- cessing and Computational Natural Language Learn- ing (EMNLP-CoNLL), pages 688-697, Prague, Czech Republic, June.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Computation and reuse in language",
                "authors": [
                    {
                        "first": "Timothy",
                        "middle": [
                            "J"
                        ],
                        "last": "O'donnell",
                        "suffix": ""
                    },
                    {
                        "first": "Noah",
                        "middle": [
                            "D"
                        ],
                        "last": "Goodman",
                        "suffix": ""
                    },
                    {
                        "first": "Jesse",
                        "middle": [],
                        "last": "Snedeker",
                        "suffix": ""
                    },
                    {
                        "first": "Joshua",
                        "middle": [
                            "B"
                        ],
                        "last": "Tenenbaum",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "31st Annual Conference of the Cognitive Science Society",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Timothy J. O'Donnell, Noah D. Goodman, Jesse Snedeker, and Joshua B. Tenenbaum. 2009a. Com- putation and reuse in language. In 31st Annual Con- ference of the Cognitive Science Society, Amsterdam, The Netherlands, July. To appear.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Fragment grammar: Exploring reuse in hierarchical generative processes",
                "authors": [
                    {
                        "first": "Timothy",
                        "middle": [
                            "J"
                        ],
                        "last": "O'donnell",
                        "suffix": ""
                    },
                    {
                        "first": "Noah",
                        "middle": [
                            "D"
                        ],
                        "last": "Goodman",
                        "suffix": ""
                    },
                    {
                        "first": "Joshua",
                        "middle": [
                            "B"
                        ],
                        "last": "Tenenbaum",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Timothy J. O'Donnell, Noah D. Goodman, and Joshua B. Tenenbaum. 2009b. Fragment grammar: Exploring reuse in hierarchical generative processes. Technical Report MIT-CSAIL-TR-2009-013, MIT.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Improved inference for unlexicalized parsing",
                "authors": [
                    {
                        "first": "Slav",
                        "middle": [],
                        "last": "Petrov",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Klein",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of Human Language Technologies 2007: The Conference of the North American Chapter",
                "volume": "",
                "issue": "",
                "pages": "404--411",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Slav Petrov and Dan Klein. 2007. Improved infer- ence for unlexicalized parsing. In Proceedings of Hu- man Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics, pages 404-411, Rochester, New York, April. Association for Computational Lin- guistics.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Learning accurate, compact, and interpretable tree annotation",
                "authors": [
                    {
                        "first": "Slav",
                        "middle": [],
                        "last": "Petrov",
                        "suffix": ""
                    },
                    {
                        "first": "Leon",
                        "middle": [],
                        "last": "Barrett",
                        "suffix": ""
                    },
                    {
                        "first": "Romain",
                        "middle": [],
                        "last": "Thibaux",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Klein",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "433--440",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and inter- pretable tree annotation. In Proceedings of the 21st In- ternational Conference on Computational Linguistics and 44th Annual Meeting of the Association for Com- putational Linguistics, pages 433-440, Sydney, Aus- tralia, July.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "On the statistical consistency of dop estimators",
                "authors": [
                    {
                        "first": "Detlef",
                        "middle": [],
                        "last": "Prescher",
                        "suffix": ""
                    },
                    {
                        "first": "Remko",
                        "middle": [],
                        "last": "Scha",
                        "suffix": ""
                    },
                    {
                        "first": "Khalil",
                        "middle": [],
                        "last": "Sima'an",
                        "suffix": ""
                    },
                    {
                        "first": "Andreas",
                        "middle": [],
                        "last": "Zollmann",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of the 14th Meeting of Computational Linguistics in the Netherlands",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Detlef Prescher, Remko Scha, Khalil Sima'an, and An- dreas Zollmann. 2004. On the statistical consistency of dop estimators. In Proceedings of the 14th Meet- ing of Computational Linguistics in the Netherlands, Antwerp, Belgium.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Automatic grammar generation from two different perspectives",
                "authors": [
                    {
                        "first": "Fei",
                        "middle": [],
                        "last": "Xia",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Fei Xia. 2002. Automatic grammar generation from two different perspectives. Ph.D. thesis, University of Pennsylvania.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "A consistent and efficient estimator for data-oriented parsing",
                "authors": [
                    {
                        "first": "Andreas",
                        "middle": [],
                        "last": "Zollmann",
                        "suffix": ""
                    },
                    {
                        "first": "Khalil",
                        "middle": [],
                        "last": "Sima'an",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Journal of Automata, Languages and Combinatorics",
                "volume": "10",
                "issue": "2",
                "pages": "367--388",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Andreas Zollmann and Khalil Sima'an. 2005. A consis- tent and efficient estimator for data-oriented parsing. Journal of Automata, Languages and Combinatorics, 10(2):367-388.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Parsimonious data-oriented parsing",
                "authors": [
                    {
                        "first": "Willem",
                        "middle": [],
                        "last": "Zuidema",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning",
                "volume": "",
                "issue": "",
                "pages": "551--560",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Willem Zuidema. 2007. Parsimonious data-oriented parsing. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Process- ing and Computational Natural Language Learning, pages 551-560, Prague, Czech Republic, June.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: Example derivations for the same tree, where arrows indicate substitution sites. The elementary trees used in (a) and (b) are shown below as grammar productions in bracketed tree notation.",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure2: Gibbs state e specifying the derivation in Figure1a. Each node is labelled with its substitution indicator variable.",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Figure 4: Grammar statistics for a TSG P M 0 model trained on section 2 of the Penn treebank, showing a histogram over elementary tree depth, number of nodes, terminals (lexemes) and frontier nonterminals (vars).",
                "uris": null,
                "fig_num": "4",
                "type_str": "figure"
            },
            "FIGREF3": {
                "num": null,
                "text": "(NN %)) (PP (IN of) NP) (NP ($ $) CD) (NP (DT a) (NN share)) (NP (DT the) ( NP (NN company) POS)) NP (NP QP (NN %)) (PP (IN of) NP) (NP CD (NNS cents)) (NP (DT a) (NN share)) (NP (NNP Mr.) ( NP NNP (POS 's))) NN QP (NN %) (NP (NN president)) (PP (IN of) NP) (NP (NNP Mr.) ( NP NNP (POS 's))) NP NNP ( NP NNP (NNP Corp.)) NNP ( NP NNP (NNP Inc.)) (NP (NN chairman)) (PP (IN of) NP) VP \u2192 (VBD said) (SBAR (S (NP (PRP it)) VP)) (VBD said) (SBAR (S NP VP)) (VBD rose) ( VP (NP CD (NN %)) VP) (VBP want) S (VBD said) (SBAR (S (NP (PRP he)) VP)) (VBZ plans) S (VBD said) (SBAR S) (VBZ says) (SBAR (S NP VP)) (VBP think) (SBAR S) (VBD agreed) (S (VP (TO to) (VP VB VP))) (VBZ includes) NP (VBZ says) (SBAR (S (NP (PRP he)) VP)) (VBZ wants) S (VBD closed) ( VP (PP (IN at) NP) ( VP , ADVP))",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td>PCFG</td><td>F1 60.20 4.29 EX</td><td># rules 3500</td></tr><tr><td colspan=\"2\">TSG P M 0 : MPD 72.17 11.92 MPP 71.27 12.33 MER 74.25 12.30</td><td>6609 6609 6609</td></tr><tr><td colspan=\"3\">TSG P C 0 : MPD 75.24 15.18 14923 MPP 75.30 15.74 14923 MER 76.89 15.76 14923 SM \u03c4 =2 : MPD 71.93 11.30 16168 MER 74.32 11.77 16168 SM \u03c4 =5 : MPD 75.33 15.64 39758 MER 77.93 16.94 39758</td></tr></table>",
                "type_str": "table",
                "text": "See http://nlp.cs.nyu.edu/evalb/.",
                "html": null,
                "num": null
            },
            "TABREF1": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Development results for models trained on section 2 of the Penn tree-bank, showing labelled constituent F1 and exact match accuracy. Grammar sizes are the number of rules with count \u2265 1.",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Most frequent lexicalised expansions for noun and verb phrases, excluding auxiliary verbs.",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Top fifteen expansions sorted by frequency (most frequent at top), taken from the final sample of a model trained on the full Penn treebank. Non-terminals shown with an over-bar denote a binarised sub span of the given phrase type.",
                "html": null,
                "num": null
            }
        }
    }
}