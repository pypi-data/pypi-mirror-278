{
    "paper_id": "D11-1132",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T11:54:32.266977Z"
    },
    "title": "Relation Extraction with Relation Topics",
    "authors": [
        {
            "first": "Chang",
            "middle": [],
            "last": "Wang",
            "suffix": "",
            "affiliation": {
                "laboratory": "Research Lab",
                "institution": "",
                "location": {
                    "addrLine": "19 Skyline Drive",
                    "postCode": "10532",
                    "settlement": "Hawthorne",
                    "region": "New York"
                }
            },
            "email": "wangchan@us.ibm.com"
        },
        {
            "first": "James",
            "middle": [],
            "last": "Fan",
            "suffix": "",
            "affiliation": {
                "laboratory": "Research Lab",
                "institution": "",
                "location": {
                    "addrLine": "19 Skyline Drive",
                    "postCode": "10532",
                    "settlement": "Hawthorne",
                    "region": "New York"
                }
            },
            "email": "fanj@us.ibm.com"
        },
        {
            "first": "Aditya",
            "middle": [],
            "last": "Kalyanpur",
            "suffix": "",
            "affiliation": {
                "laboratory": "Research Lab",
                "institution": "",
                "location": {
                    "addrLine": "19 Skyline Drive",
                    "postCode": "10532",
                    "settlement": "Hawthorne",
                    "region": "New York"
                }
            },
            "email": "adityakal@us.ibm.com"
        },
        {
            "first": "David",
            "middle": [],
            "last": "Gondek",
            "suffix": "",
            "affiliation": {
                "laboratory": "Research Lab",
                "institution": "",
                "location": {
                    "addrLine": "19 Skyline Drive",
                    "postCode": "10532",
                    "settlement": "Hawthorne",
                    "region": "New York"
                }
            },
            "email": "dgondek@us.ibm.com"
        },
        {
            "first": "Ibm",
            "middle": [
                "T J"
            ],
            "last": "Watson",
            "suffix": "",
            "affiliation": {
                "laboratory": "Research Lab",
                "institution": "",
                "location": {
                    "addrLine": "19 Skyline Drive",
                    "postCode": "10532",
                    "settlement": "Hawthorne",
                    "region": "New York"
                }
            },
            "email": ""
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "This paper describes a novel approach to the semantic relation detection problem. Instead of relying only on the training instances for a new relation, we leverage the knowledge learned from previously trained relation detectors. Specifically, we detect a new semantic relation by projecting the new relation's training instances onto a lower dimension topic space constructed from existing relation detectors through a three step process. First, we construct a large relation repository of more than 7,000 relations from Wikipedia. Second, we construct a set of non-redundant relation topics defined at multiple scales from the relation repository to characterize the existing relations. Similar to the topics defined over words, each relation topic is an interpretable multinomial distribution over the existing relations. Third, we integrate the relation topics in a kernel function, and use it together with SVM to construct detectors for new relations. The experimental results on Wikipedia and ACE data have confirmed that backgroundknowledge-based topics generated from the Wikipedia relation repository can significantly improve the performance over the state-of-theart relation detection approaches.",
    "pdf_parse": {
        "paper_id": "D11-1132",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "This paper describes a novel approach to the semantic relation detection problem. Instead of relying only on the training instances for a new relation, we leverage the knowledge learned from previously trained relation detectors. Specifically, we detect a new semantic relation by projecting the new relation's training instances onto a lower dimension topic space constructed from existing relation detectors through a three step process. First, we construct a large relation repository of more than 7,000 relations from Wikipedia. Second, we construct a set of non-redundant relation topics defined at multiple scales from the relation repository to characterize the existing relations. Similar to the topics defined over words, each relation topic is an interpretable multinomial distribution over the existing relations. Third, we integrate the relation topics in a kernel function, and use it together with SVM to construct detectors for new relations. The experimental results on Wikipedia and ACE data have confirmed that backgroundknowledge-based topics generated from the Wikipedia relation repository can significantly improve the performance over the state-of-theart relation detection approaches.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Detecting semantic relations in text is very useful in both information retrieval and question answering because it enables knowledge bases to be leveraged to score passages and retrieve candidate answers. To extract semantic relations from text, three types of approaches have been applied. Rule-based methods (Miller et al., 2000) employ a number of linguistic rules to capture relation patterns. Featurebased methods (Kambhatla, 2004; Zhao and Grishman, 2005) transform relation instances into a large amount of linguistic features like lexical, syntactic and semantic features, and capture the similarity between these feature vectors. Recent results mainly rely on kernel-based approaches. Many of them focus on using tree kernels to learn parse tree structure related features (Collins and Duffy, 2001; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005) . Other researchers study how different approaches can be combined to improve the extraction performance. For example, by combining tree kernels and convolution string kernels, (Zhang et al., 2006) achieved the state of the art performance on ACE (ACE, 2004) , which is a benchmark dataset for relation extraction.",
                "cite_spans": [
                    {
                        "start": 311,
                        "end": 332,
                        "text": "(Miller et al., 2000)",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 420,
                        "end": 437,
                        "text": "(Kambhatla, 2004;",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 438,
                        "end": 462,
                        "text": "Zhao and Grishman, 2005)",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 783,
                        "end": 808,
                        "text": "(Collins and Duffy, 2001;",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 809,
                        "end": 836,
                        "text": "Culotta and Sorensen, 2004;",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 837,
                        "end": 862,
                        "text": "Bunescu and Mooney, 2005)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 1040,
                        "end": 1060,
                        "text": "(Zhang et al., 2006)",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 1110,
                        "end": 1121,
                        "text": "(ACE, 2004)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Although a large set of relations have been identified, adapting the knowledge extracted from these relations for new semantic relations is still a challenging task. Most of the work on domain adaptation of relation detection has focused on how to create detectors from ground up with as little training data as possible through techniques such as bootstrapping (Etzioni et al., 2005) . We take a different approach, focusing on how the knowledge extracted from the existing relations can be reused to help build detectors for new relations. We believe by reusing knowledge one can build a more cost effective relation detector, but there are several challenges associated with reusing knowledge.",
                "cite_spans": [
                    {
                        "start": 362,
                        "end": 384,
                        "text": "(Etzioni et al., 2005)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The first challenge to address in this approach is how to construct a relation repository that has suffi-cient coverage. In this paper, we introduce a method that automatically extracts the knowledge characterizing more than 7,000 relations from Wikipedia. Wikipedia is comprehensive, containing a diverse body of content with significant depth and grows rapidly. Wikipedia's infoboxes are particularly interesting for relation extraction. They are short, manually-created, and often have a relational summary of an article: a set of attribute/value pairs describing the article's subject.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Another challenge is how to deal with overlap of relations in the repository. For example, Wikipedia authors may make up a name when a new relation is needed without checking if a similar relation has already been created. This leads to relation duplication. We refine the relation repository based on an unsupervised multiscale analysis of the correlations between existing relations. This method is parameter free, and able to produce a set of non-redundant relation topics defined at multiple scales. Similar to the topics defined over words (Blei et al., 2003) , we define relation topics as multinomial distributions over the existing relations. The relation topics extracted in our approach are interpretable, orthonormal to each other, and can be used as basis relations to re-represent the new relation instances.",
                "cite_spans": [
                    {
                        "start": 545,
                        "end": 564,
                        "text": "(Blei et al., 2003)",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The third challenge is how to use the relation topics for a relation detector. We map relation instances in the new domains to the relation topic space, resulting in a set of new features characterizing the relationship between the relation instances and existing relations. By doing so, background knowledge from the existing relations can be introduced into the new relations, which overcomes the limitations of the existing approaches when the training data is not sufficient. Our work fits in to a class of relation extraction research based on \"distant supervision\", which studies how knowledge and resources external to the target domain can be used to improve relation extraction. (Mintz et al., 2009; Jiang, 2009; Chan and Roth, 2010) . One distinction between our approach and other existing approaches is that we represent the knowledge from distant supervision using automatically constructed topics. When we test on new instances, we do not need to search against the knowledge base. In addition, our topics also model the indirect relationship between relations. Such information cannot be directly found from the knowledge base.",
                "cite_spans": [
                    {
                        "start": 688,
                        "end": 708,
                        "text": "(Mintz et al., 2009;",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 709,
                        "end": 721,
                        "text": "Jiang, 2009;",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 722,
                        "end": 742,
                        "text": "Chan and Roth, 2010)",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The contributions of this paper are three-fold. Firstly, we extract a large amount of training data for more than 7,000 semantic relations from Wikipedia (Wikipedia, 2011) and DBpedia (Auer et al., 2007) . A key part of this step is how we handle noisy data with little human effort. Secondly, we present an unsupervised way to construct a set of relation topics at multiple scales. This step is parameter free, and results in a nonredundant, multiscale relation topic space. Thirdly, we design a new kernel for relation detection by integrating the relation topics into the relation detector construction. The experimental results on Wikipedia and ACE data (ACE, 2004) have confirmed that background-knowledge-based features generated from the Wikipedia relation repository can significantly improve the performance over the state-of-the-art relation detection approaches.",
                "cite_spans": [
                    {
                        "start": 154,
                        "end": 171,
                        "text": "(Wikipedia, 2011)",
                        "ref_id": null
                    },
                    {
                        "start": 184,
                        "end": 203,
                        "text": "(Auer et al., 2007)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 635,
                        "end": 669,
                        "text": "Wikipedia and ACE data (ACE, 2004)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Our training data is from two parts: relation instances from DBpedia (extracted from Wikipedia infoboxes), and sentences describing the relations from the corresponding Wikipedia pages.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Extracting Relations from Wikipedia",
                "sec_num": "2"
            },
            {
                "text": "Since our relations correspond to Wikipedia infobox properties, we use an approach similar to that described in (Hoffmann et al., 2010) to collect positive training data instances. We assume that a Wikipedia page containing a particular infobox property is likely to express the same relation in the text of the page. We further assume that the relation is most likely expressed in the first sentence on the page which mentions the arguments of the relation. For example, the Wikipedia page for \"Albert Einstein\" contains an infobox property \"alma mater\" with value \"University of Zurich\", and the first sentence mentioning the arguments is the following: \"Einstein was awarded a PhD by the University of Zurich\", which expresses the relation. When looking for relation arguments on the page, we go beyond (sub)string matching, and use link information to match entities which may have different surface forms. Using this technique, we are able to collect a large amount of positive training instances of DBpe-dia relations.",
                "cite_spans": [
                    {
                        "start": 112,
                        "end": 135,
                        "text": "(Hoffmann et al., 2010)",
                        "ref_id": "BIBREF12"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Collecting the Training Data",
                "sec_num": "2.1"
            },
            {
                "text": "To get precise type information for the arguments of a DBpedia relation, we use the DBpedia knowledge base (Auer et al., 2007) and the associated YAGO type system (Suchanek et al., 2007) . Note that for every Wikipedia page, there is a corresponding DBpedia entry which has captured the infobox-properties as RDF triples. Some of the triples include type information, where the subject of the triple is a Wikipedia entity, and the object is a YAGO type for the entity. For example, the DBpedia entry for the entity \"Albert Einstein\" includes YAGO types such as Scientist, Philosopher, Violinist etc. These YAGO types are also linked to appropriate WordNet concepts, providing for accurate sense disambiguation. Thus, for any entity argument of a relation we are learning, we obtain sense-disambiguated type information (including super-types, sub-types, siblings etc.), which become useful generalization features in the relation detection model. Given a common noun, we can also retrieve its type information by checking against WordNet (Fellbaum, 1998) .",
                "cite_spans": [
                    {
                        "start": 107,
                        "end": 126,
                        "text": "(Auer et al., 2007)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 163,
                        "end": 186,
                        "text": "(Suchanek et al., 2007)",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 1038,
                        "end": 1054,
                        "text": "(Fellbaum, 1998)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Collecting the Training Data",
                "sec_num": "2.1"
            },
            {
                "text": "We use a set of rules together with their popularities (occurrence count) to characterize a relation. A rule representing the relations between two arguments has five components (ordered): argument 1 type, argument 2 type, noun, preposition and verb. A rule example of ActiveYearsEndDate relation (about the year that a person retired) is:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Extracting Rules from the Training Data",
                "sec_num": "2.2"
            },
            {
                "text": "person100007846|year115203791|-|in|retire.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Extracting Rules from the Training Data",
                "sec_num": "2.2"
            },
            {
                "text": "In this example, argument 1 type is per-son100007846, argument 2 type is year115203791, both of which are from YAGO type system. The key words connecting these two arguments are in (preposition) and retire (verb). This rule does not have a noun, so we use a '-' to take the position of noun. The same relation can be represented in many different ways. Another rule example characterizing the same relation is",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Extracting Rules from the Training Data",
                "sec_num": "2.2"
            },
            {
                "text": "person100007846|year115203791|retirement|-|announce.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Extracting Rules from the Training Data",
                "sec_num": "2.2"
            },
            {
                "text": "This paper only considers three types of words: noun, verb and preposition. It is straightforward to expand or simplify the rules by including more or removing some word types. The keywords are extracted from the shortest path on the dependency tree between the two arguments. A dependency tree (Figure 1 ) represents grammatical relations between words in a sentence. We used a slot grammar parser (McCord, 1995) to generate the parse tree of each sentence. Note that there could be multiple paths between two arguments in the tree. We only take the shortest path into consideration. The popularity value corresponding to each rule represents how many times this rule applies to the given relation in the given data. Multiple rules can be constructed from one relation instance, if multiple argument types are associated with the instance, or multiple nouns, prepositions or verbs are in the dependency path.",
                "cite_spans": [
                    {
                        "start": 399,
                        "end": 413,
                        "text": "(McCord, 1995)",
                        "ref_id": "BIBREF18"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 303,
                        "end": 304,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Extracting Rules from the Training Data",
                "sec_num": "2.2"
            },
            {
                "text": "To find a sentence on the Wikipedia page that is likely to express a relation in its infobox, we consider the first sentence on the page that mentions both arguments of the relation. This heuristic approach returns reasonably good results, but brings in about 20% noise in the form of false positives, which is a concern when building an accurate statistical relation detector. To address this issue, we have developed a two-step technique to automatically remove some of the noisy data. In the first step, we extract popular argument types and keywords for each DBpedia relation from the given data, and then use the combinations of those types and words to create initial rules. Many of the argument types and keywords introduced by the noisy data are often not very popular, so they can be filtered out in the first step. Not all initial rules make sense. In the second step, we check each rule against the training data to see if that rule really exists in the training data or not. If it does not exist, we filter it out. If a sentence does not have a single rule passing the above procedure, that sentence will be removed. Using the above techniques, we collect examples characterizing 7,628 DBpedia relations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Cleaning the Training Data",
                "sec_num": "2.3"
            },
            {
                "text": "An extra step extracting knowledge from the raw data is needed for two reasons: Firstly, many DBpedia relations are inter-related. For example, some DBpedia relations have a subclass relationship, e.g. \"AcademyAward\" and \"Award\"; others overlap in their scope and use, e.g., \"Composer\" and \"Artist\"; while some are equivalent, e.g., \"DateOfBirth\" and \"BirthDate\". Secondly, a fairly large amount of the noisy labels are still in the training data.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Multiscale Relation Topics",
                "sec_num": "3"
            },
            {
                "text": "To reveal the intrinsic structure of the current DBpedia relation space and filter out noise, we carried out a correlation analysis of relations in the training data, resulting in a relation topic space. Each relation topic is a multinomial distribution over the existing relations. We adapted diffusion wavelets (Coifman and Maggioni, 2006) for this task. Compared to the other well-known topic extraction methods like LDA (Blei et al., 2003) and LSI (Deerwester et al., 1990) , diffusion wavelets can efficiently extract a hierarchy of interpretable topics without any user input parameter (Wang and Mahadevan, 2009) .",
                "cite_spans": [
                    {
                        "start": 313,
                        "end": 341,
                        "text": "(Coifman and Maggioni, 2006)",
                        "ref_id": null
                    },
                    {
                        "start": 424,
                        "end": 443,
                        "text": "(Blei et al., 2003)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 452,
                        "end": 477,
                        "text": "(Deerwester et al., 1990)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 592,
                        "end": 618,
                        "text": "(Wang and Mahadevan, 2009)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Multiscale Relation Topics",
                "sec_num": "3"
            },
            {
                "text": "The diffusion wavelets algorithm constructs a compressed representation of the dyadic powers of a square matrix by representing the associated matrices at each scale not in terms of the original (unit vector) basis, but rather using a set of custom generated bases (Coifman and Maggioni, 2006) . Figure 2 summarizes the procedure to generate diffusion wavelets. Given a matrix T , the QR (a modified QR decomposition) subroutine decomposes T into an orthogonal matrix Q and a triangular matrix R such that T \u2248 QR, where |T i,k -(QR) i,k | < \u03b5 for any i and k. Columns in Q are orthonormal basis functions spanning the column space of T at the finest scale. RQ is the new representation of T with {[\u03c6 j ] \u03c60 } = DWT (T, \u03b5, J) //INPUT: //T : The input matrix. //\u03b5: Desired precision, which can be set to a small number or simply machine precision. //J: Number of levels (optional). //OUTPUT: //[\u03c6 j ] \u03c60 : extended diffusion scaling functions at scale j. \u03c6a denotes matrix T whose column space is represented using basis \u03c6 b at scale b, and row space is represented using basis \u03c6 a at scale a. The notation [\u03c6 b ] \u03c6a denotes basis \u03c6 b represented on the basis \u03c6 a . At an arbitrary scale j, we have p j basis functions, and length of each function is l j . The number of p j is determined by the intrinsic structure of the given dataset in QR routine.",
                "cite_spans": [
                    {
                        "start": 265,
                        "end": 293,
                        "text": "(Coifman and Maggioni, 2006)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 303,
                        "end": 304,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "An Overview of Diffusion Wavelets",
                "sec_num": "3.1"
            },
            {
                "text": "\u03c6 0 = I; F or j = 0 to J -1 { ([\u03c6 j+1 ] \u03c6j , [T 2 j ] \u03c6j+1 \u03c6j ) \u2190 QR([T 2 j ] \u03c6j \u03c6j , \u03b5); [\u03c6 j+1 ] \u03c60 = [\u03c6 j+1 ] \u03c6j [\u03c6 j ] \u03c60 ; [T 2 j+1 ] \u03c6j+1 \u03c6j+1 = ([T 2 j ] \u03c6j+1 \u03c6j [\u03c6 j+1 ] \u03c6j ) 2 ; }",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "An Overview of Diffusion Wavelets",
                "sec_num": "3.1"
            },
            {
                "text": "[T ] \u03c6 b \u03c6a is a p b \u00d7 l a matrix, and [\u03c6 b ] \u03c6a is an l a \u00d7 p b matrix.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "An Overview of Diffusion Wavelets",
                "sec_num": "3.1"
            },
            {
                "text": "respect to the space spanned by the columns of Q (this result is based on the matrix invariant subspace theory). At an arbitrary level j, DWT learns the basis functions from T 2 j using QR. Compared to the number of basis functions spanning T 2 j 's original column space, we usually get fewer basis functions, since some high frequency information (corresponding to the \"noise\" at that level) can be filtered out.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "An Overview of Diffusion Wavelets",
                "sec_num": "3.1"
            },
            {
                "text": "DWT then computes T 2 j+1 using the low frequency representation of T 2 j and the procedure repeats.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "An Overview of Diffusion Wavelets",
                "sec_num": "3.1"
            },
            {
                "text": "Assume we have M relations, and the i th of them is characterized by m i <rule, popularity> pairs. We use s(a, b) to represent the similarity between the a th and b th relations. To compute s(a, b), we first normalize the popularities for each relation, and then look for the rules that are shared by both relation a and b. We use the product of corresponding popularity values to represent the similarity score between two relations with respect to each common rule. s(a, b) is set to the sum of such scores over all common rules. The relation-relation correlation matrix S is constructed as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Relation Correlations",
                "sec_num": null
            },
            {
                "text": "S = [ s(1, 1) \u2022 \u2022 \u2022 s(1, M ) \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 s(M, 1) \u2022 \u2022 \u2022 s(M, M ) ]",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Relation Correlations",
                "sec_num": null
            },
            {
                "text": "We have more than 200, 000 argument types, tens of thousands of distinct nouns, prepositions, and verbs, so we potentially have trillions of distinct rules. One rule may appear in multiple relations. The more rules two relations share, the more related two relations should be. The rules shared across different relations offer us a novel way to model the correlations between different relations, and further allow us to create relation topics. The rules can also be simplified. For example, we may treat argument 1 , argument 2 , noun, preposition and verb separately. This results in simple rules that only involve in one argument type or word. The correlations between relations are then computed only based on one particular component like argument 1 , noun, etc.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Relation Correlations",
                "sec_num": null
            },
            {
                "text": "Matrix S models the correlations between relations in the training data. Once S is constructed, we adapt diffusion wavelets (Coifman and Maggioni, 2006) to automatically extract the basis functions spanning the original column space of S at multiple scales. The key strength of the approach is that it is data-driven, largely parameter-free and can automatically determine the number of levels of the topical hierarchy, as well as the topics at each level. However, to apply diffusion wavelets to S, we first need to show that S is a positive semi-definite matrix. This property guarantees that all eigenvalues of S are \u2265 0. Depending on the way we formalize the rules, the methods to validate this property are slightly different. When we treat argument 1 , argument 2 , noun, preposition and verb separately, it is straightforward to see the property holds. In Theorem 1, we show the property also holds when we use more complicated rules (using the 5-tuple rule in Section 2.2 as an example in the proof).",
                "cite_spans": [
                    {
                        "start": 124,
                        "end": 152,
                        "text": "(Coifman and Maggioni, 2006)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Theoretical Analysis",
                "sec_num": null
            },
            {
                "text": "Theorem 1. S is a Positive Semi-Definite matrix.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Theoretical Analysis",
                "sec_num": null
            },
            {
                "text": "Proof: An arbitrary rule r i is uniquely characterized by a five tuple: argument 1 type| argument 2 type| noun| preposition| verb. Since the number of distinct argument types and words are constants, the number of all possible rules is also a constant: R.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Theoretical Analysis",
                "sec_num": null
            },
            {
                "text": "If we treat each rule as a feature, then the set of rules characterizing an arbitrary relation r i can be represented as a point",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Theoretical Analysis",
                "sec_num": null
            },
            {
                "text": "[p 1 i , \u2022 \u2022 \u2022 , p R i ]",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Theoretical Analysis",
                "sec_num": null
            },
            {
                "text": "in a latent R dimensional rule space, where p j i represents the popularity of rule j in relation r i in the given data.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Theoretical Analysis",
                "sec_num": null
            },
            {
                "text": "We can verify that the way to compute s(a, b) is the same as",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Theoretical Analysis",
                "sec_num": null
            },
            {
                "text": "s(a, b) =< [p 1 a \u2022 \u2022 \u2022 p R a ], [p 1 b \u2022 \u2022 \u2022 p R b ] >, where < \u2022,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Theoretical Analysis",
                "sec_num": null
            },
            {
                "text": "\u2022 > is the cosine similarity (kernel). It follows directly from the definition of positive semidefinite matrix (PSD) that S is PSD (Sch\u00f6lkopf and Smola, 2002) .",
                "cite_spans": [
                    {
                        "start": 131,
                        "end": 158,
                        "text": "(Sch\u00f6lkopf and Smola, 2002)",
                        "ref_id": "BIBREF22"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Theoretical Analysis",
                "sec_num": null
            },
            {
                "text": "In our approach, we construct multiscale relation topics by applying DWT to decompose S/\u03bb max (S), where \u03bb max (S) represents the largest eigenvalue of S. Theorem 2 shows that this decomposition will converge, resulting in a relation topic hierarchy with one single topic at the top level.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Theoretical Analysis",
                "sec_num": null
            },
            {
                "text": "Theorem 2. Let \u03bb max (S) represent the largest eigenvalue of matrix S, then DWT (S/\u03bb max (S), \u03b5) produces a set of nested subspaces of the column space of S, and the highest level of the resulting subspace hierarchy is spanned by one basis function.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Theoretical Analysis",
                "sec_num": null
            },
            {
                "text": "Proof: From Theorem 1, we know that S is a PSD matrix. This means \u03bb max (S) \u2208 [0, +\u221e) (all eigenvalues of S are non-negative). This further implies that \u039b(S)/\u03bb max (S) \u2208 [0, 1], where \u039b(S) represents any eigenvalue of S.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Theoretical Analysis",
                "sec_num": null
            },
            {
                "text": "The idea underlying diffusion wavelets is based on decomposing the spectrum of an input matrix into various spectral bands, spanned by basis functions (Coifman and Maggioni, 2006) . Let T = S/\u03bb max (S). In Figure 2 , we construct spectral bands of eigenvalues, whose associated eigenvectors span the corresponding subspaces. Define dyadic spatial scales t j as",
                "cite_spans": [
                    {
                        "start": 151,
                        "end": 179,
                        "text": "(Coifman and Maggioni, 2006)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 213,
                        "end": 214,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Theoretical Analysis",
                "sec_num": null
            },
            {
                "text": "t j = j t=0 2 t = 2 j+1 -1, j \u2265 0 .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Theoretical Analysis",
                "sec_num": null
            },
            {
                "text": "At each spatial scale, the spectral band is defined as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Theoretical Analysis",
                "sec_num": null
            },
            {
                "text": "\u2126 j (T ) = {\u03bb \u2208 \u039b(T ), \u03bb tj \u2265 \u03b5},",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Theoretical Analysis",
                "sec_num": null
            },
            {
                "text": "where \u039b(T ) represents any eigenvalue of T , and \u03b5 \u2208 (0, 1) is a pre-defined threshold in Figure 2 . We can now associate with each of the spectral bands a vector subspace spanned by the corresponding eigenvectors:",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 97,
                        "end": 98,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Theoretical Analysis",
                "sec_num": null
            },
            {
                "text": "V j = {\u03be \u03bb : \u03bb \u2208 \u039b(T ), \u03bb tj \u2265 \u03b5} , j \u2265 0 .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Theoretical Analysis",
                "sec_num": null
            },
            {
                "text": "In the limit, we obtain",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Theoretical Analysis",
                "sec_num": null
            },
            {
                "text": "lim j\u2192\u221e V j = {\u03be \u03bb : \u03bb = 1}",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Theoretical Analysis",
                "sec_num": null
            },
            {
                "text": "That is, the highest level of the resulting subspace hierarchy is spanned by the eigenvector associated with the largest eigenvalue of T .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Theoretical Analysis",
                "sec_num": null
            },
            {
                "text": "This result shows that the multiscale analysis of the relation space will automatically terminate at the level spanned by one basis, which is the most popular relation topic in the training data.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Theoretical Analysis",
                "sec_num": null
            },
            {
                "text": "We first create a set of rules to characterize each input relation. Since these rules may occur in multiple relations, they provide a way to model the cooccurrence relationship between different relations. Our algorithm starts with the relation co-occurrence matrix and then repeatedly applies QR decomposition to learn the topics at the current level while at the same time modifying the matrix to focus more on low-frequency indirect co-occurrences (between relations) for the next level. Running DWT is equivalent to running a Markov chain on the input data forward in time, integrating the local geometry and therefore revealing the relevant geometric structures of the whole data set at different scales. At scale j, the representation of T 2 j+1 is compressed based on the amount of remaining information and the desired precision. This procedure is illustrated in Figure 3. In the resulting topic space, instances with related relations will be grouped together. This approach may significantly help us detect new relations, since it potentially expands the information brought in by new relation instances from making use of the knowledge extracted from the existing relation repository.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "High Level Explanation",
                "sec_num": "3.3"
            },
            {
                "text": "As shown in Figure 3 , the topic spaces at different levels are spanned by a different number of basis functions. These numbers reveal the dimensions of the relevant geometric structures of data at different levels. These numbers are completely data-driven: the diffusion wavelets approach can automatically find the number of levels and simultaneously generate the topics at each level. Experiments show that most multiscale topics are interpretable (due to the sparsity of the scaling functions), such that we can interpret the topics at different scales and select the best scale for embedding. Compared to bootstrapping approach, our approach is accumulative; that is as the system learns more relations, it gets better at learning new relations. Because our approach takes advantage of the previously learned relations, and the topic space is enriched as we learn more and more relations.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 19,
                        "end": 20,
                        "text": "3",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Benefits",
                "sec_num": "3.4"
            },
            {
                "text": "\u00a1 \u00a3\u00a2 \u00a4 \u00a5 \u00a7\u00a6 \u00a9 ! #\" $ \" !% & ' )( $ % 10 0 32 54 ! # \u00a3 6 7 98 @ BA C D E F G 9H I BP Q D R F S TQ U \u00a3V D E D E W X Y !a #Y . . .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Benefits",
                "sec_num": "3.4"
            },
            {
                "text": "We use diffusion wavelets (DWT) rather than other hierarchy topic models like hLDA (Blei et al., 2004) to extract relation topics for two reasons. First, DWT is parameter free while other models need some user-input parameters like hierarchy level. Second, DWT is more efficient than the other models. After the relation correlation matrix is constructed, DWT only needs a couple of minutes to extract multiscale topics on a regular computer. A direct experimental comparison between DWT and hLDA can be found in (Wang and Mahadevan, 2009) .",
                "cite_spans": [
                    {
                        "start": 83,
                        "end": 102,
                        "text": "(Blei et al., 2004)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 513,
                        "end": 539,
                        "text": "(Wang and Mahadevan, 2009)",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Benefits",
                "sec_num": "3.4"
            },
            {
                "text": "Multiscale Relation Topics ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Constructing Relation Detectors with",
                "sec_num": "4"
            },
            {
                "text": "X t = [< r t (1), x t (1) >, \u2022 \u2022 \u2022 , < r t (M ), x t (M ) >],",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Constructing Relation Detectors with",
                "sec_num": "4"
            },
            {
                "text": "where < \u2022, \u2022 > is the cosine similarity of two vectors. At level k, the embedding of",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Constructing Relation Detectors with",
                "sec_num": "4"
            },
            {
                "text": "x is E k x = [E k Xarg 1 , E k Xarg 2 , E k Xnoun , E k X verb ], where E k Xt = ([\u03c6 k ] \u03c6 0 ) T X t , and [\u03c6 k ] \u03c6 0 is defined in Figure 2.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Constructing Relation Detectors with",
                "sec_num": "4"
            },
            {
                "text": "We combine E k",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Design New Kernel Using Topic Features",
                "sec_num": "4.2"
            },
            {
                "text": "x with 3 existing kernels (K Argument , K P ath and K BOW ) to create a new kernel for relation detection.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Design New Kernel Using Topic Features",
                "sec_num": "4.2"
            },
            {
                "text": "(1) K Argument matches two arguments, it returns the number of common argument types that the input arguments share.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Design New Kernel Using Topic Features",
                "sec_num": "4.2"
            },
            {
                "text": "(2) K P ath matches two dependency paths. This kernel is formally defined in (Zhao and Grishman, 2005) . We extended this kernel by also matching the common nouns, prepositions and verbs in the dependency paths. We assign weight 1 to verbs, 0.5 to nouns and prepositions.",
                "cite_spans": [
                    {
                        "start": 77,
                        "end": 102,
                        "text": "(Zhao and Grishman, 2005)",
                        "ref_id": "BIBREF27"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Design New Kernel Using Topic Features",
                "sec_num": "4.2"
            },
            {
                "text": "(3) K BOW models the number of common nouns, prepositions and verbs in the given sentences but not in the dependency paths. Since these words are not as important as the words inside the dependency path, we assign weight 0.25 to them.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Design New Kernel Using Topic Features",
                "sec_num": "4.2"
            },
            {
                "text": "(4) K T F k (x, y) =< E k",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Design New Kernel Using Topic Features",
                "sec_num": "4.2"
            },
            {
                "text": "x , E k y >, where x, y are two input relation instances, and < \u2022, \u2022 > models the cosine similarity of two vectors. T F stands for topic feature.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Design New Kernel Using Topic Features",
                "sec_num": "4.2"
            },
            {
                "text": "(5) The final kernel used in this paper is",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Design New Kernel Using Topic Features",
                "sec_num": "4.2"
            },
            {
                "text": "\u03b1 1 K Argument + \u03b1 2 K P ath + \u03b1 3 K BOW + \u03b1 4 K T F k ,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Design New Kernel Using Topic Features",
                "sec_num": "4.2"
            },
            {
                "text": "where \u03b1 i can be tuned for each individual domain. In this paper, we set \u03b1 i = 1 for i \u2208 {1, 2, 3, 4}. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Design New Kernel Using Topic Features",
                "sec_num": "4.2"
            },
            {
                "text": "Given the training data from a new relation, project the data onto level k of the multiscale topic hierarchy, where k is chosen by users (Section 4.1). Apply SVM classifiers together with our kernel (Section 4.2) to create detectors for new relations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Construct relation detectors for new relations.",
                "sec_num": "3."
            },
            {
                "text": "We used SVMLight (Joachims, 1999) together with the user defined kernel setting in our approach. The trade-off parameter between training error and margin c is 1 for all experiments. Our approach to learn multiscale relation topics is largely parameter free. The only parameter to be set is the precision \u03b5 = 10 -5 , which is also the default value in the diffusion wavelets implementation.",
                "cite_spans": [
                    {
                        "start": 17,
                        "end": 33,
                        "text": "(Joachims, 1999)",
                        "ref_id": "BIBREF15"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Results",
                "sec_num": "5"
            },
            {
                "text": "Following the approach discussed in Section 2.1, we collect more than 620,000 training instances for ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Multiscale Relation Topics",
                "sec_num": "5.1"
            },
            {
                "text": "\u2022 \u2022 \u2022 ],",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Multiscale Relation Topics",
                "sec_num": "5.1"
            },
            {
                "text": "where doctoraladvisor is a DBpedia relation and 0.683366 is its contribution to the topic. The length of this relation vector is 7,628. We only list the top 10 relations here.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Learning Multiscale Relation Topics",
                "sec_num": "5.1"
            },
            {
                "text": "Our approach identifies 5 different topic hierarchies under different settings (use args, noun, preposition and verb; arg 1 only; arg 2 only; noun only and verb only). The number of the topics at each level is shown in Table 1 . At the first level, each input relation is treated as a topic. At the second level, numbers of topics go down to reasonable numbers like 269. Finally at the top level, the number of topic is down to 1 (Theorem 2 also proves this). We show some topic examples under the first setting. The 3 topics at level 5 are shown in Table 2 . They represent the most popular DBpedia relation topics. Almost all 269 topics at level 5 look semantically meaningful. They nicely capture the related relations. Some examples are in Table 3 . ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 225,
                        "end": 226,
                        "text": "1",
                        "ref_id": "TABREF1"
                    },
                    {
                        "start": 556,
                        "end": 557,
                        "text": "2",
                        "ref_id": "TABREF2"
                    },
                    {
                        "start": 750,
                        "end": 751,
                        "text": "3",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Learning Multiscale Relation Topics",
                "sec_num": "5.1"
            },
            {
                "text": "In previous experiment, 20,000 relation instances were held and not used to construct the topic space. We compare our approach against the regular rule-based approach (Lin and Pantel, 2001) and two other kernel-based approaches (presented in Section 4.2) for relation detection task. The comparison results are summarized in Table 4 . The approach using relation topics (level 2) consistently outperforms the other three approaches in all three settings. When n = 5, it achieves the largest improvement over the other three. This indicates that using relation topics that integrate the knowledge extracted from the existing relations, can significantly benefit us when the training data is insufficient. This is reasonable, since the prior knowledge becomes more valuable in this scenario.",
                "cite_spans": [
                    {
                        "start": 167,
                        "end": 189,
                        "text": "(Lin and Pantel, 2001)",
                        "ref_id": "BIBREF17"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 331,
                        "end": 332,
                        "text": "4",
                        "ref_id": "TABREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Relation Detection on Wikipedia Data",
                "sec_num": "5.2"
            },
            {
                "text": "The users can select the level that is the most appropriate for their applications. In this example, we only have alignment results at 7 levels. Choosing the space at level 2 spanned by a couple of hundreds of basis functions is a natural choice, since the levels below and above this have too many or too few features, respectively. A user can also select the most appropriate level by checking if the related relation topics are meaningful for their applications.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Relation Detection on Wikipedia Data",
                "sec_num": "5.2"
            },
            {
                "text": "In this experiment, we use the news domain documents of the ACE 2004 corpus (ACE, 2004) to compare our approaches against the state-of-the-art approaches. This dataset includes 348 documents and around 4400 relation instances. 7 relation types, 7 entity types, numerous relation sub-types, entity sub-types, and mention types are defined on this set. The task is to classify the relation instances into one of the 7 relation types or \"NONE\", which means there is no relation. For comparison, we use the same setting as (Zhang et al., 2006) , by applying a 5-fold cross-validation. The scores reported here are the average of all 5 folds. This is also how the other approaches are evaluated. In this test, we treat entity types, entity sub-types and mention types equally as argument types. Table 5 summarizes the performance after applying the kernels presented in Section 4.2 incrementally, showing the improvement from each individual kernel. We also compare our approaches to the other state-of-the-art approaches including Convolution Tree kernel (Collins and Duffy, 2001) , Syntactic kernel (Zhao and Grishman, 2005) , Composite kernel (linear) (Zhang et al., 2006) and the best kernel in (Nguyen et al., 2009) . Our approach with relation topics at level 2 has the best performance, achieving a 73.24% F-measure. The impact of the relation topics is huge. They improve the F-measure from 61.15% to 73.24%. We also test our approach using the topics at level 3. The performance is slightly worse than using level 2, but still better than the others.",
                "cite_spans": [
                    {
                        "start": 519,
                        "end": 539,
                        "text": "(Zhang et al., 2006)",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 1051,
                        "end": 1076,
                        "text": "(Collins and Duffy, 2001)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 1096,
                        "end": 1121,
                        "text": "(Zhao and Grishman, 2005)",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 1150,
                        "end": 1170,
                        "text": "(Zhang et al., 2006)",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 1194,
                        "end": 1215,
                        "text": "(Nguyen et al., 2009)",
                        "ref_id": "BIBREF21"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 796,
                        "end": 797,
                        "text": "5",
                        "ref_id": "TABREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Relation Detection on ACE Data",
                "sec_num": "5.3"
            },
            {
                "text": "This paper studies how relation topics extracted from Wikipedia relation repository can help improve relation detection performance. We do not want to tune our approach to one particular relation detection task, like ACE 2004. In our experiments, no parameter tuning was taken and no domain specific heuristic rules were applied. We are aware of some methods that could stack on our approach to further improve the performance on ACE test. The Composite kernel result in Table 5 is based on a linear combination of the Argument kernel and Convolution Tree kernel. (Zhang et al., 2006) showed that by carefully choosing the weight of each component and using a polynomial expansion, they could achieve the best performance on this data: 72.1% Fmeasure. (Nguyen et al., 2009) further showed that the performance can be improved by taking syntactic and semantic structures into consideration. They used several types of syntactic information including constituent and dependency syntactic parse trees to improve the state of the art approaches to 71.5% on F-measure. Heuristic rules extracted from the target data can also help improve the performance. (Jiang and Zhai, 2007) reported that by taking several heuristic rules they can improve the F-measure of Composite Kernel to 70.4%. They also showed that using maximum entropy classifier rather than SVM achieved the best performance on this task: 72.9% F-measure. To the best of our knowledge, the most recent result was reported by (Zhou and Zhu, 2011) , who extended their previous work in (Zhou et al., 2007) . By using several heuristics to define an effective portion of constituent trees, and training the classifiers using ACE relation sub-types (rather than on types), they achieved an impressive 75.8% F-measure. However, as pointed out in (Nguyen et al., 2009) , such heuristics are tuned on the target relation extraction task and might not be appropriate to compare against the automatic learning approaches. Even though we have not done any domain specific parameter tuning or applied any heuristics, our approach still achieve significant improvements over all approaches mentioned above except one, which is based on heuristics extracted from the target domain. This also implies that by combining some of the above ideas with relation topics, the performance on ACE data may be further improved.",
                "cite_spans": [
                    {
                        "start": 564,
                        "end": 584,
                        "text": "(Zhang et al., 2006)",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 752,
                        "end": 773,
                        "text": "(Nguyen et al., 2009)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 1150,
                        "end": 1172,
                        "text": "(Jiang and Zhai, 2007)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 1483,
                        "end": 1503,
                        "text": "(Zhou and Zhu, 2011)",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 1542,
                        "end": 1561,
                        "text": "(Zhou et al., 2007)",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 1799,
                        "end": 1820,
                        "text": "(Nguyen et al., 2009)",
                        "ref_id": "BIBREF21"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 477,
                        "end": 478,
                        "text": "5",
                        "ref_id": "TABREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Relation Detection on ACE Data",
                "sec_num": "5.3"
            },
            {
                "text": "This paper proposes a novel approach to create detectors for new relations integrating the knowledge extracted from the existing relations. The contributions of this paper are three-fold. Firstly, we pro- vide an automatic way to collect training data for more than 7,000 relations from Wikipedia and DBpedia. Secondly, we present an unsupervised way to construct a set of relation topics at multiple scales. Different from the topics defined over words, relation topics are defined over the existing relations. Thirdly, we design a new kernel for relation detection by integrating the relation topics in the representation of the relation instances. By leveraging the knowledge extracted from the Wikipedia relation repository, our approach significantly improves the performance over the state-of-the-art approaches on ACE data. This paper makes use of all DBpedia relations to create relation topics. It is possible that using a subset of them (more related to the target relations) might improve the performance. We will explore this in future work.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions",
                "sec_num": "6"
            }
        ],
        "back_matter": [
            {
                "text": "We thank the reviewers for their helpful comments. This material is based upon work supported in part by the IBM DeepQA (Watson) project. We also gratefully acknowledge the support of Defense Ad-vanced Research Projects Agency (DARPA) Machine Reading Program under Air Force Research Laboratory (AFRL) prime contract no. FA8750-09-C-0172. Any opinions, findings, and conclusion or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the view of the DARPA, AFRL, or the US government.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "The automatic content extraction projects",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ace",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "ACE. 2004. The automatic content extraction projects, http://projects.ldc.upenn.edu/ace/.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "DBpedia: A nucleus for a web of open data",
                "authors": [
                    {
                        "first": "S\u00f6ren",
                        "middle": [],
                        "last": "Auer",
                        "suffix": ""
                    },
                    {
                        "first": "Christian",
                        "middle": [],
                        "last": "Bizer",
                        "suffix": ""
                    },
                    {
                        "first": "Georgi",
                        "middle": [],
                        "last": "Kobilarov",
                        "suffix": ""
                    },
                    {
                        "first": "Jens",
                        "middle": [],
                        "last": "Lehmann",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Cyganiak",
                        "suffix": ""
                    },
                    {
                        "first": "Zachary",
                        "middle": [],
                        "last": "Ives",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "11--15",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "S\u00f6ren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives. 2007. DBpedia: A nucleus for a web of open data. In Pro- ceedings of the 6th International Semantic Web Con- ference, Busan, Korea, pages 11-15. Springer.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Latent Dirichlet allocation",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Blei",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Ng",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Jordan",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Journal of Machine Learning Research",
                "volume": "3",
                "issue": "",
                "pages": "993--1022",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. Blei, A. Ng, and M. Jordan. 2003. Latent Dirich- let allocation. Journal of Machine Learning Research, 3:993-1022.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Hierarchical topic models and the nested Chinese restaurant process",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Blei",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Griffiths",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Jordan",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Tenenbaum",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of the Advances in Neural Information Processing Systems (NIPS)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. Blei, T. Griffiths, M. Jordan, and J. Tenenbaum. 2004. Hierarchical topic models and the nested Chinese restaurant process. In Proceedings of the Advances in Neural Information Processing Systems (NIPS).",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "A shortest path dependency kernel for relation extraction",
                "authors": [
                    {
                        "first": "Razvan",
                        "middle": [],
                        "last": "Bunescu",
                        "suffix": ""
                    },
                    {
                        "first": "Raymond",
                        "middle": [],
                        "last": "Mooney",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Razvan Bunescu and Raymond Mooney. 2005. A short- est path dependency kernel for relation extraction. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Lan- guage Processing.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Exploiting background knowledge for relation extraction",
                "authors": [
                    {
                        "first": "Yee",
                        "middle": [],
                        "last": "Seng",
                        "suffix": ""
                    },
                    {
                        "first": "Chan",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Roth",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the 23rd International Conference on Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "152--160",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yee Seng Chan and Dan Roth. 2010. Exploiting back- ground knowledge for relation extraction. In Proceed- ings of the 23rd International Conference on Compu- tational Linguistics, pages 152-160.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Convolution kernels for natural language",
                "authors": [
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Collins",
                        "suffix": ""
                    },
                    {
                        "first": "Nigel",
                        "middle": [],
                        "last": "Duffy",
                        "suffix": ""
                    }
                ],
                "year": 2001,
                "venue": "Proceedings of the Advances in Neural Information Processing Systems (NIPS)",
                "volume": "",
                "issue": "",
                "pages": "625--632",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Michael Collins and Nigel Duffy. 2001. Convolution kernels for natural language. In Proceedings of the Advances in Neural Information Processing Systems (NIPS), pages 625-632.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Dependency tree kernels for relation extraction",
                "authors": [
                    {
                        "first": "Aron",
                        "middle": [],
                        "last": "Culotta",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Sorensen",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL)",
                "volume": "",
                "issue": "",
                "pages": "423--429",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree kernels for relation extraction. In Proceedings of the 42nd Annual Meeting of the Association for Com- putational Linguistics (ACL), pages 423-429.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Indexing by latent semantic analysis",
                "authors": [
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Deerwester",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [
                            "T"
                        ],
                        "last": "Dumais",
                        "suffix": ""
                    },
                    {
                        "first": "G",
                        "middle": [
                            "W"
                        ],
                        "last": "Furnas",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [
                            "K"
                        ],
                        "last": "Landauer",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Harshman",
                        "suffix": ""
                    }
                ],
                "year": 1990,
                "venue": "Journal of the American Society for Information Science",
                "volume": "41",
                "issue": "6",
                "pages": "391--407",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Lan- dauer, and R. Harshman. 1990. Indexing by latent se- mantic analysis. Journal of the American Society for Information Science, 41(6):391-407.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Unsupervised named-entity extraction from the web: An experimental study",
                "authors": [
                    {
                        "first": "Oren",
                        "middle": [],
                        "last": "Etzioni",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Cafarella",
                        "suffix": ""
                    },
                    {
                        "first": "Doug",
                        "middle": [],
                        "last": "Downey",
                        "suffix": ""
                    },
                    {
                        "first": "Ana-Maria",
                        "middle": [],
                        "last": "Popescu",
                        "suffix": ""
                    },
                    {
                        "first": "Tal",
                        "middle": [],
                        "last": "Shaked",
                        "suffix": ""
                    },
                    {
                        "first": "Stephen",
                        "middle": [],
                        "last": "Soderland",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [
                            "S"
                        ],
                        "last": "Weld",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [],
                        "last": "Yates",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Artificial Intelligence",
                "volume": "165",
                "issue": "",
                "pages": "91--134",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Oren Etzioni, Michael Cafarella, Doug Downey, Ana- Maria Popescu, Tal Shaked, Stephen Soderland, Daniel S. Weld, and Alexander Yates. 2005. Unsuper- vised named-entity extraction from the web: An ex- perimental study. Artificial Intelligence, 165:91-134.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "WordNet: An Electronic Lexical Database",
                "authors": [
                    {
                        "first": "Christiane",
                        "middle": [],
                        "last": "Fellbaum",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Learning 5000 relational extractors",
                "authors": [
                    {
                        "first": "Raphael",
                        "middle": [],
                        "last": "Hoffmann",
                        "suffix": ""
                    },
                    {
                        "first": "Congle",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [
                            "S"
                        ],
                        "last": "Weld",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL)",
                "volume": "",
                "issue": "",
                "pages": "286--295",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Raphael Hoffmann, Congle Zhang, and Daniel S. Weld. 2010. Learning 5000 relational extractors. In Pro- ceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 286-295.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "A systematic exploration of the feature space for relation extraction",
                "authors": [
                    {
                        "first": "Jing",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    },
                    {
                        "first": "Chengxiang",
                        "middle": [],
                        "last": "Zhai",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "113--120",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jing Jiang and Chengxiang Zhai. 2007. A systematic ex- ploration of the feature space for relation extraction. In Proceedings of the Human Language Technology Con- ference of the North American Chapter of the Associ- ation for Computational Linguistics, pages 113-120.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Multi-task transfer learning for weakly-supervised relation extraction",
                "authors": [
                    {
                        "first": "Jing",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics (ACL) and the 4th International Joint Conference on Natural Language Processing (IJCNLP)",
                "volume": "",
                "issue": "",
                "pages": "1012--1020",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jing Jiang. 2009. Multi-task transfer learning for weakly-supervised relation extraction. In Proceedings of the Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics (ACL) and the 4th International Joint Conference on Natural Language Processing (IJCNLP), pages 1012-1020.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Making Large-Scale SVM Learning Practical",
                "authors": [
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Joachims",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "T. Joachims. 1999. Making Large-Scale SVM Learning Practical. MIT Press.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Combining lexical, syntactic, and semantic features with maximum entropy models for extracting relations",
                "authors": [
                    {
                        "first": "Nanda",
                        "middle": [],
                        "last": "Kambhatla",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of the ACL 2004 on Interactive poster and demonstration sessions",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Nanda Kambhatla. 2004. Combining lexical, syntactic, and semantic features with maximum entropy mod- els for extracting relations. In Proceedings of the ACL 2004 on Interactive poster and demonstration sessions.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "DIRT -discovery of inference rules from text",
                "authors": [
                    {
                        "first": "Dekang",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    },
                    {
                        "first": "Patrick",
                        "middle": [],
                        "last": "Pantel",
                        "suffix": ""
                    }
                ],
                "year": 2001,
                "venue": "Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining",
                "volume": "",
                "issue": "",
                "pages": "323--328",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dekang Lin and Patrick Pantel. 2001. DIRT -discov- ery of inference rules from text. In Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 323-328.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Slot grammar: A system for simpler construction of practical natural language grammars",
                "authors": [
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Mccord",
                        "suffix": ""
                    }
                ],
                "year": 1995,
                "venue": "Communications of the ACM",
                "volume": "38",
                "issue": "11",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Michael McCord. 1995. Slot grammar: A system for simpler construction of practical natural language grammars. Communications of the ACM, 38(11).",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "A novel use of statistical parsing to extract information from text",
                "authors": [
                    {
                        "first": "Scott",
                        "middle": [],
                        "last": "Miller",
                        "suffix": ""
                    },
                    {
                        "first": "Heidi",
                        "middle": [],
                        "last": "Fox",
                        "suffix": ""
                    },
                    {
                        "first": "Lance",
                        "middle": [],
                        "last": "Ramshaw",
                        "suffix": ""
                    },
                    {
                        "first": "Ralph",
                        "middle": [],
                        "last": "Weischedel",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "Proceedings of the 1st North American Chapter",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Scott Miller, Heidi Fox, Lance Ramshaw, and Ralph Weischedel. 2000. A novel use of statistical pars- ing to extract information from text. In Proceedings of the 1st North American Chapter of the Association for Computational Linguistics Conference.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Distant supervision for relation extraction without labeled data",
                "authors": [
                    {
                        "first": "Mike",
                        "middle": [],
                        "last": "Mintz",
                        "suffix": ""
                    },
                    {
                        "first": "Steven",
                        "middle": [],
                        "last": "Bills",
                        "suffix": ""
                    },
                    {
                        "first": "Rion",
                        "middle": [],
                        "last": "Snow",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Jurafsky",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics (ACL) and the 4th International Joint Conference on Natural Language Processing (IJCNLP)",
                "volume": "",
                "issue": "",
                "pages": "1003--1011",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction with- out labeled data. In Proceedings of the Joint Confer- ence of the 47th Annual Meeting of the Association for Computational Linguistics (ACL) and the 4th Interna- tional Joint Conference on Natural Language Process- ing (IJCNLP), pages 1003-1011.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Convolution kernels on constituent, dependency and sequential structures for relation extraction",
                "authors": [
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Truc-Vien",
                        "suffix": ""
                    },
                    {
                        "first": "Alessandro",
                        "middle": [],
                        "last": "Nguyen",
                        "suffix": ""
                    },
                    {
                        "first": "Giuseppe",
                        "middle": [],
                        "last": "Moschitti",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Riccardi",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Truc-Vien T. Nguyen, Alessandro Moschitti, and Giuseppe Riccardi. 2009. Convolution kernels on constituent, dependency and sequential structures for relation extraction. In Proceedings of the 2009 Con- ference on Empirical Methods in Natural Language Processing (EMNLP).",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond",
                "authors": [
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Sch\u00f6lkopf",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [
                            "J"
                        ],
                        "last": "Smola",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "B. Sch\u00f6lkopf and A. J. Smola. 2002. Learning with Ker- nels: Support Vector Machines, Regularization, Opti- mization, and Beyond. MIT Press.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "YAGO: A large ontology from Wikipedia and WordNet",
                "authors": [
                    {
                        "first": "Fabian",
                        "middle": [
                            "M"
                        ],
                        "last": "Suchanek",
                        "suffix": ""
                    },
                    {
                        "first": "Gjergji",
                        "middle": [],
                        "last": "Kasneci",
                        "suffix": ""
                    },
                    {
                        "first": "Gerhard",
                        "middle": [],
                        "last": "Weikum",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "",
                "volume": "6",
                "issue": "",
                "pages": "203--217",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. YAGO: A large ontology from Wikipedia and WordNet. Web Semantics: Science, Services and Agents on the World Wide Web, 6(3):203- 217.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Multiscale analysis of document corpora based on diffusion models",
                "authors": [
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Mahadevan",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)",
                "volume": "",
                "issue": "",
                "pages": "1592--1597",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "C. Wang and S. Mahadevan. 2009. Multiscale analysis of document corpora based on diffusion models. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), pages 1592-1597.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "A composite kernel to extract relations between entities with both flat and structured features",
                "authors": [
                    {
                        "first": "Min",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Jie",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Jian",
                        "middle": [],
                        "last": "Su",
                        "suffix": ""
                    },
                    {
                        "first": "Guodong",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (ACL)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou. 2006. A composite kernel to extract relations between entities with both flat and structured features. In Pro- ceedings of the 21st International Conference on Com- putational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (ACL).",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Extracting relations with integrated information using kernel methods",
                "authors": [
                    {
                        "first": "Shubin",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "Ralph",
                        "middle": [],
                        "last": "Grishman",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL)",
                "volume": "",
                "issue": "",
                "pages": "419--426",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Shubin Zhao and Ralph Grishman. 2005. Extracting re- lations with integrated information using kernel meth- ods. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL), pages 419-426.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Kernel-based semantic relation detection and classification via enriched parse tree structure",
                "authors": [
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Q",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Journal of Computer Science and Technology",
                "volume": "26",
                "issue": "",
                "pages": "45--56",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "G. Zhou and Q. Zhu. 2011. Kernel-based semantic rela- tion detection and classification via enriched parse tree structure. Journal of Computer Science and Technol- ogy, 26:45-56.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Tree kernel-based relation extraction with context-sensitive structured parse tree information",
                "authors": [
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Ji",
                        "suffix": ""
                    },
                    {
                        "first": "Q",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "G. Zhou, M. Zhang, D. Ji, and Q. Zhu. 2007. Tree kernel-based relation extraction with context-sensitive structured parse tree information. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP).",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 1: A dependency tree example.",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure2: Diffusion Wavelets construct multiscale representations of the input matrix at different scales. QR is a modified QR decomposition. J is the max step number (this is optional, since the algorithm automatically terminates when it reaches a matrix of size 1 \u00d7 1). The notation [T ] \u03c6 b \u03c6a denotes matrix T whose column space is represented using basis \u03c6 b at scale b, and row space is represented using basis \u03c6 a at scale a. The notation [\u03c6 b ] \u03c6a denotes basis \u03c6 b represented on the basis \u03c6 a . At an arbitrary scale j, we have p j basis functions, and length of each function is l j . The number of p j is determined by the intrinsic structure of the given dataset in QR routine.[T ] \u03c6 b \u03c6a is a p b \u00d7 l a matrix, and [\u03c6 b ] \u03c6a is an l a \u00d7 p b matrix.",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Figure 3: Learning Relation Topics at Multiple Scales.",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "FIGREF3": {
                "num": null,
                "text": "Construct a relation repository from Wikipedia. (a) Collect training data from Wikipedia and DBpedia (Section 2.1); (b) Clean the data representing each input relation (Section 2.2 and 2.3); (c) Create relation correlation matrix S following the approach described in Section 3.2, resulting in an M \u00d7 M matrix. 2. Create multiscale relation topics. [\u03c6 k ] \u03c60 = DWT (S/\u03bb max (S), \u03b5), where DWT () is the diffusion wavelets implementation described in Section 3.1. [\u03c6 k ] \u03c60 are the scaling function bases at level k represented as an M \u00d7 p k matrix, k = 1, \u2022 \u2022 \u2022 , h represents the level in the topic hierarchy. The value of p k is determined in DWT () based on the intrinsic structure of the given dataset. Columns of [\u03c6 k ] \u03c60 are used as relation topics at level k.",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF4": {
                "num": null,
                "text": "These instances are randomly selected from 100 relations (200 instances from each relation). This set is used as a benchmark to compare different relation detection approaches. In this experiment, 100 instances from each relation are used for training, and the other 100 are for testing. In training, we try three different settings: n = 5, 20 and 100, where n is the size of the training set for each relation. When we train a model for one relation, we use the training positive instances from the other 99 relations as training negatives. For example, we use 5 training positive instances and 5*99=495 training negatives to train a detector for each relation.",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table/>",
                "type_str": "table",
                "text": "we design detectors for new relations, we treat arg 1 , arg 2 , noun, and verb separately to get stronger correlations between relations. We do not directly use preposition.Any DBpedia relation r \u2208 {1, \u2022 \u2022 \u2022 , M } is represented with 4 vectors r t = [r t (1), \u2022 \u2022 \u2022 , r t (N t )],where t \u2208 {arg 1 , arg 2 , noun, verb}, N t represents the size of the vocabulary set of the type t component in the Wikipedia training data, and r t (j) represents the occurrence count of type t component in relation r. For example, N verb is the size of the verb vocabulary set in the training data and r verb (j) represents the occurrence count of the j th verb in relation r. When a new relation instance x is given, we extract the dependency path between two arguments, and create four vectors x",
                "html": null,
                "num": null
            },
            "TABREF1": {
                "content": "<table><tr><td colspan=\"6\">Level args &amp; words arg 1 1 7628 7628 7628 7628 7628 arg 2 noun verb</td></tr><tr><td>2</td><td>269</td><td>119</td><td>155</td><td>249</td><td>210</td></tr><tr><td>3</td><td>32</td><td>17</td><td>19</td><td>25</td><td>35</td></tr><tr><td>4</td><td>7</td><td>5</td><td>5</td><td>7</td><td>10</td></tr><tr><td>5</td><td>3</td><td>2</td><td>3</td><td>4</td><td>4</td></tr><tr><td>6</td><td>2</td><td>1</td><td>2</td><td>2</td><td>2</td></tr><tr><td>7</td><td>1</td><td/><td>1</td><td>1</td><td>1</td></tr><tr><td colspan=\"6\">7,628 toralstudents (0.113201), candidate (0.014662), academ-</td></tr><tr><td colspan=\"6\">icadvisors (0.008623), notablestudents (0.003829), col-</td></tr><tr><td colspan=\"6\">lege (0.003021), operatingsystem (0.002964), combatant</td></tr><tr><td colspan=\"6\">(0.002826), influences (0.002285), training (0.002148),</td></tr></table>",
                "type_str": "table",
                "text": "Number of topics at different levels (DBpedia Relations) under 5 different settings: use args, noun, preposition and verb; arg 1 only; arg 2 only; noun only and verb only. DBpedia relations. For any given topic vector v, we know it is a column vector of length M , where M is the size of the DBpedia relation set and v = 1. The entry v[i] represents the contribution of relation i to this topic. To explain the main concept of topic v, we sort the entries on v and print out the relations corresponding to the top entries. These relations summarize the topics in the relation repository. One topic example is as follows: [doctoraladvisor (0.683366), doc-",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table><tr><td>Top</td></tr></table>",
                "type_str": "table",
                "text": "3 topics at level 5 (all word types and args). 4 Relations and Their Contributions starring 86.6%, writer 3.8%, producer 3.2%, director 1.6% birthplace 75.3%, clubs 6.1%, deathplace 5.1%, location 4.1% clubs 55.3%, teams 9.3%, nationalteam 6.3% college 6.0%",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td>Top Relations</td></tr><tr><td>activeyearsenddate, careerend, finalyear, retired</td></tr><tr><td>commands, partof, battles, notablecommanders</td></tr><tr><td>occupation, shortdescription, profession, dates</td></tr><tr><td>influenced, schooltradition, notableideas, maininterests</td></tr><tr><td>destinations, end, through, posttown</td></tr><tr><td>prizes, award, academyawards, highlights</td></tr><tr><td>inflow, outflow, length, maxdepth</td></tr><tr><td>after, successor, endingterminus</td></tr><tr><td>college, almamater, education</td></tr></table>",
                "type_str": "table",
                "text": "Some topics at level 2 (all word types and args).",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table><tr><td>Approaches</td><td>100</td><td>20</td><td>5</td></tr><tr><td>Rule Based</td><td colspan=\"3\">37.70% 27.45% 13.20%</td></tr><tr><td>AG+ DP</td><td colspan=\"3\">73.64% 51.85% 22.95%</td></tr><tr><td>AG+ DP+ BOW</td><td colspan=\"3\">78.74% 62.76% 31.98%</td></tr><tr><td colspan=\"4\">AG+ DP+ BOW+ TF 2 81.18% 68.03% 41.60%</td></tr></table>",
                "type_str": "table",
                "text": "F-measure comparison of different approaches over 100 DBpedia relations with 5, 20 and 100 positive examples per relation. AG: K Argument , DP: K P ath , BOW: K BOW , TF k : K T F k .",
                "html": null,
                "num": null
            },
            "TABREF5": {
                "content": "<table><tr><td>Approaches</td><td colspan=\"3\">P(%) R(%) F(%)</td></tr><tr><td>Convolution Tree Kernel</td><td>72.5</td><td>56.7</td><td>63.6</td></tr><tr><td colspan=\"4\">Composite Kernel (linear) 73.50 67.00 70.10</td></tr><tr><td>Syntactic Kernel</td><td colspan=\"3\">69.23 70.50 69.86</td></tr><tr><td>Nguyen, et al. (2009)</td><td colspan=\"3\">76.60 67.00 71.50</td></tr><tr><td>AG</td><td colspan=\"3\">59.56 46.22 52.02</td></tr><tr><td>AG + DP</td><td colspan=\"3\">64.44 54.93 59.28</td></tr><tr><td>AG + DP + BOW</td><td colspan=\"3\">62.00 61.19 61.15</td></tr><tr><td>AG + DP + BOW + TF 3 AG + DP + BOW + TF 2</td><td colspan=\"3\">69.63 76.51 72.90 69.15 77.88 73.24</td></tr></table>",
                "type_str": "table",
                "text": "Performance comparison of different approaches with SVM over the ACE 2004 data. P: Precision, R: Recall, F: F-measure, AG: K Argument , DP: K P ath , BOW: K BOW , TF k : K T F k .",
                "html": null,
                "num": null
            }
        }
    }
}