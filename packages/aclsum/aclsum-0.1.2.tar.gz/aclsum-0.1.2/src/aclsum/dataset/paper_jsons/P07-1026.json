{
    "paper_id": "P07-1026",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T11:56:02.233987Z"
    },
    "title": "A Grammar-driven Convolution Tree Kernel for Semantic Role Classification",
    "authors": [
        {
            "first": "Min",
            "middle": [],
            "last": "Zhang",
            "suffix": "",
            "affiliation": {},
            "email": "mzhang@i2r.a-star.edu.sg"
        },
        {
            "first": "Wanxiang",
            "middle": [],
            "last": "Che",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Harbin Institute of Technology",
                "location": {}
            },
            "email": ""
        },
        {
            "first": "Ai",
            "middle": [
                "Ti"
            ],
            "last": "Aw",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Chew",
            "middle": [
                "Lim"
            ],
            "last": "Tan",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "National University of Singapore",
                "location": {}
            },
            "email": ""
        },
        {
            "first": "Guodong",
            "middle": [],
            "last": "Zhou",
            "suffix": "",
            "affiliation": {},
            "email": "gdzhou@suda.edu.cn"
        },
        {
            "first": "Ting",
            "middle": [],
            "last": "Liu",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Harbin Institute of Technology",
                "location": {}
            },
            "email": "tliu@ir.hit.edu.cn"
        },
        {
            "first": "Sheng",
            "middle": [],
            "last": "Li",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Harbin Institute of Technology",
                "location": {}
            },
            "email": "lisheng@hit.edu.cn"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Convolution tree kernel has shown promising results in semantic role classification. However, it only carries out hard matching, which may lead to over-fitting and less accurate similarity measure. To remove the constraint, this paper proposes a grammardriven convolution tree kernel for semantic role classification by introducing more linguistic knowledge into the standard tree kernel. The proposed grammar-driven tree kernel displays two advantages over the previous one: 1) grammar-driven approximate substructure matching and 2) grammardriven approximate tree node matching. The two improvements enable the grammardriven tree kernel explore more linguistically motivated structure features than the previous one. Experiments on the CoNLL-2005 SRL shared task show that the grammardriven tree kernel significantly outperforms the previous non-grammar-driven one in SRL. Moreover, we present a composite kernel to integrate feature-based and tree kernel-based methods. Experimental results show that the composite kernel outperforms the previously best-reported methods.",
    "pdf_parse": {
        "paper_id": "P07-1026",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Convolution tree kernel has shown promising results in semantic role classification. However, it only carries out hard matching, which may lead to over-fitting and less accurate similarity measure. To remove the constraint, this paper proposes a grammardriven convolution tree kernel for semantic role classification by introducing more linguistic knowledge into the standard tree kernel. The proposed grammar-driven tree kernel displays two advantages over the previous one: 1) grammar-driven approximate substructure matching and 2) grammardriven approximate tree node matching. The two improvements enable the grammardriven tree kernel explore more linguistically motivated structure features than the previous one. Experiments on the CoNLL-2005 SRL shared task show that the grammardriven tree kernel significantly outperforms the previous non-grammar-driven one in SRL. Moreover, we present a composite kernel to integrate feature-based and tree kernel-based methods. Experimental results show that the composite kernel outperforms the previously best-reported methods.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Given a sentence, the task of Semantic Role Labeling (SRL) consists of analyzing the logical forms expressed by some target verbs or nouns and some constituents of the sentence. In particular, for each predicate (target verb or noun) all the constituents in the sentence which fill semantic arguments (roles) of the predicate have to be recognized. Typical semantic roles include Agent, Patient, Instrument, etc. and also adjuncts such as Locative, Temporal, Manner, and Cause, etc. Generally, semantic role identification and classification are regarded as two key steps in semantic role labeling. Semantic role identification involves classifying each syntactic element in a sentence into either a semantic argument or a non-argument while semantic role classification involves classifying each semantic argument identified into a specific semantic role. This paper focuses on semantic role classification task with the assumption that the semantic arguments have been identified correctly.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Both feature-based and kernel-based learning methods have been studied for semantic role classification (Carreras and M\u00e0rquez, 2004; Carreras and M\u00e0rquez, 2005) . In feature-based methods, a flat feature vector is used to represent a predicateargument structure while, in kernel-based methods, a kernel function is used to measure directly the similarity between two predicate-argument structures. As we know, kernel methods are more effective in capturing structured features. Moschitti (2004) and Che et al. (2006) used a convolution tree kernel (Collins and Duffy, 2001) for semantic role classification. The convolution tree kernel takes sub-tree as its feature and counts the number of common sub-trees as the similarity between two predicate-arguments. This kernel has shown very promising results in SRL. However, as a general learning algorithm, the tree kernel only carries out hard matching between any two sub-trees without considering any linguistic knowledge in kernel design. This makes the kernel fail to handle similar phrase structures (e.g., \"buy a car\" vs. \"buy a red car\") and near-synonymic grammar tags (e.g., the POS variations between \"high/JJ degree/NN\" 1 and \"higher/JJR degree/NN\") 2 . To some degree, it may lead to over-fitting and compromise performance.",
                "cite_spans": [
                    {
                        "start": 104,
                        "end": 132,
                        "text": "(Carreras and M\u00e0rquez, 2004;",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 133,
                        "end": 160,
                        "text": "Carreras and M\u00e0rquez, 2005)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 478,
                        "end": 494,
                        "text": "Moschitti (2004)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 499,
                        "end": 516,
                        "text": "Che et al. (2006)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 548,
                        "end": 573,
                        "text": "(Collins and Duffy, 2001)",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "This paper reports our preliminary study in addressing the above issue by introducing more linguistic knowledge into the convolution tree kernel. To our knowledge, this is the first attempt in this research direction. In detail, we propose a grammar-driven convolution tree kernel for semantic role classification that can carry out more linguistically motivated substructure matching. Experimental results show that the proposed method significantly outperforms the standard convolution tree kernel on the data set of the CoNLL-2005 SRL shared task.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The remainder of the paper is organized as follows: Section 2 reviews the previous work and Section 3 discusses our grammar-driven convolution tree kernel. Section 4 shows the experimental results. We conclude our work in Section 5.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Feature-based Methods for SRL: most features used in prior SRL research are generally extended from Gildea and Jurafsky (2002) , who used a linear interpolation method and extracted basic flat features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998) .",
                "cite_spans": [
                    {
                        "start": 100,
                        "end": 126,
                        "text": "Gildea and Jurafsky (2002)",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 278,
                        "end": 298,
                        "text": "(Baker et al., 1998)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Previous Work",
                "sec_num": "2"
            },
            {
                "text": "Here, the basic features include Phrase Type, Parse Tree Path, and Position. Most of the following work focused on feature engineering (Xue and Palmer, 2004; Jiang et al., 2005) and machine learning models (Nielsen and Pradhan, 2004; Pradhan et al., 2005a) . Some other work paid much attention to the robust SRL (Pradhan et al., 2005b) and post inference (Punyakanok et al., 2004) . These featurebased methods are considered as the state of the art methods for SRL. However, as we know, the standard flat features are less effective in modeling the 1 Please refer to http://www.cis.upenn.edu/~treebank/ for the detailed definitions of the grammar tags used in the paper. 2 Some rewrite rules in English grammar are generalizations of others: for example, \"NP DET JJ NN\" is a specialized version of \"NP DET NN\". The same applies to POS. The standard convolution tree kernel is unable to capture the two cases. syntactic structured information. For example, in SRL, the Parse Tree Path feature is sensitive to small changes of the syntactic structures. Thus, a predicate argument pair will have two different Path features even if their paths differ only for one node. This may result in data sparseness and model generalization problems. Kernel-based Methods for SRL: as an alternative, kernel methods are more effective in modeling structured objects. This is because a kernel can measure the similarity between two structured objects using the original representation of the objects instead of explicitly enumerating their features. Many kernels have been proposed and applied to the NLP study. In particular, Haussler (1999) proposed the well-known convolution kernels for a discrete structure. In the context of it, more and more kernels for restricted syntaxes or specific domains (Collins and Duffy, 2001; Lodhi et al., 2002; Zelenko et al., 2003; Zhang et al., 2006) are proposed and explored in the NLP domain.",
                "cite_spans": [
                    {
                        "start": 135,
                        "end": 157,
                        "text": "(Xue and Palmer, 2004;",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 158,
                        "end": 177,
                        "text": "Jiang et al., 2005)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 206,
                        "end": 233,
                        "text": "(Nielsen and Pradhan, 2004;",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 234,
                        "end": 256,
                        "text": "Pradhan et al., 2005a)",
                        "ref_id": null
                    },
                    {
                        "start": 313,
                        "end": 336,
                        "text": "(Pradhan et al., 2005b)",
                        "ref_id": null
                    },
                    {
                        "start": 356,
                        "end": 381,
                        "text": "(Punyakanok et al., 2004)",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 1612,
                        "end": 1627,
                        "text": "Haussler (1999)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 1786,
                        "end": 1811,
                        "text": "(Collins and Duffy, 2001;",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 1812,
                        "end": 1831,
                        "text": "Lodhi et al., 2002;",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 1832,
                        "end": 1853,
                        "text": "Zelenko et al., 2003;",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 1854,
                        "end": 1873,
                        "text": "Zhang et al., 2006)",
                        "ref_id": "BIBREF25"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Previous Work",
                "sec_num": "2"
            },
            {
                "text": "Of special interest here, Moschitti (2004) proposed Predicate Argument Feature (PAF) kernel for SRL under the framework of convolution tree kernel. He selected portions of syntactic parse trees as predicateargument feature spaces, which include salient substructures of predicate-arguments, to define convolution kernels for the task of semantic role classification. Under the same framework, Che et al. (2006) proposed a hybrid convolution tree kernel, which consists of two individual convolution kernels: a Path kernel and a Constituent Structure kernel. Che et al. (2006) showed that their method outperformed PAF on the CoNLL-2005 SRL dataset.",
                "cite_spans": [
                    {
                        "start": 26,
                        "end": 42,
                        "text": "Moschitti (2004)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 393,
                        "end": 410,
                        "text": "Che et al. (2006)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 558,
                        "end": 575,
                        "text": "Che et al. (2006)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Previous Work",
                "sec_num": "2"
            },
            {
                "text": "The above two kernels are special instances of convolution tree kernel for SRL. As discussed in Section 1, convolution tree kernel only carries out hard matching, so it fails to handle similar phrase structures and near-synonymic grammar tags. This paper presents a grammar-driven convolution tree kernel to solve the two problems 3 Grammar-driven Convolution Tree Kernel",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Previous Work",
                "sec_num": "2"
            },
            {
                "text": "In convolution tree kernel (Collins and Duffy, 2001) , a parse tree T is represented by a vector of integer counts of each sub-tree type (regardless of its ancestors): ( ) T \u03c6 = ( \u2026, # subtree i (T), \u2026), where # subtree i (T) is the occurrence number of the i th sub-tree type (subtree i ) in T. Since the number of different sub-trees is exponential with the parse tree size, it is computationally infeasible to directly use the feature vector ( ) T \u03c6 . To solve this computational issue, Collins and Duffy (2001) proposed the following parse tree kernel to calculate the dot product between the above high dimensional vectors implicitly.",
                "cite_spans": [
                    {
                        "start": 27,
                        "end": 52,
                        "text": "(Collins and Duffy, 2001)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 490,
                        "end": 514,
                        "text": "Collins and Duffy (2001)",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Convolution Tree Kernel",
                "sec_num": "3.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "1 1 2 2 1 1 2 2 1 2 1 2 1 2 1 2 ( , ) ( ), ( ) ( ) ( ) ( , )",
                        "eq_num": "(( ) ("
                    }
                ],
                "section": "Convolution Tree Kernel",
                "sec_num": "3.1"
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Convolution Tree Kernel",
                "sec_num": "3.1"
            },
            {
                "text": ") i i subtree subtree i n N n N n N n N K T T T T I n I n n n \u03c6 \u03c6 \u2208 \u2208 \u2208 \u2208 =< > = = \u2206 \u22c5 \u2211 \u2211 \u2211 \u2211 \u2211",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Convolution Tree Kernel",
                "sec_num": "3.1"
            },
            {
                "text": "where N 1 and N 2 are the sets of nodes in trees T 1 and T 2 , respectively, and",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Convolution Tree Kernel",
                "sec_num": "3.1"
            },
            {
                "text": "( ) i subtree I",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Convolution Tree Kernel",
                "sec_num": "3.1"
            },
            {
                "text": "n is a function that is 1 iff the subtree i occurs with root at node n and zero otherwise, and",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Convolution Tree Kernel",
                "sec_num": "3.1"
            },
            {
                "text": "1 2 ( , ) n n \u2206",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Convolution Tree Kernel",
                "sec_num": "3.1"
            },
            {
                "text": "is the number of the common subtrees rooted at n 1 and n 2 , i.e.,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Convolution Tree Kernel",
                "sec_num": "3.1"
            },
            {
                "text": "1 2 1 2 ( , ) ( ) ( ) i i subtree subtree i n n I n I n \u2206 = \u22c5 \u2211 1 2 ( , )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Convolution Tree Kernel",
                "sec_num": "3.1"
            },
            {
                "text": "n n \u2206 can be further computed efficiently by the following recursive rules:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Convolution Tree Kernel",
                "sec_num": "3.1"
            },
            {
                "text": "Rule 1: if the productions (CFG rules) at 1 n and 2 n are different, 1 2 ( , ) 0 n n \u2206 = ;",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Convolution Tree Kernel",
                "sec_num": "3.1"
            },
            {
                "text": "Rule 2: else if both 1 n and 2 n are pre-terminals",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Convolution Tree Kernel",
                "sec_num": "3.1"
            },
            {
                "text": "(POS tags), 1 2 ( , ) 1 n n \u03bb \u2206 = \u00d7 ; Rule 3: else, 1 ( ) 1 2 1 2 1 ( , )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Convolution Tree Kernel",
                "sec_num": "3.1"
            },
            {
                "text": "(1 ( ( , ), ( , )))",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Convolution Tree Kernel",
                "sec_num": "3.1"
            },
            {
                "text": "nc n j n n ch n j ch n j \u03bb = \u2206 = +\u2206 \u220f , where 1 ( ) nc n is the child number of 1 n , ch(n,j) is",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Convolution Tree Kernel",
                "sec_num": "3.1"
            },
            {
                "text": "the j th child of node n and \u03bb (0< \u03bb <1) is the decay factor in order to make the kernel value less variable with respect to the subtree sizes. In addition, the recursive Rule 3 holds because given two nodes with the same children, one can construct common sub-trees using these children and common sub-trees of further offspring. The time complexity for computing this kernel is",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Convolution Tree Kernel",
                "sec_num": "3.1"
            },
            {
                "text": "1 2 (| | | |) O N N \u22c5 .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Convolution Tree Kernel",
                "sec_num": "3.1"
            },
            {
                "text": "This Subsection introduces the two improvements and defines our grammar-driven tree kernel. Improvement 1: Grammar-driven approximate matching between substructures. The conven-tional tree kernel requires exact matching between two contiguous phrase structures. This constraint may be too strict. For example, the two phrase structures \"NP DT JJ NN\" (NP a red car) and \"NP DT NN\" (NP->a car) are not identical, thus they contribute nothing to the conventional kernel although they should share the same semantic role given a predicate. In this paper, we propose a grammar-driven approximate matching mechanism to capture the similarity between such kinds of quasi-structures for SRL. First, we construct reduced rule set by defining optional nodes, for example, \"NP->DT [JJ] NP\" or \"VP-> VB [ADVP] PP\", where [*] denotes optional nodes. For convenience, we call \"NP-> DT JJ NP\" the original rule and \"NP->DT [JJ] NP\" the reduced rule. Here, we define two grammar-driven criteria to select optional nodes:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Grammar-driven Convolution Tree Kernel",
                "sec_num": "3.2"
            },
            {
                "text": "1) The reduced rules must be grammatical. It means that the reduced rule should be a valid rule in the original rule set. For example, \"NP->DT [JJ] NP\" is valid only when \"NP->DT NP\" is a valid rule in the original rule set while \"NP->DT [JJ NP]\" may not be valid since \"NP->DT\" is not a valid rule in the original rule set.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Grammar-driven Convolution Tree Kernel",
                "sec_num": "3.2"
            },
            {
                "text": "2) A valid reduced rule must keep the head child of its corresponding original rule and has at least two children. This can make the reduced rules retain the underlying semantic meaning of their corresponding original rules.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Grammar-driven Convolution Tree Kernel",
                "sec_num": "3.2"
            },
            {
                "text": "Given the reduced rule set, we can then formulate the approximate substructure matching mechanism as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Grammar-driven Convolution Tree Kernel",
                "sec_num": "3.2"
            },
            {
                "text": "1 1 2 1 2 , ( , ) ( ( , ) ) a b i j i j T r r i j M r r I T T \u03bb + = \u00d7 \u2211 (1)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Grammar-driven Convolution Tree Kernel",
                "sec_num": "3.2"
            },
            {
                "text": "where 1 r is a production rule, representing a sub-tree of depth one3 , and 1 i r T is the i th variation of the sub- tree 1 r by removing one ore more optional nodes4 , and likewise for 2",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Grammar-driven Convolution Tree Kernel",
                "sec_num": "3.2"
            },
            {
                "text": "r and 2 j r T . ( , ) T I \u2022 \u2022 is a function",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Grammar-driven Convolution Tree Kernel",
                "sec_num": "3.2"
            },
            {
                "text": "that is 1 iff the two sub-trees are identical and zero otherwise. 1",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Grammar-driven Convolution Tree Kernel",
                "sec_num": "3.2"
            },
            {
                "text": "\u03bb (0\u2264 1 \u03bb \u22641",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Grammar-driven Convolution Tree Kernel",
                "sec_num": "3.2"
            },
            {
                "text": ") is a small penalty to penal- ize optional nodes and the two parameters i a and j b stand for the numbers of occurrence of removed optional nodes in subtrees 1 i r T and 2 j r T , respectively. 1 2 ( , ) M r r returns the similarity (ie., the kernel value) between the two sub-trees 1 r and 2 r by sum- ming up the similarities between all possible variations of the sub-trees 1 r and 2 r .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Grammar-driven Convolution Tree Kernel",
                "sec_num": "3.2"
            },
            {
                "text": "Under the new approximate matching mechanism, two structures are matchable (but with a small penalty 1 \u03bb ) if the two structures are identical after removing one or more optional nodes. In this case, the above example phrase structures \"NP->a red car\" and \"NP->a car\" are matchable with a penalty 1 \u03bb in our new kernel. It means that one co- occurrence of the two structures contributes 1 \u03bb to our proposed kernel while it contributes zero to the traditional one. Therefore, by this improvement, our method would be able to explore more linguistically appropriate features than the previous one (which is formulated as",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Grammar-driven Convolution Tree Kernel",
                "sec_num": "3.2"
            },
            {
                "text": "1 2 ( , ) T I r r ).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Grammar-driven Convolution Tree Kernel",
                "sec_num": "3.2"
            },
            {
                "text": "Improvement 2: Grammar-driven tree nodes approximate matching. The conventional tree kernel needs an exact matching between two (terminal/non-terminal) nodes. But, some similar POSs may represent similar roles, such as NN (dog) and NNS (dogs). In order to capture this phenomenon, we allow approximate matching between node features. The following illustrates some equivalent node feature sets:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Grammar-driven Convolution Tree Kernel",
                "sec_num": "3.2"
            },
            {
                "text": "\u2022 JJ, JJR, JJS \u2022 VB, VBD, VBG, VBN, VBP, VBZ \u2022 \u2026\u2026 where POSs in the same line can match each other with a small penalty 0\u2264 2 \u03bb \u22641. We call this case node feature mutation. This improvement further generalizes the conventional tree kernel to get better coverage. The approximate node matching can be formulated as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Grammar-driven Convolution Tree Kernel",
                "sec_num": "3.2"
            },
            {
                "text": "2 1 2 1 2 , ( , ) ( ( , ) ) a b i j i j f i j M f f I f f \u03bb + = \u00d7 \u2211 (2)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Grammar-driven Convolution Tree Kernel",
                "sec_num": "3.2"
            },
            {
                "text": "where 1",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Grammar-driven Convolution Tree Kernel",
                "sec_num": "3.2"
            },
            {
                "text": "f is a node feature, 1 i f is the i th mutation of 1",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Grammar-driven Convolution Tree Kernel",
                "sec_num": "3.2"
            },
            {
                "text": "f and i a is 0 iff 1 i f and 1 f are identical and 1 oth- erwise, and likewise for 2 f . ( , )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Grammar-driven Convolution Tree Kernel",
                "sec_num": "3.2"
            },
            {
                "text": "f I \u2022 \u2022 is a function",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Grammar-driven Convolution Tree Kernel",
                "sec_num": "3.2"
            },
            {
                "text": "that is 1 iff the two features are identical and zero otherwise. Eq. ( 2) sums over all combinations of feature mutations as the node feature similarity.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Grammar-driven Convolution Tree Kernel",
                "sec_num": "3.2"
            },
            {
                "text": "The same as Eq. ( 1), the reason for taking all the possibilities into account in Eq. ( 2) is to make sure that the new kernel is a proper kernel.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Grammar-driven Convolution Tree Kernel",
                "sec_num": "3.2"
            },
            {
                "text": "The above two improvements are grammardriven, i.e., the two improvements retain the underlying linguistic grammar constraints and keep semantic meanings of original rules.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Grammar-driven Convolution Tree Kernel",
                "sec_num": "3.2"
            },
            {
                "text": "The Grammar-driven Kernel Definition: Given the two improvements discussed above, we can define the new kernel by beginning with the feature vector representation of a parse tree T as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Grammar-driven Convolution Tree Kernel",
                "sec_num": "3.2"
            },
            {
                "text": "( ) T \u03c6 = \u2032 (# subtree 1 (T), \u2026, # subtree n (T))",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Grammar-driven Convolution Tree Kernel",
                "sec_num": "3.2"
            },
            {
                "text": "where # subtree i (T) is the occurrence number of the i th sub-tree type (subtree i ) in T. Please note that, different from the previous tree kernel, here we loosen the condition for the occurrence of a subtree by allowing both original and reduced rules (Improvement 1) and node feature mutations (Improvement 2). In other words, we modify the criteria by which a subtree is said to occur. For example, one occurrence of the rule \"NP->DT JJ NP\" shall contribute 1 times to the feature \"NP->DT JJ NP\" and 1 \u03bb times to the feature \"NP->DT NP\" in the new kernel while it only contributes 1 times to the feature \"NP->DT JJ NP\" in the previous one. Now we can define the new grammar-driven kernel 1 2 ( , ) G K T T as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Grammar-driven Convolution Tree Kernel",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "1 1 2 2 1 1 2 2 1 2 1 2 1 2 1 2 ( , )",
                        "eq_num": "( ), ( ) ( ) ( ) ( , ) (( ) ("
                    }
                ],
                "section": "Grammar-driven Convolution Tree Kernel",
                "sec_num": "3.2"
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Grammar-driven Convolution Tree Kernel",
                "sec_num": "3.2"
            },
            {
                "text": ") i i G subtree subtree i n N n N n N n N K T T T T I n I n n n \u03c6 \u03c6 \u2208 \u2208 \u2208 \u2208 \u2032 \u2032 =< > \u2032 \u2032 = \u2032 = \u2206 \u22c5 \u2211 \u2211 \u2211 \u2211 \u2211 (3)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Grammar-driven Convolution Tree Kernel",
                "sec_num": "3.2"
            },
            {
                "text": "where N 1 and N 2 are the sets of nodes in trees T 1 and T 2 , respectively. ( ) T respectively) by removing different optional nodes, then:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Grammar-driven Convolution Tree Kernel",
                "sec_num": "3.2"
            },
            {
                "text": "i subtree I n \u2032 is a function that is 1 2 a b \u03bb \u03bb \u2022 iff",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Grammar-driven Convolution Tree Kernel",
                "sec_num": "3.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "1 1 1 2 1 2 , ( , ) 1 2 1 ( , )",
                        "eq_num": "( ( , )"
                    }
                ],
                "section": "Grammar-driven Convolution Tree Kernel",
                "sec_num": "3.2"
            },
            {
                "text": "(1 ( ( , , ), ( , , )))",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Grammar-driven Convolution Tree Kernel",
                "sec_num": "3.2"
            },
            {
                "text": "a b i j i j T n n i j nc n i k n n I T T ch n i k ch n j k \u03bb \u03bb + = \u2032 \u2206 = \u00d7 \u00d7 \u2032 \u00d7 +\u2206 \u2211 \u220f (6)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Grammar-driven Convolution Tree Kernel",
                "sec_num": "3.2"
            },
            {
                "text": "where",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Grammar-driven Convolution Tree Kernel",
                "sec_num": "3.2"
            },
            {
                "text": "\u2022 1 i n",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Grammar-driven Convolution Tree Kernel",
                "sec_num": "3.2"
            },
            {
                "text": "T and 2 j n T stand for the i th and j th variations in sub-tree set 1 n T and 2 n T , respectively.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Grammar-driven Convolution Tree Kernel",
                "sec_num": "3.2"
            },
            {
                "text": "T I \u2022 \u2022 is a function that is 1 iff the two sub- trees are identical and zero otherwise.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\u2022 ( , )",
                "sec_num": null
            },
            {
                "text": "\u2022 i a and j b stand for the number of removed op- tional nodes in subtrees 1 i n T and 2 j n T , respectively.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\u2022 ( , )",
                "sec_num": null
            },
            {
                "text": "\u2022 1 ( , ) nc n i returns the child number of 1 n in its i th subtree variation 1 i n T .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\u2022 ( , )",
                "sec_num": null
            },
            {
                "text": "\u2022 1 ( , , ) ch n i k is the k th child of node 1 n in its i th variation subtree 1 i n T , and likewise for 2 ( , , ) ch n j k .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\u2022 ( , )",
                "sec_num": null
            },
            {
                "text": "\u2022 Finally, the same as the previous tree kernel, \u03bb (0< \u03bb <1) is the decay factor (see the discussion in Subsection 3.1).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "\u2022 ( , )",
                "sec_num": null
            },
            {
                "text": "1 2 ( , ) 0 n n \u2032 \u2206 = ============================================================================",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Rule C: else",
                "sec_num": null
            },
            {
                "text": "Rule A accounts for Improvement 2 while Rule B accounts for Improvement 1. In Rule B, Eq. ( 6) is able to carry out multi-layer sub-tree approximate matching due to the introduction of the recursive part while Eq. ( 1) is only effective for subtrees of depth one. Moreover, we note that Eq. ( 4) is a convolution kernel according to the definition and the proof given in Haussler (1999) , and Eqs ( 5) and (6) reformulate Eq. ( 4) so that it can be computed efficiently, in this way, our kernel defined by Eq (3) is also a valid convolution kernel. Finally, let us study the computational issue of the new convolution tree kernel. Clearly, computing Eq. ( 6) requires exponential time in its worst case. However, in practice, it may only need",
                "cite_spans": [
                    {
                        "start": 371,
                        "end": 386,
                        "text": "Haussler (1999)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Rule C: else",
                "sec_num": null
            },
            {
                "text": "1 2 (| | | |) O N N",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Rule C: else",
                "sec_num": null
            },
            {
                "text": "\u22c5 . This is because there are only 9.9% rules (647 out of the total 6,534 rules in the parse trees) have optional nodes and most of them have only one optional node. In fact, the actual running time is even much less and is close to linear in the size of the trees since 1 2 ( , ) 0 n n \u2032 \u2206 = holds for many node pairs (Collins and Duffy, 2001) . In theory, we can also design an efficient algorithm to compute Eq. ( 6) using a dynamic programming algorithm (Moschitti, 2006) . We just leave it for our future work.",
                "cite_spans": [
                    {
                        "start": 319,
                        "end": 344,
                        "text": "(Collins and Duffy, 2001)",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 458,
                        "end": 475,
                        "text": "(Moschitti, 2006)",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Rule C: else",
                "sec_num": null
            },
            {
                "text": "In above discussion, we show that the conventional convolution tree kernel is a special case of the grammar-driven tree kernel. From kernel function viewpoint, our kernel can carry out not only exact matching (as previous one described by Rules 2 and 3 in Subsection 3.1) but also approximate matching (Eqs. ( 5) and ( 6) in Subsection 3.2). From feature exploration viewpoint, although they explore the same sub-structure feature space (defined recursively by the phrase parse rules), their feature values are different since our kernel captures the structure features in a more linguistically appropriate way by considering more linguistic knowledge in our kernel design. Moschitti (2006) proposes a partial tree (PT) kernel which can carry out partial matching between sub-trees. The PT kernel generates a much larger feature space than both the conventional and the grammar-driven kernels. In this point, one can say that the grammar-driven tree kernel is a specialization of the PT kernel. However, the important difference between them is that the PT kernel is not grammar-driven, thus many nonlinguistically motivated structures are matched in the PT kernel. This may potentially compromise the performance since some of the over-generated features may possibly be noisy due to the lack of linguistic interpretation and constraint. Kashima and Koyanagi (2003) proposed a convolution kernel over labeled order trees by generalizing the standard convolution tree kernel. The labeled order tree kernel is much more flexible than the PT kernel and can explore much larger sub-tree features than the PT kernel. However, the same as the PT kernel, the labeled order tree kernel is not grammar-driven. Thus, it may face the same issues (such as over-generated features) as the PT kernel when used in NLP applications. Shen el al. (2003) proposed a lexicalized tree kernel to utilize LTAG-based features in parse reranking. Their methods need to obtain a LTAG derivation tree for each parse tree before kernel calculation. In contrast, we use the notion of optional arguments to define our grammar-driven tree kernel and use the empirical set of CFG rules to determine which arguments are optional.",
                "cite_spans": [
                    {
                        "start": 674,
                        "end": 690,
                        "text": "Moschitti (2006)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 1339,
                        "end": 1366,
                        "text": "Kashima and Koyanagi (2003)",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 1818,
                        "end": 1836,
                        "text": "Shen el al. (2003)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Comparison with previous work",
                "sec_num": "3.3"
            },
            {
                "text": "Data: We use the CoNLL-2005 SRL shared task data (Carreras and M\u00e0rquez, 2005) as our experimental corpus. The data consists of sections of the Wall Street Journal part of the Penn TreeBank (Marcus et al., 1993) , with information on predicate-argument structures extracted from the Prop-Bank corpus (Palmer et al., 2005) . As defined by the shared task, we use sections 02-21 for training, section 24 for development and section 23 for test. There are 35 roles in the data including 7 Core (A0-A5, AA), 14 Adjunct (AM-) and 14 Reference (R-) arguments. We assume that the semantic role identification has been done correctly. In this way, we can focus on the classification task and evaluate it more accurately. We evaluate the performance with Accuracy. SVM (Vapnik, 1998) is selected as our classifier and the one vs. others strategy is adopted and the one with the largest margin is selected as the final answer. In our implementation, we use the binary SVMLight (Joachims, 1998) and modify the Tree Kernel Tools (Moschitti, 2004) to a grammardriven one. Kernel Setup: We use the Constituent, Predicate, and Predicate-Constituent related features, which are reported to get the best-reported performance (Pradhan et al., 2005a) , as the baseline features. We use Che et al. (2006) 's hybrid convolution tree ker-nel (the best-reported method for kernel-based SRL) as our baseline kernel. It is defined as",
                "cite_spans": [
                    {
                        "start": 49,
                        "end": 77,
                        "text": "(Carreras and M\u00e0rquez, 2005)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 189,
                        "end": 210,
                        "text": "(Marcus et al., 1993)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 299,
                        "end": 320,
                        "text": "(Palmer et al., 2005)",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 759,
                        "end": 773,
                        "text": "(Vapnik, 1998)",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 966,
                        "end": 982,
                        "text": "(Joachims, 1998)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 1016,
                        "end": 1033,
                        "text": "(Moschitti, 2004)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 1207,
                        "end": 1230,
                        "text": "(Pradhan et al., 2005a)",
                        "ref_id": null
                    },
                    {
                        "start": 1266,
                        "end": 1283,
                        "text": "Che et al. (2006)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Setting",
                "sec_num": "4.1"
            },
            {
                "text": "(1 ) (0 1) the grammar-driven approximate substructure matching is more effective than the grammar-driven approximate node matching. However, we find that the performance of the grammar-driven kernel is still a bit lower than the feature-based method. This is not surprising since tree kernel methods only focus on modeling tree structure information. In this paper, it captures the syntactic parse tree structure features only while the features used in the featurebased methods cover more knowledge sources.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Setting",
                "sec_num": "4.1"
            },
            {
                "text": "hybrid path cs K K K \u03b8 \u03b8 \u03b8 = + - \u2264 \u2264 (",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Setting",
                "sec_num": "4.1"
            },
            {
                "text": "In order to make full use of the syntactic structure information and the other useful diverse flat features, we present a composite kernel to combine the grammar-driven hybrid kernel and feature-based method with polynomial kernel:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Results",
                "sec_num": "4.2"
            },
            {
                "text": "(1 ) (0 1)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Results",
                "sec_num": "4.2"
            },
            {
                "text": "comp hybrid poly K K K \u03b3 \u03b3 \u03b3 = + - \u2264 \u2264",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Results",
                "sec_num": "4.2"
            },
            {
                "text": "Evaluation on the development set shows that the composite kernel yields the best performance when \u03b3 is set to 0.3. Using the same setting, the system achieves the performance of 91.02% in Accuracy in the same test set. It shows statistically significant improvement (\u03c7 2 test with p= 0.10) over using the standard features with the polynomial kernel (\u03b3 = 0, Accuracy = 89.92%) and using the grammar-driven hybrid convolution tree kernel (\u03b3 = 1, Accuracy = 87.96%). The main reason is that the tree kernel can capture effectively more structure features while the standard flat features can cover some other useful features, such as Voice, SubCat, which are hard to be covered by the tree kernel. The experimental results suggest that these two kinds of methods are complementary to each other.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experimental Results",
                "sec_num": "4.2"
            },
            {
                "text": "In order to further compare with other methods, we also do experiments on the dataset of English PropBank I (LDC2004T14). The training, develop-ment and test sets follow the conventional split of Sections 02-21, 00 and 23. Table 3 compares our method with other previously best-reported methods with the same setting as discussed previously. It shows that our method outperforms the previous best-reported one with a relative error rate reduction of 10.8% (0.97/(100-91)). This further verifies the effectiveness of the grammar-driven kernel method for semantic role classification.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 229,
                        "end": 230,
                        "text": "3",
                        "ref_id": "TABREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Experimental Results",
                "sec_num": "4.2"
            },
            {
                "text": "Accuracy (%) Ours (Composite Kernel) 91.97 Moschitti (2006) 4 reports the training times of the two kernels. We can see that 1) the two kinds of convolution tree kernels have similar computing time. Although computing the grammar-driven one requires exponential time in its worst case, however, in practice, it may only need ",
                "cite_spans": [
                    {
                        "start": 43,
                        "end": 59,
                        "text": "Moschitti (2006)",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 60,
                        "end": 61,
                        "text": "4",
                        "ref_id": "TABREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Method",
                "sec_num": null
            },
            {
                "text": "In this paper, we propose a novel grammar-driven convolution tree kernel for semantic role classification. More linguistic knowledge is considered in the new kernel design. The experimental results verify that the grammar-driven kernel is more effective in capturing syntactic structure features than the previous convolution tree kernel because it allows grammar-driven approximate matching of substructures and node features. We also discuss the criteria to determine the optional nodes in a CFG rule in defining our grammar-driven convolution tree kernel.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion and Future Work",
                "sec_num": "5"
            },
            {
                "text": "The extension of our work is to improve the performance of the entire semantic role labeling system using the grammar-driven tree kernel, including all four stages: pruning, semantic role identification, classification and post inference. In addition, a more interesting research topic is to study how to integrate linguistic knowledge and tree kernel methods to do feature selection for tree kernelbased NLP applications (Suzuki et al., 2004) . In detail, a linguistics and statistics-based theory that can suggest the effectiveness of different substructure features and whether they should be generated or not by the tree kernels would be worked out.",
                "cite_spans": [
                    {
                        "start": 422,
                        "end": 443,
                        "text": "(Suzuki et al., 2004)",
                        "ref_id": "BIBREF21"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion and Future Work",
                "sec_num": "5"
            },
            {
                "text": "Eq.(1) is defined over sub-structure of depth one. The approximate matching between structures of depth more than one can be achieved easily through the matching of sub-structures of depth one in the recursively-defined convolution kernel. We will discuss this issue when defining our kernel.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "4 To make sure that the new kernel is a proper kernel, we have to consider all the possible variations of the original sub-trees. Training program converges only when using a proper kernel.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "The Berkeley FrameNet Project. COLING-ACL-1998",
                "authors": [
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Baker",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [
                            "J"
                        ],
                        "last": "Fillmore",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [
                            "B"
                        ],
                        "last": "Lowe",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "F. Baker, C. J. Fillmore, and J. B. Lowe. 1998. The Berkeley FrameNet Project. COLING-ACL-1998",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Introduction to the CoNLL-2004 shared task: Semantic role labeling",
                "authors": [
                    {
                        "first": "Xavier",
                        "middle": [],
                        "last": "Carreras",
                        "suffix": ""
                    },
                    {
                        "first": "Llu\u0131s",
                        "middle": [],
                        "last": "M\u00e0rquez",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xavier Carreras and Llu\u0131s M\u00e0rquez. 2004. Introduction to the CoNLL-2004 shared task: Semantic role labeling. CoNLL-2004",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Introduction to the CoNLL-2005 shared task: Semantic role labeling. CoNLL-2005",
                "authors": [
                    {
                        "first": "Xavier",
                        "middle": [],
                        "last": "Carreras",
                        "suffix": ""
                    },
                    {
                        "first": "Llu\u0131s",
                        "middle": [],
                        "last": "M\u00e0rquez",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xavier Carreras and Llu\u0131s M\u00e0rquez. 2005. Introduction to the CoNLL-2005 shared task: Semantic role labeling. CoNLL-2005",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "A maximum-entropy-inspired parser",
                "authors": [
                    {
                        "first": "Eugene",
                        "middle": [],
                        "last": "Charniak",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "Proceedings ofNAACL-2000",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings ofNAACL-2000",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "A hybrid convolution tree kernel for semantic role labeling",
                "authors": [
                    {
                        "first": "Wanxiang",
                        "middle": [],
                        "last": "Che",
                        "suffix": ""
                    },
                    {
                        "first": "Min",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Ting",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Sheng",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "COLING-ACL-2006",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wanxiang Che, Min Zhang, Ting Liu and Sheng Li. 2006. A hybrid convolution tree kernel for semantic role labeling. COLING-ACL-2006(poster)",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Convolution kernels for natural language",
                "authors": [
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Collins",
                        "suffix": ""
                    },
                    {
                        "first": "Nigel",
                        "middle": [],
                        "last": "Duffy",
                        "suffix": ""
                    }
                ],
                "year": 2001,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Michael Collins and Nigel Duffy. 2001. Convolution kernels for natural language. NIPS-2001",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Automatic labeling of semantic roles",
                "authors": [
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Gildea",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Jurafsky",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Computational Linguistics",
                "volume": "28",
                "issue": "3",
                "pages": "245--288",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Daniel Gildea and Daniel Jurafsky. 2002. Automatic la- beling of semantic roles. Computational Linguistics, 28(3):245-288",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Convolution kernels on discrete structures",
                "authors": [
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Haussler",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "David Haussler. 1999. Convolution kernels on discrete structures. Technical Report UCSC-CRL-99-10",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Semantic argument classification exploiting argument interdependence",
                "authors": [
                    {
                        "first": "Ping",
                        "middle": [],
                        "last": "Zheng",
                        "suffix": ""
                    },
                    {
                        "first": "Jia",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    },
                    {
                        "first": "Hwee Tou",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ng",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zheng Ping Jiang, Jia Li and Hwee Tou Ng. 2005. Se- mantic argument classification exploiting argument interdependence. IJCAI-2005",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Text Categorization with Support Vecor Machine: learning with many relevant features",
                "authors": [
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Joachims",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "T. Joachims. 1998. Text Categorization with Support Vecor Machine: learning with many relevant fea- tures. ECML-1998",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Kernels for Semi-Structured Data",
                "authors": [
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Kashima",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Koyanagi",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kashima H. and Koyanagi T. 2003. Kernels for Semi- Structured Data. ICML-2003",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Text classification using string kernels",
                "authors": [
                    {
                        "first": "Huma",
                        "middle": [],
                        "last": "Lodhi",
                        "suffix": ""
                    },
                    {
                        "first": "Craig",
                        "middle": [],
                        "last": "Saunders",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Shawe-Taylor",
                        "suffix": ""
                    },
                    {
                        "first": "Nello",
                        "middle": [],
                        "last": "Cristianini",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Watkins",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Journal of Machine Learning Research",
                "volume": "2",
                "issue": "",
                "pages": "419--444",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello Cristianini and Chris Watkins. 2002. Text classifica- tion using string kernels. Journal of Machine Learn- ing Research, 2:419-444",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Building a large annotated corpus of English: the Penn Treebank",
                "authors": [
                    {
                        "first": "Mitchell",
                        "middle": [
                            "P"
                        ],
                        "last": "Marcus",
                        "suffix": ""
                    },
                    {
                        "first": "Mary",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Ann",
                        "middle": [],
                        "last": "Marcinkiewicz",
                        "suffix": ""
                    },
                    {
                        "first": "Beatrice",
                        "middle": [],
                        "last": "Santorini",
                        "suffix": ""
                    }
                ],
                "year": 1993,
                "venue": "Computational Linguistics",
                "volume": "19",
                "issue": "2",
                "pages": "313--330",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mitchell P. Marcus, Mary Ann Marcinkiewicz and Bea- trice Santorini. 1993. Building a large annotated cor- pus of English: the Penn Treebank. Computational Linguistics, 19(2):313-330",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "A study on convolution kernels for shallow statistic parsing",
                "authors": [
                    {
                        "first": "Alessandro",
                        "middle": [],
                        "last": "Moschitti",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alessandro Moschitti. 2004. A study on convolution ker- nels for shallow statistic parsing. ACL-2004",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Syntactic kernels for natural language learning: the semantic role labeling case",
                "authors": [
                    {
                        "first": "Alessandro",
                        "middle": [],
                        "last": "Moschitti",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "HLT-NAACL-2006",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alessandro Moschitti. 2006. Syntactic kernels for natu- ral language learning: the semantic role labeling case. HLT-NAACL-2006 (short paper)",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Mixing weak learners in semantic parsing",
                "authors": [
                    {
                        "first": "Rodney",
                        "middle": [
                            "D"
                        ],
                        "last": "Nielsen",
                        "suffix": ""
                    },
                    {
                        "first": "Sameer",
                        "middle": [],
                        "last": "Pradhan",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Rodney D. Nielsen and Sameer Pradhan. 2004. Mixing weak learners in semantic parsing. EMNLP-2004",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "The proposition bank: An annotated corpus of semantic roles",
                "authors": [
                    {
                        "first": "Martha",
                        "middle": [],
                        "last": "Palmer",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Gildea",
                        "suffix": ""
                    },
                    {
                        "first": "Paul",
                        "middle": [],
                        "last": "Kingsbury",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Computational Linguistics",
                "volume": "",
                "issue": "1",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Martha Palmer, Dan Gildea and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of seman- tic roles. Computational Linguistics, 31(1)",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Support vector learning for semantic argument classification",
                "authors": [
                    {
                        "first": "Kadri",
                        "middle": [],
                        "last": "Sameer Pradhan",
                        "suffix": ""
                    },
                    {
                        "first": "Valeri",
                        "middle": [],
                        "last": "Hacioglu",
                        "suffix": ""
                    },
                    {
                        "first": "Wayne",
                        "middle": [],
                        "last": "Krugler",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [
                            "H"
                        ],
                        "last": "Ward",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Martin",
                        "suffix": ""
                    },
                    {
                        "first": ";",
                        "middle": [],
                        "last": "Jurafsky",
                        "suffix": ""
                    },
                    {
                        "first": "Wayne",
                        "middle": [],
                        "last": "Sameer Pradhan",
                        "suffix": ""
                    },
                    {
                        "first": "Kadri",
                        "middle": [],
                        "last": "Ward",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [],
                        "last": "Hacioglu",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Martin",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Jurafsky",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Journal of Machine Learning",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne Ward, James H. Martin and Daniel Jurafsky. 2005a. Support vector learning for semantic argument classi- fication. Journal of Machine Learning Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Martin and Daniel Jurafsky. 2005b. Semantic role la- beling using different syntactic views. ACL-2005",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Semantic role labeling via integer linear programming inference",
                "authors": [
                    {
                        "first": "Vasin",
                        "middle": [],
                        "last": "Punyakanok",
                        "suffix": ""
                    },
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Roth",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Vasin Punyakanok, Dan Roth, Wen-tau Yih and Dav Zi- mak. 2004. Semantic role labeling via integer linear programming inference. COLING-2004",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "The necessity of syntactic parsing for semantic role labeling",
                "authors": [
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Vasin Punyakanok",
                        "suffix": ""
                    },
                    {
                        "first": "Wen",
                        "middle": [],
                        "last": "Roth",
                        "suffix": ""
                    },
                    {
                        "first": "Yih",
                        "middle": [],
                        "last": "Tau",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Vasin Punyakanok, Dan Roth and Wen Tau Yih. 2005. The necessity of syntactic parsing for semantic role labeling. IJCAI-2005",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Using LTAG based features in parse reranking",
                "authors": [
                    {
                        "first": "Libin",
                        "middle": [],
                        "last": "Shen",
                        "suffix": ""
                    },
                    {
                        "first": "Anoop",
                        "middle": [],
                        "last": "Sarkar",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [
                            "K"
                        ],
                        "last": "Joshi",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Libin Shen, Anoop Sarkar and A. K. Joshi. 2003. Using LTAG based features in parse reranking. EMNLP-03",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Convolution kernels with feature selection for Natural Language processing tasks",
                "authors": [
                    {
                        "first": "Jun",
                        "middle": [],
                        "last": "Suzuki",
                        "suffix": ""
                    },
                    {
                        "first": "Hideki",
                        "middle": [],
                        "last": "Isozaki",
                        "suffix": ""
                    },
                    {
                        "first": "Eisaku",
                        "middle": [],
                        "last": "Maede",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jun Suzuki, Hideki Isozaki and Eisaku Maede. 2004. Convolution kernels with feature selection for Natu- ral Language processing tasks. ACL-2004",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Statistical Learning Theory",
                "authors": [
                    {
                        "first": "Vladimir",
                        "middle": [
                            "N"
                        ],
                        "last": "Vapnik",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Vladimir N. Vapnik. 1998. Statistical Learning Theory. Wiley",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Calibrating features for semantic role labeling",
                "authors": [
                    {
                        "first": "Nianwen",
                        "middle": [],
                        "last": "Xue",
                        "suffix": ""
                    },
                    {
                        "first": "Martha",
                        "middle": [],
                        "last": "Palmer",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Nianwen Xue and Martha Palmer. 2004. Calibrating features for semantic role labeling. EMNLP-2004",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Kernel methods for relation extraction",
                "authors": [
                    {
                        "first": "Dmitry",
                        "middle": [],
                        "last": "Zelenko",
                        "suffix": ""
                    },
                    {
                        "first": "Chinatsu",
                        "middle": [],
                        "last": "Aone",
                        "suffix": ""
                    },
                    {
                        "first": "Anthony",
                        "middle": [],
                        "last": "Richardella",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Machine Learning Research",
                "volume": "3",
                "issue": "",
                "pages": "1083--1106",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dmitry Zelenko, Chinatsu Aone, and Anthony Rich- ardella. 2003. Kernel methods for relation extraction. Machine Learning Research, 3:1083-1106",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "A Composite Kernel to Extract Relations between Entities with both Flat and Structured Features. COLING-ACL-2006",
                "authors": [
                    {
                        "first": "Min",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Jie",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Jian",
                        "middle": [],
                        "last": "Su",
                        "suffix": ""
                    },
                    {
                        "first": "Guodong",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Min Zhang, Jie Zhang, Jian Su and Guodong Zhou. 2006. A Composite Kernel to Extract Relations be- tween Entities with both Flat and Structured Fea- tures. COLING-ACL-2006",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "it is very time-consuming to train a SVM classifier in a large dataset.",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "TABREF0": {
                "content": "<table><tr><td colspan=\"5\">============================================================================</td></tr><tr><td colspan=\"4\">Rule A: if 1 n and 2 n are pre-terminals, then:</td><td/></tr><tr><td>\u2206</td><td>1 ( , ) 2 n n \u2032</td><td>\u03bb = \u00d7</td><td>1 ( , ) 2 M f f</td><td>(5)</td></tr><tr><td colspan=\"5\">where 1 f and 2 f are features of nodes 1 n and 2 n re-</td></tr><tr><td colspan=\"2\">spectively, and</td><td colspan=\"2\">1 ( , ) 2 M f f is defined at Eq. (2).</td><td/></tr><tr><td colspan=\"5\">Rule B: else if both 1 n and 2 n are the same non-</td></tr><tr><td colspan=\"5\">terminals, then generate all variations of the subtrees</td></tr><tr><td colspan=\"5\">of depth one rooted by 1 n and 2 n (denoted by 1 n T</td></tr><tr><td>and 2 n</td><td/><td/><td/><td/></tr><tr><td/><td/><td/><td/><td colspan=\"2\">of removed optional nodes and mutated node fea-</td></tr><tr><td/><td/><td/><td/><td>tures, respectively.</td><td>\u2206</td><td>1 ( , ) 2 n n \u2032</td><td>is the number of the</td></tr><tr><td/><td/><td/><td/><td colspan=\"2\">common subtrees rooted at n 1 and n 2 , i.e. , 1 2 1 2 ( , ) ( ) ( ) i i subtree subtree i n n I n I n \u2032 \u2032 \u2032 \u2206 = \u22c5 \u2211</td><td>(4)</td></tr><tr><td/><td/><td/><td/><td colspan=\"2\">Please note that the value of</td><td>\u2206</td><td>1 ( , ) 2 n n \u2032</td><td>is no longer</td></tr><tr><td/><td/><td/><td/><td colspan=\"2\">an integer as that in the conventional one since op-</td></tr><tr><td/><td/><td/><td/><td colspan=\"2\">tional nodes and node feature mutations are consid-</td></tr><tr><td/><td/><td/><td/><td colspan=\"2\">ered in the new kernel.</td><td>\u2206</td><td>1 ( , ) 2 n n \u2032</td><td>can be further</td></tr><tr><td/><td/><td/><td/><td colspan=\"2\">computed by the following recursive rules:</td></tr></table>",
                "type_str": "table",
                "text": "the subtree i occurs with root at node n and zero otherwise, where a and b are the numbers",
                "html": null,
                "num": null
            },
            "TABREF1": {
                "content": "<table><tr><td/><td colspan=\"2\">Training Development</td><td>Test</td></tr><tr><td>Sentences</td><td>39,832</td><td>1,346</td><td>2,416</td></tr><tr><td colspan=\"2\">Arguments 239,858</td><td>8,346</td><td>14,077</td></tr></table>",
                "type_str": "table",
                "text": "Table 1 lists counts of sentences and arguments in the three data sets. Counts on the data set",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table><tr><td>for the de-</td></tr></table>",
                "type_str": "table",
                "text": "Performance comparison",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table><tr><td/><td colspan=\"2\">: PAF kernel only</td><td>87.7</td></tr><tr><td colspan=\"3\">Jiang et al. (2005): feature based</td><td>90.50</td></tr><tr><td colspan=\"4\">Pradhan et al. (2005a): feature based 91.0</td></tr><tr><td>Method</td><td/><td colspan=\"2\">Training Time</td></tr><tr><td/><td/><td>4 Sections</td><td>19 Sections</td></tr><tr><td>Ours:</td><td>grammar-</td><td>~8.1 hours</td><td>~7.9 days</td></tr><tr><td colspan=\"2\">driven tree kernel</td><td/><td/></tr><tr><td>Moschitti</td><td>(2006):</td><td>~7.9 hours</td><td>~7.1 days</td></tr><tr><td colspan=\"2\">non-grammar-driven</td><td/><td/></tr><tr><td>tree kernel</td><td/><td/><td/></tr></table>",
                "type_str": "table",
                "text": "Performance comparison between our method and previous work",
                "html": null,
                "num": null
            },
            "TABREF5": {
                "content": "<table/>",
                "type_str": "table",
                "text": "Training time comparisonTable",
                "html": null,
                "num": null
            }
        }
    }
}