{
    "paper_id": "P04-1052",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T12:36:01.753325Z"
    },
    "title": "Generating Referring Expressions in Open Domains",
    "authors": [
        {
            "first": "Advaith",
            "middle": [],
            "last": "Siddharthan",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Ann",
            "middle": [],
            "last": "Copestake",
            "suffix": "",
            "affiliation": {},
            "email": ""
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "We present an algorithm for generating referring expressions in open domains. Existing algorithms work at the semantic level and assume the availability of a classification for attributes, which is only feasible for restricted domains. Our alternative works at the realisation level, relies on Word-Net synonym and antonym sets, and gives equivalent results on the examples cited in the literature and improved results for examples that prior approaches cannot handle. We believe that ours is also the first algorithm that allows for the incremental incorporation of relations. We present a novel corpus-evaluation using referring expressions from the Penn Wall Street Journal Treebank.",
    "pdf_parse": {
        "paper_id": "P04-1052",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "We present an algorithm for generating referring expressions in open domains. Existing algorithms work at the semantic level and assume the availability of a classification for attributes, which is only feasible for restricted domains. Our alternative works at the realisation level, relies on Word-Net synonym and antonym sets, and gives equivalent results on the examples cited in the literature and improved results for examples that prior approaches cannot handle. We believe that ours is also the first algorithm that allows for the incremental incorporation of relations. We present a novel corpus-evaluation using referring expressions from the Penn Wall Street Journal Treebank.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Referring expression generation has historically been treated as a part of the wider issue of generating text from an underlying semantic representation. The task has therefore traditionally been approached at the semantic level. Entities in the real world are logically represented; for example (ignoring quantifiers), a big brown dog might be represented as big1(x) \u2227 brown1(x) \u2227 dog1(x), where the predicates big1, brown1 and dog1 represent different attributes of the variable (entity) x. The task of referring expression generation has traditionally been framed as the identification of the shortest logical description for the referent entity that differentiates it from all other entities in the discourse domain. For example, if there were a small brown dog (small1(x) \u2227 brown1(x) \u2227 dog1(x)) in context, the minimal description for the big brown dog would be big1(x) \u2227 dog1(x)1 .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "This semantic framework makes it difficult to apply existing referring expression generation algorithms to the many regeneration tasks that are important today; for example, summarisation, openended question answering and text simplification. Unlike in traditional generation, the starting point in these tasks is unrestricted text, rather than a semantic representation of a small domain. It is difficult to extract the required semantics from unrestricted text (this task would require sense disambiguation, among other issues) and even harder to construct a classification for the extracted predicates in the manner that existing approaches require (cf., \u00a72).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this paper, we present an algorithm for generating referring expressions in open domains. We discuss the literature and detail the problems in applying existing approaches to reference generation to open domains in \u00a72. We then present our approach in \u00a73, contrasting it with existing approaches. We extend our approach to handle relations in \u00a73.3 and present a novel corpus-based evaluation on the Penn WSJ Treebank in \u00a74.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The incremental algorithm (Reiter and Dale, 1992) is the most widely discussed attribute selection algorithm. It takes as input the intended referent and a contrast set of distractors (other entities that could be confused with the intended referent). Entities are represented as attribute value matrices (AVMs). The algorithm also takes as input a *preferred-attributes* list that contains, in order of preference, the attributes that human writers use to reference objects. For example, the preference might be {colour, size, shape...}. The algorithm then repeatedly selects attributes from *preferredattributes* that rule out at least one entity in the contrast set until all distractors have been ruled out.",
                "cite_spans": [
                    {
                        "start": 26,
                        "end": 49,
                        "text": "(Reiter and Dale, 1992)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overview of Prior Approaches",
                "sec_num": "2"
            },
            {
                "text": "It is instructive to look at how the incremental algorithm works. Consider an example where a large brown dog needs to be referred to. The contrast set contains a large black dog. These are represented by the AVMs shown below. Subsequent work on referring expression generation has expanded the logical framework to allow reference by negation (the dog that is not black) and references to multiple entities (the brown or black dogs) (van Deemter, 2002) , explored different search algorithms for finding the minimal description (e.g., Horacek (2003) ) and offered different representation frameworks like graph theory (Krahmer et al., 2003) as alternatives to AVMs. However, all these approaches are based on very similar formalisations of the problem, and all make the following assumptions:",
                "cite_spans": [
                    {
                        "start": 434,
                        "end": 453,
                        "text": "(van Deemter, 2002)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 536,
                        "end": 550,
                        "text": "Horacek (2003)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 619,
                        "end": 641,
                        "text": "(Krahmer et al., 2003)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overview of Prior Approaches",
                "sec_num": "2"
            },
            {
                "text": "1. A semantic representation exists. 2. A classification scheme for attributes exists. 3. The linguistic realisations are unambiguous. 4. Attributes cannot be reference modifying.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overview of Prior Approaches",
                "sec_num": "2"
            },
            {
                "text": "All these assumptions are violated when we move from generation in a very restricted domain to regeneration in an open domain. In regeneration tasks such as summarisation, open-ended question answering and text simplification, AVMs for entities are typically constructed from noun phrases, with the head noun as the type and pre-modifiers as attributes. Converting words into semantic labels would involve sense disambiguation, adding to the cost and complexity of the analysis module. Also, attribute classification is a hard problem and there is no existing classification scheme that can be used for open domains like newswire; for example, WordNet (Miller et al., 1993) organises adjectives as concepts that are related by the non-hierarchical relations of synonymy and antonymy (unlike nouns that are related through hierarchical links such as hyponymy, hypernymy and metonymy). In addition, selecting attributes at the semantic level is risky because their linguistic realisation might be ambiguous and many common adjectives are polysemous (cf., example 1 in \u00a73.1). Reference modification, which has not been considered in the referring expression generation literature, raises further issues; for example, referring to an alleged murderer as the murderer is potentially libellous.",
                "cite_spans": [
                    {
                        "start": 652,
                        "end": 673,
                        "text": "(Miller et al., 1993)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overview of Prior Approaches",
                "sec_num": "2"
            },
            {
                "text": "In addition to the above, there is the issue of overlap between values of attributes. The case of subsumption (for example, that the colour red subsumes crimson and the type dog subsumes chihuahua) has received formal treatment in the literature; Dale and Reiter (1995) provide a find-bestvalue function that evaluates tree-like hierarchies of values. As mentioned earlier, such hierarchical knowledge bases do not exist for open domains.",
                "cite_spans": [
                    {
                        "start": 247,
                        "end": 269,
                        "text": "Dale and Reiter (1995)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overview of Prior Approaches",
                "sec_num": "2"
            },
            {
                "text": "Further, a treatment of subsumption is insufficient, and degrees of intersection between attribute values also require consideration. van Deemter (2000) discusses the generation of vague descriptions when entities have gradable attributes like size; for example, in a domain with four mice sized 2, 5, 7 and 10cm, it is possible to refer to the large mouse (the mouse sized 10cm) or the two small mice (the mice sized 2 and 5cm). However, when applying referring expression generation to regeneration tasks where the representation of entities is derived from text rather than a knowledge base, we have to consider the case where the grading of attributes is not explicit. For example, we might need to compare the attribute dark with black, light or white.",
                "cite_spans": [
                    {
                        "start": 134,
                        "end": 152,
                        "text": "van Deemter (2000)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overview of Prior Approaches",
                "sec_num": "2"
            },
            {
                "text": "In contrast to previous approaches, our algorithm works at the level of words, not semantic labels, and measures the relatedness of adjectives (lexicalised attributes) using the lexical knowledge base Word-Net rather than a semantic classification. Our approach also addresses the issue of comparing intersective attributes that are not explicitly graded, by making novel use of the synonymy and antonymy links in WordNet. Further, it treats discriminating power as only one criteria for selecting attributes and allows for the easy incorporation of other considerations such as reference modification ( \u00a75).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Overview of Prior Approaches",
                "sec_num": "2"
            },
            {
                "text": "We define the following three quotients.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Quantifying Discriminating Power",
                "sec_num": "3.1"
            },
            {
                "text": "Similarity Quotient (SQ) We define similarity as transitive synonymy. The idea is that if X is a synonym of Y and Y is a synonym of Z, then X is likely to be similar to Z. The degree of similarity between two adjectives depends on how many steps must be made through WordNet synonymy lists to get from one to the other.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Quantifying Discriminating Power",
                "sec_num": "3.1"
            },
            {
                "text": "Suppose we need to find a referring expression for e 0 . For each adjective a j describing e 0 , we calculate a similarity quotient SQ j by initialising it to 0, forming a set of WordNet synonyms S 1 of a j , forming a synonymy set S 2 containing all the Word-Net synonyms of all the adjectives in S 1 and forming S 3 from S 2 similarly. Now for each adjective describing any distractor, we increment SQ j by 4 if it is present in S 1 , by 2 if it is present in S 2 , and by 1 if it is present in S 3 . SQ j now measures how similar a j is to other adjectives describing distractors.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Quantifying Discriminating Power",
                "sec_num": "3.1"
            },
            {
                "text": "Contrastive Quotient (CQ) Similarly, we define contrastive in terms of antonymy relationships. We form the set C 1 of strict WordNet antonyms of a j . The set C 2 consists of strict WordNet antonyms of members of S 1 and WordNet synonyms of members of C 1 . C 3 is similarly constructed from S 2 and C 2 . We now initialise CQ j to zero and for each adjective describing each distractor, we add w =\u2208 {4, 2, 1} to CQ j , depending on whether it is a member of C 1 , C 2 or C 3 . CQ j now measures how contrasting a j is to other adjectives describing distractors.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Quantifying Discriminating Power",
                "sec_num": "3.1"
            },
            {
                "text": "An attribute that has a high value of SQ has bad discriminating power. An attribute that has a high value of CQ has good discriminating power. We can now define the Discriminating Quotient (DQ) as DQ = CQ -SQ. We now have an order (decreasing DQs) in which to incorporate attributes. This constitutes our *preferred* list. We illustrate the benefits of our approach with two examples. Example 1: The Importance of Lexicalisation Previous referring expression generation algorithms ignore the issue of realising the logical description for the referent. The semantic labels are chosen such that they have a direct correspondence with their linguistic realisation and the realisation is thus considered trivial. Ambiguity and syntactically optional arguments are ignored. To illustrate one problem this causes, consider the two entities below:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discriminating Quotient (DQ)",
                "sec_num": null
            },
            {
                "text": "e1 e2 \uf8ee \uf8f0 type president age old tenure current \uf8f9 \uf8fb \uf8ee \uf8f0 type president age young tenure past \uf8f9 \uf8fb",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discriminating Quotient (DQ)",
                "sec_num": null
            },
            {
                "text": "If we followed the strict typing system used by previous algorithms, with *preferred*={age, tenure}, to refer to e1 we would compare the age attributes and rule out e2 and generate the old president. This expression is ambiguous since old can also mean previous. Models that select attributes at the semantic level will run into trouble when their linguistic realisations are ambiguous. In contrast, our algorithm, given flattened attribute lists:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discriminating Quotient (DQ)",
                "sec_num": null
            },
            {
                "text": "e1 e2 head president attrib old, current head president attrib young, past",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discriminating Quotient (DQ)",
                "sec_num": null
            },
            {
                "text": "successfully picks the current president as current has a higher DQ (2) than old (0):",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discriminating Quotient (DQ)",
                "sec_num": null
            },
            {
                "text": "attribute distractor CQ SQ DQ old e2{young, past} 4 4 0 current e2{young, past} 2 0 2",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discriminating Quotient (DQ)",
                "sec_num": null
            },
            {
                "text": "In this example, old is a WordNet antonym of young and a WordNet synonym of past. Current is a WordNet synonym of present, which is a WordNet antonym of past. Note that WordNet synonym and antonym links capture the implicit gradation in the lexicalised values of the age and tenure attributes.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discriminating Quotient (DQ)",
                "sec_num": null
            },
            {
                "text": "To illustrate another problem with the original incremental algorithm, consider three dogs: e1(a big black dog), e2(a small black dog) and e3(a tiny white dog). Consider using the original incremental algorithm to refer to e1 with *preferred*={colour, size}. The colour attribute black rules out e3. We then we have to select the size attribute big as well to rule out e2, thus generating the sub-optimal expression the big black dog. Here, the use of a predetermined *preferred* list fails to capture what is obvious from the context: that e1 stands out not because it is black, but because it is big.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Example 2: Naive Incrementality",
                "sec_num": null
            },
            {
                "text": "In our approach, for each of e1's attributes, we calculate DQ with respect to e2 and e3: Overall, big has a higher discriminating power (6) than black (-2) and rules out both e2 and e3. We therefore generate the big dog. Our incremental approach thus manages to select the attribute that stands out in context. This is because we construct the *preferred* list after observing the context. We discuss this issue further in the next section. Note again that WordNet antonym and synonym links capture the gradation in the lexicalised size and colour attributes. However, this only works where the gradation is along one axis; in particular, this approach will not work for colours in general, and cannot be used to deduce the relative similarity between yellow and orange as compared to, say, yellow and blue.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Example 2: Naive Incrementality",
                "sec_num": null
            },
            {
                "text": "The psycholinguistic justification for the incremental algorithm (IA) hinges on two premises:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Justifying our Algorithm",
                "sec_num": "3.2"
            },
            {
                "text": "1. Humans build referring expressions incrementally. 2. There is a preferred order in which humans select attributes (e.g., colour>shape>size...).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Justifying our Algorithm",
                "sec_num": "3.2"
            },
            {
                "text": "Our algorithm is also incremental. However, it departs significantly from premise 2. We assume that speakers pick out attributes that are distinctive in context (cf., example 2, previous section). Averaged over contexts, some attributes have more discriminating power than others (largely because of the way we visualise entities) and premise 2 is an approximation to our approach.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Justifying our Algorithm",
                "sec_num": "3.2"
            },
            {
                "text": "We now quantify the extra effort we are making to identify attributes that \"stand out\" in a given context. Let N be the maximum number of entities in the contrast set and n be the maximum number of attributes per entity. The table below compares the computational complexity of an optimal algorithm (such as Reiter (1990) ), our algorithm and the IA.",
                "cite_spans": [
                    {
                        "start": 308,
                        "end": 321,
                        "text": "Reiter (1990)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Justifying our Algorithm",
                "sec_num": "3.2"
            },
            {
                "text": "Incremental Algo Our Algorithm Optimal Algo O(nN ) O(n 2 N ) O(n2 N )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Justifying our Algorithm",
                "sec_num": "3.2"
            },
            {
                "text": "Both the IA and our algorithm are linear in the number of entities N . This is because neither algorithm allows backtracking; an attribute, once selected, cannot be discarded. In contrast, an optimal search requires O(2 N ) comparisons. As our algorithm compares each attribute of the discourse referent to every attribute of every distractor, it is quadratic in n. The IA compares each attribute of the discourse referent to only one attribute per distractor and is linear in n. Note, however, that values for n of over 4 are rare.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Justifying our Algorithm",
                "sec_num": "3.2"
            },
            {
                "text": "Semantically, attributes describe an entity (e.g., the small grey dog) and relations relate an entity to other entities (e.g., the dog in the bin). Relations are troublesome because in relating an entity e o to e 1 , we need to recursively generate a referring expression for e 1 . The IA does not consider relations and the referring expression is constructed out of attributes alone. The Dale and Haddock (1991) algorithm allows for relational descriptions but involves exponential global search, or a greedy search approximation. To incorporate relational descriptions in the incremental framework would require a classification system which somehow takes into account the relations themselves and the secondary entities e 1 etc. This again suggests that the existing algorithms force the incrementality at the wrong stage in the generation process. Our approach computes the order in which attributes are incorporated after observing the context, by quantifying their utility through the quotient DQ. This makes it easy for us to extend our algorithm to handle relations, because we can compute DQ for relations in much the same way as we did for attributes.We illustrate this for prepositions. e o , the relation is useful and we increment CQ by 4. This is an efficient non-recursive way of computing the quotients CQ and SQ for relations. We now discuss how to calculate DQ. For attributes, we defined DQ = CQ -SQ. However, as the linguistic realisation of a relation is a phrase and not a word, we would like to normalise the discriminating power of a relation with the length of its linguistic realisation. Calculating the length involves recursively generating referring expressions for the object of the preposition, an expensive task that we want to avoid unless we are actually using that relation in the final referring expression. We therefore initially approximate the length as follows. This approach can also be extended to allow for relations such as comparatives which have syntactically optional arguments (e.g., the earlier flight vs the flight earlier than UA941) which are not allowed for by approaches which ignore realisation.",
                "cite_spans": [
                    {
                        "start": 390,
                        "end": 413,
                        "text": "Dale and Haddock (1991)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Relations",
                "sec_num": "3.3"
            },
            {
                "text": "Our lexicalised context-sensitive incremental algorithm (below) generates a referring expression for Entity. As it recurses, it keeps track of entities it has used up in order to avoid entering loops like the dog in the bin containing the dog in the bin.... To generate a referring expression for an entity, the algorithm calculates the DQs for all its attributes and approximates the DQs for all its relations (2). It then forms the *preferred* list (3) and constructs the referring expression by adding elements of *preferred* till the contrast set is empty (4). This is straightforward for attributes (5). For relations (6), it needs to recursively generate the prepositional phrase first.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Lexicalised Context-Sensitive IA",
                "sec_num": "3.5"
            },
            {
                "text": "It checks that it hasn't entered a loop (6a), generates a new contrast set for the object of the relation (6(a)i), recursively generates a referring expression for the object of the preposition (6(a)ii), recalculates DQ (6(a)iii) and either incorporates the relation in the referring expression or shifts the relation down the *preferred* list (6(a)iv). This step ensures that an initial mis-estimation in the word length of a relation doesn't force its inclusion at the expense of shorter possibilities. If after incorporating all attributes and relations, the contrast set is still nonempty, the algorithm returns the best expression it can find (7). ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Lexicalised Context-Sensitive IA",
                "sec_num": "3.5"
            },
            {
                "text": "We now trace the algorithm above as it generates a referring expression for d1 in figure 1 . The algorithm presented above is designed to return the shortest referring expression that uniquely identifies an entity. If the scene in figure 1 were cluttered with bins, the algorithm would still refer to d1 as the dog in the bin as there is only one dog that is in a bin. The user gets no help in locating the bin. If helping the user locate entities is important to the discourse plan, we need to change step 6(a)(ELSE)i so that the contrast set includes all bins in context, not just bins that are objects of in relations of distractors of d1.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 89,
                        "end": 90,
                        "text": "1",
                        "ref_id": "FIGREF4"
                    },
                    {
                        "start": 238,
                        "end": 239,
                        "text": "1",
                        "ref_id": "FIGREF4"
                    }
                ],
                "eq_spans": [],
                "section": "An Example Trace:",
                "sec_num": null
            },
            {
                "text": "Our analysis so far has assumed that attributes are adjectives. However, many nominals introduced through relations can also be introduced in compound nominals, for example: This is an important issue for regeneration applications, where the AVMs for entities are constructed from text rather than a semantic knowledge base (which could be constructed such that such cases are stored in relational form, though possibly with an underspecified relation). We need to augment our algorithm so that it can compare AVMs like: In step 2, we compare a nominal attribute a nom of e o to the head noun of the object of a relation of e i . If they are similar, it is likely that any attributes of that object might help distinguish e o from e i . We then add those attributes to the attribute list of e i . Now, if SQ is non-zero, the nominal attribute a nom has bad discriminating power and we set DQ = -SQ. If SQ = 0, then a nom has good discriminating power and we set DQ = 4.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Compound Nominals",
                "sec_num": "3.6"
            },
            {
                "text": "We also extend the algorithm for calculating DQ for a relation [prep j e j ] of e o as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Compound Nominals",
                "sec_num": "3.6"
            },
            {
                "text": "1. IF any distractor e i has a nominal attribute a nom THEN (a) IF a nom is similar to the head of e j THEN i. Add all attributes of e o to the attribute list and calculate their DQs 2. calculate DQ for the relation as in section 3.4",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Compound Nominals",
                "sec_num": "3.6"
            },
            {
                "text": "We can demonstrate how this approach works using entities extracted from the following sentence (from the Wall Street Journal):",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Compound Nominals",
                "sec_num": "3.6"
            },
            {
                "text": "Also contributing to the firmness in copper, the analyst noted, was a report by Chicago purchasing agents, which precedes the full purchasing agents report that is due out today and gives an indication of what the full report might hold.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Compound Nominals",
                "sec_num": "3.6"
            },
            {
                "text": "Consider generating a referring expression for e o when the distractor is e 1 :",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Compound Nominals",
                "sec_num": "3.6"
            },
            {
                "text": "e o = \uf8ee \uf8ef \uf8ef \uf8f0 head report by \uf8ee \uf8f0 head agents attrib [Chicago, purchasing] \uf8f9 \uf8fb \uf8f9 \uf8fa \uf8fa \uf8fb e 1 = head report attributes [full, purchasing, agents]",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Compound Nominals",
                "sec_num": "3.6"
            },
            {
                "text": "The distractor the full purchasing agents report contains the nominal attribute agents. To compare report by Chicago purchasing agents with full purchasing agents report, our algorithm flattens the former to Chicago purchasing agents report. Our algorithm now gives: We thus generate the referring expression the Chicago report. This approach takes advantage of the flexibility of the relationships that can hold between nouns in a compound: although examples can be devised where removing a nominal causes ungrammaticality, it works well enough empirically.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Compound Nominals",
                "sec_num": "3.6"
            },
            {
                "text": "To generate a referring expression for e1 (full purchasing agents report) when the distractor is e o (report by Chicago purchasing agents), our algorithm again flattens e o to obtain:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Compound Nominals",
                "sec_num": "3.6"
            },
            {
                "text": "DQ agents = -4, DQ purchasing = -4 DQ full = 4",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Compound Nominals",
                "sec_num": "3.6"
            },
            {
                "text": "The generated referring expression is the full report. This is identical to the referring expression used in the original text.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Compound Nominals",
                "sec_num": "3.6"
            },
            {
                "text": "As our algorithm works in open domains, we were able to perform a corpus-based evaluation using the Penn WSJ Treebank (Marcus et al., 1993) . Our evaluation aimed to reproduce existing referring expressions (NPs with a definite determiner) in the Penn Treebank by providing our algorithm as input:",
                "cite_spans": [
                    {
                        "start": 118,
                        "end": 139,
                        "text": "(Marcus et al., 1993)",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation",
                "sec_num": "4"
            },
            {
                "text": "1. The first mention NP for that reference.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation",
                "sec_num": "4"
            },
            {
                "text": "For each referring expression (NP with a definite determiner) in the Penn Treebank, we automatically identified its first mention and all its distractors in a four sentence window, as described in \u00a74.1. We then used our program to generate a referring expression for the first mention NP, giving it a contrastset containing the distractor NPs. Our evaluation compared this generated description with the original WSJ reference that we had started out with. Our algorithm was developed using toy examples and counter-examples constructed by hand, and the Penn Treebank was unseen data for this evaluation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The contrast set of distractor NPs",
                "sec_num": "2."
            },
            {
                "text": "For every definite noun phrase NP o in the Penn Treebank, we shortlisted all the noun phrases NP i in a discourse window of four sentences (the two preceding sentences, current sentence and the following sentence) that had a head noun identical to or a WordNet synonym of the head noun of NP o .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Identifying Antecedents and Distractors",
                "sec_num": "4.1"
            },
            {
                "text": "We compared the set of attributes and relations for each shortlisted NP i that preceded NP o in the discourse window with that of NP o . If the attributes and relations set of NP i was a superset of that of NP o , we assumed that NP o referred to NP i and added NP i to an antecedent set. We added all other NP i to the contrast set of distractors.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Identifying Antecedents and Distractors",
                "sec_num": "4.1"
            },
            {
                "text": "Similarly, we excluded any noun phrase NP i that appeared in the discourse after NP o whose attributes and relations set was a subset of NP o 's and added the remaining NP i to the contrast set. We then selected the longest noun phrase in the antecedent set to be the antecedent that we would try and generate a referring expression from.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Identifying Antecedents and Distractors",
                "sec_num": "4.1"
            },
            {
                "text": "The table below gives some examples of distractors that our program found using WordNet synonyms to compare head nouns: ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 10,
                        "end": 15,
                        "text": "below",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Identifying Antecedents and Distractors",
                "sec_num": "4.1"
            },
            {
                "text": "There were 146 instances of definite descriptions in the WSJ where the following conditions (that ensure that the referring expression generation task is nontrivial) were satisfied:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "4.2"
            },
            {
                "text": "1. The definite NP (referring expression) contained at least one attribute or relation. 2. An antecedent was found for the definite NP. 3. There was at least one distractor NP in the discourse window.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "4.2"
            },
            {
                "text": "In 81.5% of these cases, our program returned a referring expression that was identical to the one used in the WSJ. This is a surprisingly high accuracy, considering that there is a fair amount of variability in the way human writers use referring expressions. For comparison, the baseline of reproducing the antecedent NP performed at 48%2 . Some errors were due to non-recognition of multiword expessions in the antecedent (for example, our program generated care product from personal care product). In many of the remaining error cases, it was difficult to decide whether what our program generated was acceptable or wrong. For example, the WSJ contained the referring expression the one-day limit, where the automatically detected antecedent was the maximum one-day limit for the S&P 500 stock-index futures contract and the automatically detected contrast set was: {the five-point opening limit for the contract, the 12-point limit, the 30-point limit, the intermediate limit of 20 points} Our program generated the maximum limit, where the WSJ writer preferred the one-day limit.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "4.2"
            },
            {
                "text": "The analysis thus far has assumed that all attributes modify the referent rather than the reference to the referent. However, for example, if e1 is an alleged murderer, the attribute alleged modifies the reference murderer rather than the referent e1 and referring to e1 as the murderer would be factually incorrect. Logically e1 could be represented as (alleged1(murderer1))(x), rather than alleged1(x) \u2227 murderer1(x). This is no longer first-order, and presents new difficulties for the traditional formalisation of the reference generation problem. One (inelegant) solution would be to introduce a new predicate allegedMurderer1(x).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Reference Modifying Attributes",
                "sec_num": "5.1"
            },
            {
                "text": "A working approach in our framework would be to add a large positive weight to the DQs of reference modifying attributes, thus forcing them to be selected in the referring expression.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Reference Modifying Attributes",
                "sec_num": "5.1"
            },
            {
                "text": "The incremental algorithm assumes the availability of a contrast set and does not provide an algorithm for constructing and updating it. The contrast set, in general, needs to take context into account. Krahmer and Theune (2002) propose an extension to the IA which treats the context set as a combination of a discourse domain and a salience function. The black dog would then refer to the most salient entity in the discourse domain that is both black and a dog.",
                "cite_spans": [
                    {
                        "start": 203,
                        "end": 228,
                        "text": "Krahmer and Theune (2002)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discourse Context and Salience",
                "sec_num": "5.2"
            },
            {
                "text": "Incorporating salience into our algorithm is straightforward. As described earlier, we compute the quotients SQ and CQ for each attribute or relation by adding an amount w \u2208 {4, 2, 1} to the relevant quotient based on a comparison with the attributes and relations of each distractor. We can incorporate salience by weighting w with the salience of the distractor whose attribute or relation we are considering. This will result in attributes and relations with high discriminating power with regard to more salient distractors getting selected first in the incremental process.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discourse Context and Salience",
                "sec_num": "5.2"
            },
            {
                "text": "In many situations, attributes and relations serve different discourse functions. For example, attributes might be used to help the hearer identify an entity while relations might serve to help locate the entity. This needs to be taken into account when generating a referring expression. If we were generating instructions for using a machine, we might want to include both attributes and relations; so to instruct the user to switch on the power, we might say switch on the red button on the top-left corner. This would help the user locate the switch (on the top-left corner) and identify it (red). If we were helping a chef find the salt in a kitchen, we might want to use only relations because the chef knows what salt looks like. The salt behind the corn flakes on the shelf above the fridge is in this context preferable to the white powder. If the discourse plan that controls generation requires our algorithm to preferentially select relations or attributes, it can add a positive amount \u03b1 to their DQs. Then, the resultant formula is DQ = (CQ -SQ)/length + \u03b1, where length = 1 for attributes and by default \u03b1 = 0 for both relations and attributes.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discourse Plans",
                "sec_num": "5.3"
            },
            {
                "text": "We have described an algorithm for generating referring expressions that can be used in any domain. Our algorithm selects attributes and relations that are distinctive in context. It does not rely on the availability of an adjective classification scheme and uses WordNet antonym and synonym lists instead. It is also, as far as we know, the first algorithm that allows for the incremental incorporation of relations and the first that handles nominals. In a novel evaluation, our algorithm successfully generates identical referring expressions to those in the Penn WSJ Treebank in over 80% of cases.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions and Future Work",
                "sec_num": "6"
            },
            {
                "text": "In future work, we plan to use this algorithm as part of a system for generation from a database of user opinions on products which has been automatically extracted from newsgroups and similar text. This is midway between regeneration and the classical task of generating from a knowledge base because, while the database itself provides structure, many of the field values are strings corresponding to phrases used in the original text. Thus, our lexicalised approach is directly applicable to this task.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions and Future Work",
                "sec_num": "6"
            },
            {
                "text": "Thanks are due to Kees van Deemter and three anonymous ACL reviewers for useful feedback on prior versions of this paper.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgements",
                "sec_num": "7"
            },
            {
                "text": "This document was generated partly in the context of the Deep Thought project, funded under the Thematic Programme User-friendly Information Society of the 5th Framework Programme of the European Community (Contract N IST-2001-37836) ",
                "cite_spans": [
                    {
                        "start": 218,
                        "end": 233,
                        "text": "IST-2001-37836)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgements",
                "sec_num": "7"
            },
            {
                "text": "The predicate dog1 is selected because it has a distinguished status, referred to as type inReiter and Dale (1992). One such predicate has to to be present in the description.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We are only evaluating content selection (the nouns and pre-and post-modifiers) and ignore determiner choice.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Generating referring expressions involving relations",
                "authors": [
                    {
                        "first": "Robert",
                        "middle": [],
                        "last": "Dale",
                        "suffix": ""
                    },
                    {
                        "first": "Nicholas",
                        "middle": [],
                        "last": "Haddock",
                        "suffix": ""
                    }
                ],
                "year": 1991,
                "venue": "Proceedings of the 5th Conference of the European Chapter of the Association for Computational Linguistics (EACL'91)",
                "volume": "",
                "issue": "",
                "pages": "161--166",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Robert Dale and Nicholas Haddock. 1991. Gen- erating referring expressions involving relations. In Proceedings of the 5th Conference of the Eu- ropean Chapter of the Association for Compu- tational Linguistics (EACL'91), pages 161-166, Berlin, Germany.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Computational interpretations of the Gricean maxims in the generation of referring expressions",
                "authors": [
                    {
                        "first": "Robert",
                        "middle": [],
                        "last": "Dale",
                        "suffix": ""
                    },
                    {
                        "first": "Ehud",
                        "middle": [],
                        "last": "Reiter",
                        "suffix": ""
                    }
                ],
                "year": 1995,
                "venue": "Cognitive Science",
                "volume": "19",
                "issue": "",
                "pages": "233--263",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Robert Dale and Ehud Reiter. 1995. Computational interpretations of the Gricean maxims in the gen- eration of referring expressions. Cognitive Sci- ence, 19:233-263.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "A best-first search algorithm for generating referring expressions",
                "authors": [
                    {
                        "first": "Helmut",
                        "middle": [],
                        "last": "Horacek",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL'03)",
                "volume": "",
                "issue": "",
                "pages": "103--106",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Helmut Horacek. 2003. A best-first search algo- rithm for generating referring expressions. In Proceedings of the 11th Conference of the Eu- ropean Chapter of the Association for Compu- tational Linguistics (EACL'03), pages 103-106, Budapest, Hungary.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Efficient context-sensitive generation of referring expressions",
                "authors": [
                    {
                        "first": "Emiel",
                        "middle": [],
                        "last": "Krahmer",
                        "suffix": ""
                    },
                    {
                        "first": "Mari\u00ebt",
                        "middle": [],
                        "last": "Theune",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Information Sharing: Givenness and Newness in Language Processing",
                "volume": "",
                "issue": "",
                "pages": "223--264",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Emiel Krahmer and Mari\u00ebt Theune. 2002. Efficient context-sensitive generation of referring expres- sions. In Kees van Deemter and Rodger Kib- ble, editors, Information Sharing: Givenness and Newness in Language Processing, pages 223- 264. CSLI Publications, Stanford,California.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Graph-based generation of referring expressions",
                "authors": [
                    {
                        "first": "Emiel",
                        "middle": [],
                        "last": "Krahmer",
                        "suffix": ""
                    },
                    {
                        "first": "Sebastiaan",
                        "middle": [],
                        "last": "Van Erk",
                        "suffix": ""
                    },
                    {
                        "first": "Andr\u00e9",
                        "middle": [],
                        "last": "Verleg",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Computational Linguistics",
                "volume": "29",
                "issue": "1",
                "pages": "53--72",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Emiel Krahmer, Sebastiaan van Erk, and Andr\u00e9 Verleg. 2003. Graph-based generation of re- ferring expressions. Computational Linguistics, 29(1):53-72.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Building a large natural language corpus of English: The Penn Treebank",
                "authors": [
                    {
                        "first": "Mitchell",
                        "middle": [],
                        "last": "Marcus",
                        "suffix": ""
                    },
                    {
                        "first": "Beatrice",
                        "middle": [],
                        "last": "Santorini",
                        "suffix": ""
                    },
                    {
                        "first": "Mary",
                        "middle": [],
                        "last": "Marcinkiewicz",
                        "suffix": ""
                    }
                ],
                "year": 1993,
                "venue": "Computational Linguistics",
                "volume": "19",
                "issue": "",
                "pages": "313--330",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mitchell Marcus, Beatrice Santorini, and Mary Marcinkiewicz. 1993. Building a large natural language corpus of English: The Penn Treebank. Computational Linguistics, 19:313-330.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "The computational complexity of avoiding conversational implicatures",
                "authors": [
                    {
                        "first": "George",
                        "middle": [
                            "A"
                        ],
                        "last": "Miller",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Beckwith",
                        "suffix": ""
                    },
                    {
                        "first": "Christiane",
                        "middle": [
                            "D"
                        ],
                        "last": "Fellbaum",
                        "suffix": ""
                    },
                    {
                        "first": "Derek",
                        "middle": [],
                        "last": "Gross",
                        "suffix": ""
                    },
                    {
                        "first": "Katherine",
                        "middle": [],
                        "last": "Miller ; Princeton",
                        "suffix": ""
                    },
                    {
                        "first": "N",
                        "middle": [
                            "J Ehud"
                        ],
                        "last": "Reiter",
                        "suffix": ""
                    }
                ],
                "year": 1990,
                "venue": "Proceedings of the 28th Annual Meeting of Association for Computational Linguistics (ACL'90)",
                "volume": "",
                "issue": "",
                "pages": "97--104",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "George A. Miller, Richard Beckwith, Christiane D. Fellbaum, Derek Gross, and Katherine Miller. 1993. Five Papers on WordNet. Technical report, Princeton University, Princeton, N.J. Ehud Reiter. 1990. The computational complex- ity of avoiding conversational implicatures. In Proceedings of the 28th Annual Meeting of Asso- ciation for Computational Linguistics (ACL'90), pages 97-104, Pittsburgh, Pennsylvania.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "A fast algorithm for the generation of referring expressions",
                "authors": [
                    {
                        "first": "Ehud",
                        "middle": [],
                        "last": "Reiter",
                        "suffix": ""
                    },
                    {
                        "first": "Robert",
                        "middle": [],
                        "last": "Dale",
                        "suffix": ""
                    }
                ],
                "year": 1992,
                "venue": "Proceedings of the 14th International Conference on Computational Linguistics (COL-ING'92)",
                "volume": "",
                "issue": "",
                "pages": "179--185",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ehud Reiter and Robert Dale. 1992. A fast al- gorithm for the generation of referring expres- sions. In Proceedings of the 14th International Conference on Computational Linguistics (COL- ING'92), pages 232-238, Nantes, France. Kees van Deemter. 2000. Generating vague de- scriptions. In Proceedings of the 1st Interna- tional Conference on Natural Language Genera- tion (INLG'00), pages 179-185, Mitzpe Ramon, Israel.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Generating referring expressions: Boolean extensions of the incremental algorithm",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Van Deemter",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Computational Linguistics",
                "volume": "28",
                "issue": "1",
                "pages": "37--52",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "van Deemter. 2002. Generating referring ex- pressions: Boolean extensions of the incremental algorithm. Computational Linguistics, 28(1):37- 52.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Assuming that the *preferred-attributes* list is [size, colour, ...], the algorithm would first compare the values of the size attribute (both large), disregard that attribute as not being discriminating, compare the values of the colour attribute and return the brown dog.",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "DQ for Relations Suppose the referent entity e ref contains a relation [prep o e o ] that we need to calculate the three quotients for (cf., figure 1 for representation of relations in AVMs). We consider each entity e i in the contrast set for e ref in turn. If e i does not have a prep o relation then the relation is useful and we increment CQ by 4. If e i has a prep o relation then two cases arise. If the object of e i 's prep o relation is e o then we increment SQ by 4. If it is not",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "The realisation of a relation [prep o e o ] consists of prep o , a determiner and the referring expression for e o . If none of e ref 's distractors have a prep o relation then we only require the head noun of e o in the referring expression and length = 3. In this case, the relation is sufficient to identify both entities; for example, even if there were multiple bins in figure 1, as long as only one dog is in a bin, the reference the dog in the bin succeeds in uniquely referencing both the dog and the bin. If n distractors of e ref contain a prep o relation with a non-e o object that is distractor for e o , we set length = 3 + n. This is an estimate for the word length of the realised relation that assumes one extra attribute for distinguishing e o from each distractor. Normalisation by estimated length is vital; if e o requires a long description, the relations's DQ should be small so that shorter possibilities are considered first in the incremental process. The formula for DQ for relations is therefore DQ = (CQ -SQ)/length.",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF3": {
                "num": null,
                "text": "set generate-ref-exp(Entity, ContrastSet, UsedEntities) 1. IF ContrastSet = [] THEN RETURN {Entity.head} 2. Calculate CQ, SQ and DQ for each attribute and relation of Entity (as in Sec 3.1 and 3.4) 3. Let *preferred* be the list of attributes/ relations sorted in decreasing order of DQs. FOR each element (Mod) of *preferred* DO steps 4, 5 and 6 4. IF ContrastSet = [] THEN RETURN RefExp \u222a {Entity.head} 5. IF Mod is an Attribute THEN (a) LET RefExp = {Mod} \u222a RefExp (b) Remove from ContrastSet, any entities Mod rules out 6. IF Mod is a Relation [prep i e i ] THEN (a) IF e i \u2208 U sedEntities THEN i. Set DQ = -\u221e ii. Move Mod to the end of *",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF4": {
                "num": null,
                "text": "Figure 1: AVMs for two dogs and a bin",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF5": {
                "num": null,
                "text": "1. a church in Paris \u2194 a Paris church 2. a novel by Archer \u2194 an Archer novel 3. a company from London \u2194 a London company",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF6": {
                "num": null,
                "text": "algorithm for calculating SQ and CQ for a nominal attribute a nom of entity e o is: FOR each distractor e i of e o DO 1. IF a nom is similar to any nominal attribute of e i THEN SQ = SQ + 4 2. IF a nom is similar to the head noun of the object of any relation of e i THEN (a) SQ = SQ + 4 (b) flatten that relation for e i , i.e., add the attributes of the object of the relation to the attribute list for e i",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            },
            "FIGREF7": {
                "num": null,
                "text": "DQ agents = -4, DQ purchasing = -4, DQ Chicago = 4, DQ by Chicago purchasing agents = 4/4",
                "uris": null,
                "fig_num": null,
                "type_str": "figure"
            }
        }
    }
}