{
    "paper_id": "P07-1037",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T11:57:43.013219Z"
    },
    "title": "Supertagged Phrase-Based Statistical Machine Translation",
    "authors": [
        {
            "first": "Hany",
            "middle": [],
            "last": "Hassan",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Dublin City University",
                "location": {
                    "settlement": "Dublin 9",
                    "country": "Ireland"
                }
            },
            "email": "hhasan@computing.dcu.ie"
        },
        {
            "first": "Sima",
            "middle": [
                "'"
            ],
            "last": "Khalil",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Amsterdam",
                "location": {
                    "settlement": "Amsterdam",
                    "country": "The Netherlands"
                }
            },
            "email": "simaan@science.uva.nl"
        },
        {
            "first": "",
            "middle": [],
            "last": "An",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Amsterdam",
                "location": {
                    "settlement": "Amsterdam",
                    "country": "The Netherlands"
                }
            },
            "email": ""
        },
        {
            "first": "Andy",
            "middle": [],
            "last": "Way",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Dublin City University",
                "location": {
                    "settlement": "Dublin 9",
                    "country": "Ireland"
                }
            },
            "email": "away@computing.dcu.ie"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Until quite recently, extending Phrase-based Statistical Machine Translation (PBSMT) with syntactic structure caused system performance to deteriorate. In this work we show that incorporating lexical syntactic descriptions in the form of supertags can yield significantly better PBSMT systems. We describe a novel PBSMT model that integrates supertags into the target language model and the target side of the translation model. Two kinds of supertags are employed: those from Lexicalized Tree-Adjoining Grammar and Combinatory Categorial Grammar. Despite the differences between these two approaches, the supertaggers give similar improvements. In addition to supertagging, we also explore the utility of a surface global grammaticality measure based on combinatory operators. We perform various experiments on the Arabic to English NIST 2005 test set addressing issues such as sparseness, scalability and the utility of system subcomponents. Our best result (0.4688 BLEU) improves by 6.1% relative to a state-of-theart PBSMT model, which compares very favourably with the leading systems on the NIST 2005 task.",
    "pdf_parse": {
        "paper_id": "P07-1037",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Until quite recently, extending Phrase-based Statistical Machine Translation (PBSMT) with syntactic structure caused system performance to deteriorate. In this work we show that incorporating lexical syntactic descriptions in the form of supertags can yield significantly better PBSMT systems. We describe a novel PBSMT model that integrates supertags into the target language model and the target side of the translation model. Two kinds of supertags are employed: those from Lexicalized Tree-Adjoining Grammar and Combinatory Categorial Grammar. Despite the differences between these two approaches, the supertaggers give similar improvements. In addition to supertagging, we also explore the utility of a surface global grammaticality measure based on combinatory operators. We perform various experiments on the Arabic to English NIST 2005 test set addressing issues such as sparseness, scalability and the utility of system subcomponents. Our best result (0.4688 BLEU) improves by 6.1% relative to a state-of-theart PBSMT model, which compares very favourably with the leading systems on the NIST 2005 task.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Within the field of Machine Translation, by far the most dominant paradigm is Phrase-based Statistical Machine Translation (PBSMT) (Koehn et al., 2003; Tillmann & Xia, 2003) . However, unlike in rule-and example-based MT, it has proven difficult to date to incorporate linguistic, syntactic knowledge in order to improve translation quality. Only quite recently have (Chiang, 2005) and (Marcu et al., 2006) shown that incorporating some form of syntactic structure could show improvements over a baseline PBSMT system. While (Chiang, 2005) avails of structure which is not linguistically motivated, (Marcu et al., 2006) employ syntactic structure to enrich the entries in the phrase table.",
                "cite_spans": [
                    {
                        "start": 131,
                        "end": 151,
                        "text": "(Koehn et al., 2003;",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 152,
                        "end": 173,
                        "text": "Tillmann & Xia, 2003)",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 367,
                        "end": 381,
                        "text": "(Chiang, 2005)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 386,
                        "end": 406,
                        "text": "(Marcu et al., 2006)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 525,
                        "end": 539,
                        "text": "(Chiang, 2005)",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 599,
                        "end": 619,
                        "text": "(Marcu et al., 2006)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In this paper we explore a novel approach towards extending a standard PBSMT system with syntactic descriptions: we inject lexical descriptions into both the target side of the phrase translation table and the target language model. Crucially, the kind of lexical descriptions that we employ are those that are commonly devised within lexicon-driven approaches to linguistic syntax, e.g. Lexicalized Tree-Adjoining Grammar (Joshi & Schabes, 1992; Bangalore & Joshi, 1999) and Combinary Categorial Grammar (Steedman, 2000) . In these linguistic approaches, it is assumed that the grammar consists of a very rich lexicon and a tiny, impoverished 1 set of combinatory operators that assemble lexical entries together into parse-trees. The lexical entries consist of syntactic constructs ('supertags') that describe information such as the POS tag of the word, its subcategorization information and the hierarchy of phrase categories that the word projects upwards. In this work we employ the lexical entries but exchange the algebraic combinatory operators with the more robust and efficient supertagging approach: like standard taggers, supertaggers employ probabilities based on local context and can be implemented using finite state technology, e.g. Hidden Markov Models (Bangalore & Joshi, 1999) .",
                "cite_spans": [
                    {
                        "start": 423,
                        "end": 446,
                        "text": "(Joshi & Schabes, 1992;",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 447,
                        "end": 471,
                        "text": "Bangalore & Joshi, 1999)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 505,
                        "end": 521,
                        "text": "(Steedman, 2000)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 1272,
                        "end": 1297,
                        "text": "(Bangalore & Joshi, 1999)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "There are currently two supertagging approaches available: LTAG-based (Bangalore & Joshi, 1999) and CCG-based (Clark & Curran, 2004) . Both the LTAG (Chen et al., 2006) and the CCG supertag sets (Hockenmaier, 2003) were acquired from the WSJ section of the Penn-II Treebank using handbuilt extraction rules. Here we test both the LTAG and CCG supertaggers. We interpolate (log-linearly) the supertagged components (language model and phrase table) with the components of a standard PBSMT system. Our experiments on the Arabic-English NIST 2005 test suite show that each of the supertagged systems significantly improves over the baseline PBSMT system. Interestingly, combining the two taggers together diminishes the benefits of supertagging seen with the individual LTAG and CCG systems. In this paper we discuss these and other empirical issues.",
                "cite_spans": [
                    {
                        "start": 59,
                        "end": 95,
                        "text": "LTAG-based (Bangalore & Joshi, 1999)",
                        "ref_id": null
                    },
                    {
                        "start": 110,
                        "end": 132,
                        "text": "(Clark & Curran, 2004)",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 149,
                        "end": 168,
                        "text": "(Chen et al., 2006)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 195,
                        "end": 214,
                        "text": "(Hockenmaier, 2003)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The remainder of the paper is organised as follows: in section 2 we discuss the related work on enriching PBSMT with syntactic structure. In section 3, we describe the baseline PBSMT system which our work extends. In section 4, we detail our approach. Section 5 describes the experiments carried out, together with the results obtained. Section 6 concludes, and provides avenues for further work.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Until very recently, the experience with adding syntax to PBSMT systems was negative. For example, (Koehn et al., 2003) demonstrated that adding syntax actually harmed the quality of their SMT system. Among the first to demonstrate improvement when adding recursive structure was (Chiang, 2005) , who allows for hierarchical phrase probabilities that handle a range of reordering phenomena in the correct fashion. Chiang's derived grammar does not rely on any linguistic annotations or assumptions, so that the 'syntax' induced is not linguistically motivated.",
                "cite_spans": [
                    {
                        "start": 99,
                        "end": 119,
                        "text": "(Koehn et al., 2003)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 280,
                        "end": 294,
                        "text": "(Chiang, 2005)",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "Coming right up to date, (Marcu et al., 2006 ) demonstrate that 'syntactified' target language phrases can improve translation quality for Chinese-English. They employ a stochastic, top-down transduction process that assigns a joint probability to a source sentence and each of its alternative translations when rewriting the target parse-tree into a source sentence. The rewriting/transduction process is driven by \"xRS rules\", each consisting of a pair of a source phrase and a (possibly only partially) lexicalized syntactified target phrase. In order to extract xRS rules, the word-to-word alignment induced from the parallel training corpus is used to guide heuristic tree 'cutting' criteria.",
                "cite_spans": [
                    {
                        "start": 25,
                        "end": 44,
                        "text": "(Marcu et al., 2006",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "While the research of (Marcu et al., 2006) has much in common with the approach proposed here (such as the syntactified target phrases), there remain a number of significant differences. Firstly, rather than induce millions of xRS rules from parallel data, we extract phrase pairs in the standard way (Och & Ney, 2003) and associate with each phrase-pair a set of target language syntactic structures based on supertag sequences. Relative to using arbitrary parse-chunks, the power of supertags lies in the fact that they are, syntactically speaking, rich lexical descriptions. A supertag can be assigned to every word in a phrase. On the one hand, the correct sequence of supertags could be assembled together, using only impoverished combinatory operators, into a small set of constituents/parses ('almost' a parse). On the other hand, because supertags are lexical entries, they facilitate robust syntactic processing (using Markov models, for instance) which does not necessarily aim at building a fully connected graph.",
                "cite_spans": [
                    {
                        "start": 22,
                        "end": 42,
                        "text": "(Marcu et al., 2006)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 301,
                        "end": 318,
                        "text": "(Och & Ney, 2003)",
                        "ref_id": "BIBREF11"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "A second major difference with xRS rules is that our supertag-enriched target phrases need not be generalized into (xRS or any other) rules that work with abstract categories. Finally, like POS tagging, supertagging is more efficient than actual parsing or tree transduction.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "2"
            },
            {
                "text": "We present the baseline PBSMT model which we extend with supertags in the next section. Our baseline PBSMT model uses GIZA++2 to obtain word-level alignments in both language directions. The bidirectional word alignment is used to obtain phrase translation pairs using heuristics presented in (Och & Ney, 2003) and (Koehn et al., 2003) , and the Moses decoder was used for phrase extraction and decoding. 3Let t and s be the target and source language sentences respectively. Any (target or source) sentence x will consist of two parts: a bag of elements (words/phrases etc.) and an order over that bag. In other words, x = \u03c6 x , O x , where \u03c6 x stands for the bag of phrases that constitute x, and O x for the order of the phrases as given in x (O x can be implemented as a function from a bag of tokens \u03c6 x to a set with a finite number of positions). Hence, we may separate order from content:",
                "cite_spans": [
                    {
                        "start": 293,
                        "end": 310,
                        "text": "(Och & Ney, 2003)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 315,
                        "end": 335,
                        "text": "(Koehn et al., 2003)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baseline Phrase-Based SMT System",
                "sec_num": "3"
            },
            {
                "text": "arg max t P (t|s) = arg max t P (s | t)P (t) (1) = arg max \u03c6t,Ot T M P (\u03c6 s | \u03c6 t ) distortion P (O s | O t ) LM P w (t) (2)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baseline Phrase-Based SMT System",
                "sec_num": "3"
            },
            {
                "text": "Here, P w (t) is the target language model, P (O s |O t ) represents the conditional (order) linear distortion probability, and P (\u03c6 s |\u03c6 t ) stands for a probabilistic translation model from target language bags of phrases to source language bags of phrases using a phrase translation table. As commonly done in PB-SMT, we interpolate these models log-linearly (using different \u03bb weights) together with a word penalty weight which allows for control over the length of the target sentence t:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baseline Phrase-Based SMT System",
                "sec_num": "3"
            },
            {
                "text": "arg max \u03c6t,Ot P (\u03c6 s | \u03c6 t ) P (O s | O t ) \u03bbo P w (t) \u03bb lm exp |t|\u03bbw",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baseline Phrase-Based SMT System",
                "sec_num": "3"
            },
            {
                "text": "For convenience of notation, the interpolation factor for the bag of phrases translation model is shown in formula (3) at the phrase level (but that does not entail any difference). For a bag of phrases \u03c6 t consisting of phrases t i , and bag \u03c6 s consisting of phrases s i , the phrase translation model is given by:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baseline Phrase-Based SMT System",
                "sec_num": "3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "P (\u03c6s | \u03c6t) = Y s i t i P (si|ti) P (si| ti) = P ph (si|ti) \u03bb t1 Pw(si|ti) \u03bb t2 Pr(ti|si) \u03bb t3",
                        "eq_num": "(3)"
                    }
                ],
                "section": "Baseline Phrase-Based SMT System",
                "sec_num": "3"
            },
            {
                "text": "where P ph and P r are the phrase-translation probability and its reverse probability, and P w is the lexical translation probability.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baseline Phrase-Based SMT System",
                "sec_num": "3"
            },
            {
                "text": "4 Our Approach: Supertagged PBSMT",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baseline Phrase-Based SMT System",
                "sec_num": "3"
            },
            {
                "text": "We extend the baseline model with lexical linguistic representations (supertags) both in the language model as well as in the phrase translation model. Before we describe how our model extends the baseline, we shortly review the supertagging approaches in Lexicalized Tree-Adjoining Grammar and Combinatory Categorial Grammar. Modern linguistic theory proposes that a syntactic parser has access to an extensive lexicon of wordstructure pairs and a small, impoverished set of operations to manipulate and combine the lexical entries into parses. Examples of formal instantiations of this idea include CCG and LTAG. The lexical entries are syntactic constructs (graphs) that specify information such as POS tag, subcategorization/dependency information and other syntactic constraints at the level of agreement features. One important way of portraying such lexical descriptions is via the supertags devised in the LTAG and CCG frameworks (Bangalore & Joshi, 1999; Clark & Curran, 2004) .",
                "cite_spans": [
                    {
                        "start": 951,
                        "end": 963,
                        "text": "Joshi, 1999;",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 964,
                        "end": 985,
                        "text": "Clark & Curran, 2004)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Baseline Phrase-Based SMT System",
                "sec_num": "3"
            },
            {
                "text": "A supertag (see Figure 1 ) represents a complex, linguistic word category that encodes a syntactic structure expressing a specific local behaviour of a word, in terms of the arguments it takes (e.g. subject, object) and the syntactic environment in which it appears. In fact, in LTAG a supertag is an elementary tree and in CCG it is a CCG lexical category. Both descriptions can be viewed as closely related functional descriptions.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 23,
                        "end": 24,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Supertags: Lexical Syntax",
                "sec_num": "4.1"
            },
            {
                "text": "The term \"supertagging\" (Bangalore & Joshi, 1999) refers to tagging the words of a sentence, each with a supertag. When well-formed, an ordered sequence of supertags can be viewed as a compact representation of a small set of constituents/parses that can be obtained by assembling the supertags together using the appropriate combinatory operators (such as substitution and adjunction in LTAG or function application and combination in CCG). Akin to POS tagging, the process of supertagging an input utterance proceeds with statistics that are based on the probability of a word-supertag pair given their Markovian or local context (Bangalore & Joshi, 1999; Clark & Curran, 2004) . This is the main difference with full parsing: supertagging the input utterance need not result in a fully connected graph.",
                "cite_spans": [
                    {
                        "start": 645,
                        "end": 657,
                        "text": "Joshi, 1999;",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 658,
                        "end": 679,
                        "text": "Clark & Curran, 2004)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Supertags: Lexical Syntax",
                "sec_num": "4.1"
            },
            {
                "text": "The LTAG-based supertagger of (Bangalore & Joshi, 1999) is a standard HMM tagger and consists of a (second-order) Markov language model over supertags and a lexical model conditioning the probability of every word on its own supertag (just like standard HMM-based POS taggers).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Supertags: Lexical Syntax",
                "sec_num": "4.1"
            },
            {
                "text": "The CCG supertagger (Clark & Curran, 2004 ) is based on log-linear probabilities that condition a supertag on features representing its context. The CCG supertagger does not constitute a language model nor are the Maximum Entropy estimates directly interpretable as such. In our model we employ the CCG supertagger to obtain the best sequences of supertags for a corpus of sentences from which we obtain language model statistics. Besides the difference in probabilities and statistical estimates, these two supertaggers differ in the way the supertags are extracted from the Penn Treebank, cf. (Hockenmaier, 2003; Chen et al., 2006) . Both supertaggers achieve a supertagging accuracy of 90-92%.",
                "cite_spans": [
                    {
                        "start": 20,
                        "end": 41,
                        "text": "(Clark & Curran, 2004",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 595,
                        "end": 614,
                        "text": "(Hockenmaier, 2003;",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 615,
                        "end": 633,
                        "text": "Chen et al., 2006)",
                        "ref_id": "BIBREF1"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Supertags: Lexical Syntax",
                "sec_num": "4.1"
            },
            {
                "text": "Three aspects make supertags attractive in the context of SMT. Firstly, supertags are rich syntactic constructs that exist for individual words and so they are easy to integrate into SMT models that can be based on any level of granularity, be it wordor phrase-based. Secondly, supertags specify the local syntactic constraints for a word, which resonates well with sequential (finite state) statistical (e.g. Markov) models. Finally, because supertags are rich lexical descriptions that represent underspecification in parsing, it is possible to have some of the benefits of full parsing without imposing the strict connectedness requirements that it demands.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Supertags: Lexical Syntax",
                "sec_num": "4.1"
            },
            {
                "text": "We employ the aforementioned supertaggers to enrich the English side of the parallel training corpus with a single supertag sequence per sentence. Then we extract phrase-pairs together with the cooccuring English supertag sequence from this corpus via the same phrase extraction method used in the baseline model. This way we directly extend the baseline model described in section 3 with supertags both in the phrase translation table and in the language model. Next we define the probabilistic model that accompanies this syntactic enrichment of the baseline model.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Supertag-Based SMT model",
                "sec_num": "4.2"
            },
            {
                "text": "Let ST represent a supertag sequence of the same length as a target sentence t. Equation ( 2 The approximations made in this formula are of two kinds: the standard split into components and the search for the most likely joint probability of a target hypothesis and a supertag sequence cooccuring with the source sentence (a kind of Viterbi approach to avoid the complex optimization involving the sum over supertag sequences). The distortion and word penalty models are the same as those used in the baseline PBSMT model.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A Supertag-Based SMT model",
                "sec_num": "4.2"
            },
            {
                "text": "The 'language model' P ST (t, ST ) is a supertagger assigning probabilities to sequences of word-supertag pairs. The language model is further smoothed by log-linear interpolation with the baseline language model over word sequences.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Supertagged Language Model",
                "sec_num": null
            },
            {
                "text": "The supertagged phrase translation probability consists of a combination of supertagged components analogous to their counterparts in the baseline model (equation ( 3)), i.e. it consists of P (s | t, ST ), its reverse and a word-level probability. We smooth this probability by log-linear interpolation with the factored backoff version P (s | t)P (s | ST ), where we import the baseline phrase table probability and exploit the probability of a source phrase given the target supertag sequence. A model in which we omit P (s | ST ) turns out to be slightly less optimal than this one.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Supertags in Phrase Tables",
                "sec_num": null
            },
            {
                "text": "As in most state-of-the-art PBSMT systems, we use GIZA++ to obtain word-level alignments in both language directions. The bidirectional word alignment is used to obtain lexical phrase translation pairs using heuristics presented in (Och & Ney, 2003) and (Koehn et al., 2003) . Given the collected phrase pairs, we estimate the phrase translation probability distribution by relative frequency as follows:",
                "cite_spans": [
                    {
                        "start": 232,
                        "end": 249,
                        "text": "(Och & Ney, 2003)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 254,
                        "end": 274,
                        "text": "(Koehn et al., 2003)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Supertags in Phrase Tables",
                "sec_num": null
            },
            {
                "text": "Pph (s|t) = count(s, t) s count(s, t)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Supertags in Phrase Tables",
                "sec_num": null
            },
            {
                "text": "For each extracted lexical phrase pair, we extract the corresponding supertagged phrase pairs from the supertagged target sequence in the training corpus (cf. section 5). For each lexical phrase pair, there is at least one corresponding supertagged phrase pair. The probability of the supertagged phrase pair is estimated by relative frequency as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Supertags in Phrase Tables",
                "sec_num": null
            },
            {
                "text": "P st (s|t, st) = count(s, t, st) s count(s, t, st) 4.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Supertags in Phrase Tables",
                "sec_num": null
            },
            {
                "text": "The supertags usually encode dependency information that could be used to construct an 'almost parse' with the help of the CCG/LTAG composition operators. The n-gram language model over supertags applies a kind of statistical 'compositionality check' but due to smoothing effects this could mask crucial violations of the compositionality operators of the grammar formalism (CCG in this case). It is interesting to observe the effect of integrating into the language model a penalty imposed when formal compostion operators are violated. We combine the n-gram language model with a penalty factor that measures the number of encountered combinatory operator violations in a sequence of supertags (cf. Figure 2 ). For a supertag sequence of length (L) which has (V ) operator violations (as measured by the CCG system), the language model P will be adjusted as P * = P \u00d7 (1 -V L ). This is of course no longer a simple smoothed maximum-likelihood estimate nor is it a true probability. Nevertheless, this mechanism provides a simple, efficient integration of a global compositionality (grammaticality) measure into the n-gram language model over supertags.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 708,
                        "end": 709,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "LMs with a Grammaticality Factor",
                "sec_num": "3"
            },
            {
                "text": "Decoder The decoder used in this work is Moses, a log-linear decoder similar to Pharaoh (Koehn, 2004) , modified to accommodate supertag phrase probabilities and supertag language models.",
                "cite_spans": [
                    {
                        "start": 88,
                        "end": 101,
                        "text": "(Koehn, 2004)",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "LMs with a Grammaticality Factor",
                "sec_num": "3"
            },
            {
                "text": "In this section we present a number of experiments that demonstrate the effect of lexical syntax on translation quality. We carried out experiments on the NIST open domain news translation task from Arabic into English. We performed a number of experiments to examine the effect of supertagging approaches (CCG or LTAG) with varying data sizes.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "5"
            },
            {
                "text": "The experiments were conducted for Arabic to English translation and tested on the NIST 2005 evaluation set. The systems were trained on the LDC Arabic-English parallel corpus; we use the news part (130K sentences, about 5 million words) to train systems with what we call the small data set, and the news and a large part of the UN data (2 million sentences, about 50 million words) for experiments with large data sets.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data and Settings",
                "sec_num": null
            },
            {
                "text": "The n-gram target language model was built using 250M words from the English GigaWord Corpus using the SRILM toolkit. 4 Taking 10% of the English GigaWord Corpus used for building our target language model, the supertag-based target language models were built from 25M words that were supertagged. For the LTAG supertags experiments, we used the LTAG English supertagger5 (Bangalore & Joshi, 1999) to tag the English part of the parallel data and the supertag language model data. For the CCG supertag experiments, we used the CCG supertagger of (Clark & Curran, 2004 ) and the Edinburgh CCG tools 6 to tag the English part of the parallel corpus as well as the CCG supertag language model data.",
                "cite_spans": [
                    {
                        "start": 372,
                        "end": 397,
                        "text": "(Bangalore & Joshi, 1999)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 546,
                        "end": 567,
                        "text": "(Clark & Curran, 2004",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data and Settings",
                "sec_num": null
            },
            {
                "text": "The NIST MT03 test set is used for development, particularly for optimizing the interpolation weights using Minimum Error Rate training (Och, 2003) .",
                "cite_spans": [
                    {
                        "start": 136,
                        "end": 147,
                        "text": "(Och, 2003)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data and Settings",
                "sec_num": null
            },
            {
                "text": "Baseline System The baseline system is a stateof-the-art PBSMT system as described in section 3. We built two baseline systems with two different-sized training sets: 'Base-SMALL' (5 million words) and 'Base-LARGE' (50 million words) as described above. Both systems use a trigram language model built using 250 million words from the English GigaWord Corpus. Table 1 presents the BLEU scores (Papineni et al., 2002) The Usefulness of a Supertagged LM In these experiments we study the effect of the two added feature (cost) functions: supertagged translation and language models. We compare the baseline system to the supertags system with the supertag phrasetable probability but without the supertag LM. Table 4 lists the baseline system (Base-SMALL), the LTAG system without supertagged language model (LTAG-TM-ONLY) and the LTAG-SMALL system with both supertagged translation and language models. The results presented in ",
                "cite_spans": [
                    {
                        "start": 393,
                        "end": 416,
                        "text": "(Papineni et al., 2002)",
                        "ref_id": "BIBREF12"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 366,
                        "end": 367,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Data and Settings",
                "sec_num": null
            },
            {
                "text": "Outperforming a PBSMT system on small amounts of training data is less impressive than doing so on really large sets. The issue here is scalability as well as whether the PBSMT system is able to bridge the performance gap with the supertagged system when reasonably large sizes of training data are used. To this end, we trained the systems on 2 million sentences of parallel data, deploying LTAG supertags and CCG supertags. We see that bringing the grammaticality tests to bear onto the supertagged system gives a further improvement of 0.79 BLEU points, a 1.7% relative increase, culminating in an overall increase of 2.7 BLEU points, or a 6.1% relative improvement over the baseline system.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Scalability: Larger Training Corpora",
                "sec_num": "5.2"
            },
            {
                "text": "A natural question to ask is whether LTAG and CCG supertags are playing similar (overlapping, or con- 2 ). However, our efforts to build a system that benefits from the combination using a simple loglinear combination of the two models did not give any significant performance change relative to the baseline CCG system. Obviously, more informed ways of combining the two could result in better performance than a simple log-linear interpolation of the components. Figure 3 shows some example system output. While the baseline system omits the verb giving \"the authorities that it had...\", both the LTAG and CCG found a formulation \"authorities reported that\" with a closer meaning to the reference translation \"The authorities said that\". Omitting verbs turns out to be a problem for the baseline system when translating the notorious verbless Arabic sentences (see Figure 4 ). The supertagged systems have a more grammatically strict language model than a standard word-level Markov model, thereby exhibiting a preference (in the CCG system especially) for the insertion of a verb with a similar meaning to that contained in the reference sentence.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 102,
                        "end": 103,
                        "text": "2",
                        "ref_id": null
                    },
                    {
                        "start": 472,
                        "end": 473,
                        "text": "3",
                        "ref_id": null
                    },
                    {
                        "start": 874,
                        "end": 875,
                        "text": "4",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Discussion",
                "sec_num": "5.3"
            },
            {
                "text": "SMT practitioners have on the whole found it difficult to integrate syntax into their systems. In this work, we have presented a novel model of PBSMT which integrates supertags into the target language model and the target side of the translation model.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions",
                "sec_num": "6"
            },
            {
                "text": "Using LTAG supertags gives the best improvement over a state-of-the-art PBSMT system for a smaller data set, while CCG supertags work best on a large 2 million-sentence pair training set. Adding grammaticality factors based on algebraic compositional operators gives the best result, namely 0.4688 BLEU, or a 6.1% relative increase over the baseline. This result compares favourably with the best systems on the NIST 2005 Arabic-English task. We expect more work on system integration to improve results still further, and anticipate that similar increases are to be seen for other language pairs.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions",
                "sec_num": "6"
            },
            {
                "text": "These operators neither carry nor presuppose further linguistic knowledge beyond what the lexicon contains.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "http://www.fjoch.com/GIZA++.html",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "http://www.statmt.org/moses/",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "http://www.speech.sri.com/projects/srilm/",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "http://www.cis.upenn.edu/\u02dcxtag/gramrelease.html",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "We would like to thank Srinivas Bangalore and the anonymous reviewers for useful comments on earlier versions of this paper. This work is partially funded by Science Foundation Ireland Principal Investigator Award 05/IN/1732, and Netherlands Organization for Scientific Research (NWO) VIDI Award.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgements",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "The authorities said he was allowed to contact family members by phone from the armored vehicle he was in. Baseline: the authorities that it had allowed him to communicate by phone with his family of the armored car where LTAG: authorities reported that it had allowed him to contact by telephone with his family of armored car where CCG: authorities reported that it had enabled him to communicate by phone his family members of the armored car where",
                "authors": [
                    {
                        "first": "S",
                        "middle": [],
                        "last": "References",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Bangalore",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Joshi",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "Computational Linguistics",
                "volume": "25",
                "issue": "2",
                "pages": "237--265",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Reference: The authorities said he was allowed to contact family members by phone from the armored vehicle he was in. Baseline: the authorities that it had allowed him to communicate by phone with his family of the armored car where LTAG: authorities reported that it had allowed him to contact by telephone with his family of armored car where CCG: authorities reported that it had enabled him to communicate by phone his family members of the armored car where References S. Bangalore and A. Joshi, \"Supertagging: An Ap- proach to Almost Parsing\", Computational Linguistics 25(2):237-265, 1999.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Automated extraction of tree-adjoining grammars from treebanks",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Bangalore",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Vijay-Shanker",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Natural Language Engineering",
                "volume": "12",
                "issue": "3",
                "pages": "251--299",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "J. Chen, S. Bangalore, and K. Vijay-Shanker, \"Au- tomated extraction of tree-adjoining grammars from treebanks\". Natural Language Engineering, 12(3):251-299, 2006.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "A Hierarchical Phrase-Based Model for Statistical Machine Translation",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Chiang",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "Proceedings of ACL 2005",
                "volume": "",
                "issue": "",
                "pages": "263--270",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. Chiang, \"A Hierarchical Phrase-Based Model for Sta- tistical Machine Translation\", in Proceedings of ACL 2005, Ann Arbor, MI., pp.263-270, 2005.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "The Importance of Supertagging for Wide-Coverage CCG Parsing",
                "authors": [
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Curran",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of COLING-04",
                "volume": "",
                "issue": "",
                "pages": "282--288",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "S. Clark and J. Curran, \"The Importance of Supertagging for Wide-Coverage CCG Parsing\", in Proceedings of COLING-04, Geneva, Switzerland, pp.282-288, 2004.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Data and Models for Statistical Parsing with Combinatory Categorial Grammar",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Hockenmaier",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "J. Hockenmaier, Data and Models for Statistical Parsing with Combinatory Categorial Grammar, PhD thesis, University of Edinburgh, UK, 2003.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Tree Adjoining Grammars and Lexicalized Grammars",
                "authors": [
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Joshi",
                        "suffix": ""
                    },
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Schabes",
                        "suffix": ""
                    }
                ],
                "year": 1992,
                "venue": "Tree Automata and Languages",
                "volume": "",
                "issue": "",
                "pages": "409--431",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "A. Joshi and Y. Schabes, \"Tree Adjoining Grammars and Lexicalized Grammars\" in M. Nivat and A. Podelski (eds.) Tree Automata and Languages, Amsterdam, The Netherlands: North-Holland, pp.409-431, 1992.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Pharaoh: A Beam Search Decoder for phrasebased Statistical Machine Translation Models",
                "authors": [
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Koehn",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of AMTA-04",
                "volume": "",
                "issue": "",
                "pages": "115--124",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "P. Koehn, \"Pharaoh: A Beam Search Decoder for phrase- based Statistical Machine Translation Models\", in Pro- ceedings of AMTA-04, Berlin/Heidelberg, Germany: Springer Verlag, pp.115-124, 2004.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Statistical Phrase-Based Translation",
                "authors": [
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Koehn",
                        "suffix": ""
                    },
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Och",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Marcu",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Proceedings of HLT-NAACL 2003",
                "volume": "",
                "issue": "",
                "pages": "127--133",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "P. Koehn, F. Och, and D. Marcu, \"Statistical Phrase- Based Translation\", in Proceedings of HLT-NAACL 2003, Edmonton, Canada, pp.127-133, 2003.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "SPMT: Statistical Machine Translation with Syntactified Target Language Phrases",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Marcu",
                        "suffix": ""
                    },
                    {
                        "first": "W",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Echihabi",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Knight",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proceedings of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "44--52",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. Marcu, W. Wang, A. Echihabi and K. Knight, \"SPMT: Statistical Machine Translation with Syntactified Tar- get Language Phrases\", in Proceedings of EMNLP, Sydney, Australia, pp.44-52, 2006.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "A Phrase-Based, Joint Probability Model for Statistical Machine Translation",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Marcu",
                        "suffix": ""
                    },
                    {
                        "first": "W",
                        "middle": [],
                        "last": "Wong",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "133--139",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. Marcu and W. Wong, \"A Phrase-Based, Joint Probabil- ity Model for Statistical Machine Translation\", in Pro- ceedings of EMNLP, Philadelphia, PA., pp.133-139, 2002.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Minimum Error Rate Training in Statistical Machine Translation",
                "authors": [
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Och",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Proceedings of ACL 2003, Sapporo",
                "volume": "",
                "issue": "",
                "pages": "160--167",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "F. Och, \"Minimum Error Rate Training in Statistical Ma- chine Translation\", in Proceedings of ACL 2003, Sap- poro, Japan, pp.160-167, 2003.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "A Systematic Comparison of Various Statistical Alignment Models",
                "authors": [
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Och",
                        "suffix": ""
                    },
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Ney",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Computational Linguistics",
                "volume": "29",
                "issue": "",
                "pages": "19--51",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "F. Och and H. Ney, \"A Systematic Comparison of Var- ious Statistical Alignment Models\", Computational Linguistics 29:19-51, 2003.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "BLEU: A Method for Automatic Evaluation of Machine Translation",
                "authors": [
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Papineni",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Roukos",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Ward",
                        "suffix": ""
                    },
                    {
                        "first": "W-J",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Proceedings of ACL 2002, Philadelphia",
                "volume": "",
                "issue": "",
                "pages": "311--318",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "K. Papineni, S. Roukos, T. Ward and W-J. Zhu, \"BLEU: A Method for Automatic Evaluation of Machine Translation\", in Proceedings of ACL 2002, Philadel- phia, PA., pp.311-318, 2002.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition",
                "authors": [
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Rabiner",
                        "suffix": ""
                    }
                ],
                "year": 1990,
                "venue": "Readings in Speech Recognition",
                "volume": "",
                "issue": "",
                "pages": "267--296",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "L. Rabiner, \"A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition\", in A. Waibel & F-K. Lee (eds.) Readings in Speech Recog- nition, San Mateo, CA.: Morgan Kaufmann, pp.267- 296, 1990.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "The Syntactic Process",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Steedman",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M. Steedman, The Syntactic Process. Cambridge, MA: The MIT Press, 2000.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "A Phrase-based Unigram Model for Statistical Machine Translation",
                "authors": [
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Tillmann",
                        "suffix": ""
                    },
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Xia",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Proceedings of HLT-NAACL 2003",
                "volume": "",
                "issue": "",
                "pages": "106--108",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "C. Tillmann and F. Xia, \"A Phrase-based Unigram Model for Statistical Machine Translation\", in Proceedings of HLT-NAACL 2003, Edmonton, Canada. pp.106-108, 2003.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure1: An LTAG supertag sequence for the sentence The purchase price includes taxes. The subcategorization information is most clearly available in the verb includes which takes a subject NP to its left and an object NP to its right.",
                "uris": null,
                "fig_num": "1",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 2: Example CCG operator violations: V = 2 and L = 3, and so the penalty factor is 1/3.",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF2": {
                "num": null,
                "text": "Figure 4: Verbless Arabic sentence and sample output from different systems",
                "uris": null,
                "fig_num": "4",
                "type_str": "figure"
            },
            "TABREF1": {
                "content": "<table><tr><td/><td/><td colspan=\"2\">baseline (4.1% relative). These significant improve-</td></tr><tr><td/><td/><td colspan=\"2\">ments indicate that the rich information in supertags</td></tr><tr><td/><td/><td colspan=\"2\">helps select better translation candidates.</td></tr><tr><td/><td/><td colspan=\"2\">POS Tags vs. Supertags A supertag is a complex</td></tr><tr><td/><td/><td colspan=\"2\">tag that localizes the dependency and the syntax in-</td></tr><tr><td/><td/><td colspan=\"2\">formation from the context, whereas a normal POS</td></tr><tr><td/><td/><td colspan=\"2\">tag just describes the general syntactic category of</td></tr><tr><td/><td/><td colspan=\"2\">the word without further constraints. In this experi-</td></tr><tr><td/><td/><td colspan=\"2\">ment we compared the effect of using supertags and</td></tr><tr><td/><td/><td colspan=\"2\">POS tags on translation quality. As can be seen</td></tr><tr><td/><td/><td>System</td><td>BLEU Score</td></tr><tr><td/><td/><td>Base-SMALL</td><td>0.4008</td></tr><tr><td/><td/><td>POS-SMALL</td><td>0.4073</td></tr><tr><td/><td/><td>LTAG-SMALL</td><td>.0.4205</td></tr><tr><td/><td/><td colspan=\"2\">Table 3: Comparing the effect of supertags and POS</td></tr><tr><td/><td/><td>tags</td></tr><tr><td/><td/><td colspan=\"2\">in Table 3, while the POS tags help (0.65 BLEU</td></tr><tr><td/><td/><td colspan=\"2\">points, or 1.7% relative increase over the baseline),</td></tr><tr><td>System</td><td>BLEU Score</td><td colspan=\"2\">they clearly underperform compared to the supertag</td></tr><tr><td>Base-SMALL</td><td>0.4008</td><td>model (by 3.2%).</td></tr><tr><td>Base-LARGE</td><td>0.4418</td><td/></tr><tr><td colspan=\"2\">Table 1: Baseline systems' BLEU scores</td><td/></tr><tr><td colspan=\"2\">5.1 Baseline vs. Supertags on Small Data Sets</td><td/></tr><tr><td colspan=\"2\">We compared the translation quality of the baseline</td><td/></tr><tr><td colspan=\"2\">systems with the LTAG and CCG supertags systems</td><td/></tr><tr><td colspan=\"2\">(LTAG-SMALL and CCG-SMALL). The results are</td><td/></tr><tr><td>System</td><td>BLEU Score</td><td/></tr><tr><td>Base-SMALL</td><td>0.4008</td><td/></tr><tr><td>LTAG-SMALL</td><td>0.4205</td><td/></tr><tr><td>CCG-SMALL</td><td>0.4174</td><td/></tr><tr><td colspan=\"2\">Table 2: LTAG and CCG systems on small data</td><td/></tr><tr><td colspan=\"2\">given in Table 2. All systems were trained on the</td><td/></tr><tr><td colspan=\"2\">same parallel data. The LTAG supertag-based sys-</td><td/></tr><tr><td colspan=\"2\">tem outperforms the baseline by 1.97 BLEU points</td><td/></tr><tr><td colspan=\"2\">absolute (or 4.9% relative), while the CCG supertag-</td><td/></tr><tr><td colspan=\"2\">based system scores 1.66 BLEU points over the</td><td/></tr><tr><td colspan=\"2\">6 http://groups.inf.ed.ac.uk/ccg/software.html</td><td/></tr></table>",
                "type_str": "table",
                "text": "of both systems on the NIST 2005 MT Evaluation test set.",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "content": "<table><tr><td>indi-</td></tr></table>",
                "type_str": "table",
                "text": "",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "content": "<table><tr><td/><td>presents the compari-</td></tr><tr><td colspan=\"2\">son between these systems and the baseline trained</td></tr><tr><td colspan=\"2\">on the same data. The LTAG system improves by</td></tr><tr><td colspan=\"2\">1.17 BLEU points (2.6% relative), but the CCG sys-</td></tr><tr><td colspan=\"2\">tem gives an even larger increase: 1.91 BLEU points</td></tr><tr><td colspan=\"2\">(4.3% relative). While this is slightly lower than</td></tr><tr><td colspan=\"2\">the 4.9% relative improvement with the smaller data</td></tr><tr><td colspan=\"2\">sets, the sustained increase is probably due to ob-</td></tr><tr><td colspan=\"2\">serving more data with different supertag contexts,</td></tr><tr><td colspan=\"2\">which enables the model to select better target lan-</td></tr><tr><td>guage phrases.</td><td/></tr><tr><td>System</td><td>BLEU Score</td></tr><tr><td>Base-LARGE</td><td>0.4418</td></tr><tr><td>LTAG-LARGE</td><td>0.4535</td></tr><tr><td>CCG-LARGE</td><td>0.4609</td></tr></table>",
                "type_str": "table",
                "text": "",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "content": "<table><tr><td>Adding a grammaticality factor As described in</td></tr><tr><td>section 4.3, we integrate an impoverished grammat-</td></tr><tr><td>icality factor based on two standard CCG combi-</td></tr><tr><td>nation operations, namely Forward and Backward</td></tr><tr><td>Application. Table 6 compares the results of the</td></tr><tr><td>baseline, the CCG with an n-gram LM-only system</td></tr><tr><td>(CCG-LARGE) and CCG-LARGE with this 'gram-</td></tr><tr><td>maticalized' LM system (CCG-LARGE-GRAM).</td></tr></table>",
                "type_str": "table",
                "text": "The effect of more training data",
                "html": null,
                "num": null
            },
            "TABREF5": {
                "content": "<table><tr><td>System</td><td>BLEU Score</td></tr><tr><td>Base-LARGE</td><td>0.4418</td></tr><tr><td>CCG-LARGE</td><td>0.4609</td></tr><tr><td>CCG-LARGE-GRAM</td><td>0.4688</td></tr></table>",
                "type_str": "table",
                "text": "Comparing the effect of CCG-GRAM flicting) roles in practice. Using an oracle to choose the best output of the two systems gives a BLEU score of 0.441, indicating that the combination provides significant room for improvement (cf. Table",
                "html": null,
                "num": null
            }
        }
    }
}