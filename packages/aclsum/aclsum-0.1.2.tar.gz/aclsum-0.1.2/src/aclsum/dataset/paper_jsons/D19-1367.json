{
    "paper_id": "D19-1367",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2023-06-16T11:56:21.271922Z"
    },
    "title": "Improved Differentiable Architecture Search for Language Modeling and Named Entity Recognition",
    "authors": [
        {
            "first": "Yufan",
            "middle": [],
            "last": "Jiang",
            "suffix": "",
            "affiliation": {
                "laboratory": "NLP Lab",
                "institution": "Northeastern University",
                "location": {
                    "settlement": "Shenyang",
                    "country": "China"
                }
            },
            "email": "jiangyufan2018@outlook.com"
        },
        {
            "first": "Chi",
            "middle": [],
            "last": "Hu",
            "suffix": "",
            "affiliation": {
                "laboratory": "NLP Lab",
                "institution": "Northeastern University",
                "location": {
                    "settlement": "Shenyang",
                    "country": "China"
                }
            },
            "email": ""
        },
        {
            "first": "Tong",
            "middle": [],
            "last": "Xiao",
            "suffix": "",
            "affiliation": {},
            "email": "xiaotong@mail.neu.edu.cn"
        },
        {
            "first": "Chunliang",
            "middle": [],
            "last": "Zhang",
            "suffix": "",
            "affiliation": {},
            "email": "zhangchunliang@mail.neu.edu.cn"
        },
        {
            "first": "Jingbo",
            "middle": [],
            "last": "Zhu",
            "suffix": "",
            "affiliation": {},
            "email": "zhujingbo@mail.neu.edu.cn"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "In this paper, we study differentiable neural architecture search (NAS) methods for natural language processing. In particular, we improve differentiable architecture search by removing the softmax-local constraint. Also, we apply differentiable NAS to named entity recognition (NER). It is the first time that differentiable NAS methods are adopted in NLP tasks other than language modeling. On both the PTB language modeling and CoNLL-2003 English NER data, our method outperforms strong baselines. It achieves a new state-ofthe-art on the NER task.",
    "pdf_parse": {
        "paper_id": "D19-1367",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "In this paper, we study differentiable neural architecture search (NAS) methods for natural language processing. In particular, we improve differentiable architecture search by removing the softmax-local constraint. Also, we apply differentiable NAS to named entity recognition (NER). It is the first time that differentiable NAS methods are adopted in NLP tasks other than language modeling. On both the PTB language modeling and CoNLL-2003 English NER data, our method outperforms strong baselines. It achieves a new state-ofthe-art on the NER task.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Neural architecture search (NAS) has become popular recently in machine learning for their ability to find new models and to free researchers from the hard work of designing network architectures. The earliest of these approaches use reinforcement learning (RL) to learn promising architectures in a discrete space (Zoph and Le, 2016) , whereas others have successfully modeled the problem in a continuous manner (Liu et al., 2019; Xie et al., 2019b; Huang and Xiang, 2019) . As an instance of the latter, differentiable architecture search (DARTS) employs continuous relaxation to architecture representation and makes gradient descent straightforwardly applicable to search. This leads to an efficient search process that is orders of magnitude faster than the RL-based counterparts.",
                "cite_spans": [
                    {
                        "start": 315,
                        "end": 334,
                        "text": "(Zoph and Le, 2016)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 413,
                        "end": 431,
                        "text": "(Liu et al., 2019;",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 432,
                        "end": 450,
                        "text": "Xie et al., 2019b;",
                        "ref_id": null
                    },
                    {
                        "start": 451,
                        "end": 473,
                        "text": "Huang and Xiang, 2019)",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Like recent methods in NAS (Xie and Yuille, 2017; Zoph and Le, 2016; Baker et al., 2016) , DARTS represents networks as a directed acyclic graph for a given computation cell (see Figure 1(a) ). An edge between nodes performs a predefined operation to transform the input (i.e., tail) Figure 1 : An overview of DARTS cell and our cell to the output (i.e., head). For a continuous network space, DARTS uses the softmax trick to relax the categorical choice of edges to soft decisions. Then, one can optimize over the graph using standard gradient descent methods. The optimized network is inferred by choosing the edges with maximum weights in softmax.",
                "cite_spans": [
                    {
                        "start": 27,
                        "end": 49,
                        "text": "(Xie and Yuille, 2017;",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 50,
                        "end": 68,
                        "text": "Zoph and Le, 2016;",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 69,
                        "end": 88,
                        "text": "Baker et al., 2016)",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 186,
                        "end": 190,
                        "text": "1(a)",
                        "ref_id": null
                    },
                    {
                        "start": 291,
                        "end": 292,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "However, DARTS is a \"local\" model because the softmax-based relaxation is imposed on each bundle of edges between two nodes. This leads to a biased model in that edges coming from different nodes are not comparable. Such a constraint limits the inference space to sub-graphs with one edge between each pair of nodes. Also, the learned network might be redundant because every node has to receive edges from all predecessors no matter they are necessary or not. This problem is similar to the bias problem in other graph-based models where local decisions make the model nonoptimal (Lafferty et al., 2001; Daphne Koller and Nir Friedman, 2009) .",
                "cite_spans": [
                    {
                        "start": 581,
                        "end": 604,
                        "text": "(Lafferty et al., 2001;",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 605,
                        "end": 642,
                        "text": "Daphne Koller and Nir Friedman, 2009)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Here we present an improvement of DARTS, called I-DARTS, that further relaxes the softmaxlocal constraint. The idea is simple -we consider all incoming edges to a given node in a single softmax. This offers a broader choice of edges and enlarges the space we infer the network from. For example, one can simultaneously select multiple important edges between two nodes and leave some node pairs unlinked (see Figure 1(b) ). I-DARTS outperforms strong baselines on the PTB language modeling and CoNLL named entity recognition (NER) tasks. This gives a new stateof-the-art on the NER dataset. To our knowledge, it is the first time to apply differentiable architecture search methods to NLP tasks other than language modeling. More interestingly, we observe that our method is 1.4X faster than DARTS for convergence of architecture search. Also, we provide the architectures learned by I-DARTS, which can be referred for related tasks.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 416,
                        "end": 420,
                        "text": "1(b)",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Although we will restrict ourselves to language modeling and NER for experiments, in the section, we discuss the more general case. We choose recurrent neural networks (RNNs) to model the sequence generation and tagging problems. Given a sequence of input vectors {x 1 , ..., x L }, we repeat applying RNN cells to generate the output {h 1 , ..., h L }. The RNN cell is defined as: h t = g(x t , h t-1 ), where t is the time step and g(\u2022, \u2022) is the function of the cell. In NAS, the objective is to search for a good g(\u2022, \u2022) in an automatic fashion.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "The Method",
                "sec_num": "2"
            },
            {
                "text": "We follow the assumption that g(\u2022, \u2022) is a DAG consisting of N nodes and edges among them (Liu et al., 2019; Xie et al., 2019b; Pham et al., 2018) . An edge o i,j (\u2022) between node pair (i, j) indicates an activation function from node j to node i. For node i, it simply sums over vectors from all predecessor nodes (j < i), followed by a linear transformation with a parameter matrix W i . More formally, let s i be the state of node i. We define s i to be:",
                "cite_spans": [
                    {
                        "start": 90,
                        "end": 108,
                        "text": "(Liu et al., 2019;",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 109,
                        "end": 127,
                        "text": "Xie et al., 2019b;",
                        "ref_id": null
                    },
                    {
                        "start": 128,
                        "end": 146,
                        "text": "Pham et al., 2018)",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Architecture Search Space",
                "sec_num": "2.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "s i = j<i o i,j (s j \u2022 W j )",
                        "eq_num": "(1)"
                    }
                ],
                "section": "Architecture Search Space",
                "sec_num": "2.1"
            },
            {
                "text": "See Figure 1 for an example network (red lines). Note that this model can encode an exponential number of graphs by choosing different sub-sets of edges (i.e., choosing o i,j (\u2022) for each (i,j)). The output of search is the optimal edge selection and the corresponding network.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 11,
                        "end": 12,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Architecture Search Space",
                "sec_num": "2.1"
            },
            {
                "text": "Given a set of edges {o i,j k }, one can try each o i,j k to induce a network, and then train and evaluate it. The optimal choice is the edge with highest accuracy on the validation set. In I-DARTS, we instead do this in a soft way. We re-define s i as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Improved DARTS",
                "sec_num": "2.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "s i = j<i k \u03b1 i,j k \u2022 o i,j k (s j \u2022 W j )",
                        "eq_num": "(2)"
                    }
                ],
                "section": "Improved DARTS",
                "sec_num": "2.2"
            },
            {
                "text": "where \u03b1 i,j k is the weight indicating the importance of o i,j k (\u2022). It is computed by the softmax normalization over edges between nodes i and j, like this",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Improved DARTS",
                "sec_num": "2.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\u03b1 i,j k = exp(w i,j k ) k exp(w i,j k )",
                        "eq_num": "(3)"
                    }
                ],
                "section": "Improved DARTS",
                "sec_num": "2.2"
            },
            {
                "text": "where w i,j k is the model parameter. This model reduces the architecture search problem to learn continuous variables {\u03b1 i,j k }, which can be implemented using efficient gradient descent methods. After training, the final architecture is encoded by the edges with largest weights.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Improved DARTS",
                "sec_num": "2.2"
            },
            {
                "text": "Eq. ( 3) imposes a constraint that weights {\u03b1 i,j k } are normalized for each j. Such a model in general faces the local decision and bias problems as pointed out in graph-based methods (Lafferty et al., 2001; Daphne Koller and Nir Friedman, 2009) . Moreover, the inference has to be performed in a smaller space because we have to infer exactly one edge between each node pair and exclude networks violating this constraint.",
                "cite_spans": [
                    {
                        "start": 186,
                        "end": 209,
                        "text": "(Lafferty et al., 2001;",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 210,
                        "end": 247,
                        "text": "Daphne Koller and Nir Friedman, 2009)",
                        "ref_id": "BIBREF3"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Improved DARTS",
                "sec_num": "2.2"
            },
            {
                "text": "Here we remove the constraint and system bias. To this end, we compute the softmax normalization over all incoming edges for node i:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Improved DARTS",
                "sec_num": "2.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\u03b1 i,j k = exp(w i,j k ) j<i k exp(w i,j k )",
                        "eq_num": "(4)"
                    }
                ],
                "section": "Improved DARTS",
                "sec_num": "2.2"
            },
            {
                "text": "It provides us a way to compare all incoming edges in the same manner, rather than making a local decision via a bundle of edges from node j.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Improved DARTS",
                "sec_num": "2.2"
            },
            {
                "text": "As another bonus, this method can search for networks that are not covered by DARTS, e.g., networks that contain two edges between the same node pair.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Improved DARTS",
                "sec_num": "2.2"
            },
            {
                "text": "See Figure 1 (b) for an illustration of our method. To infer the optimal architecture, we basically do the same thing as in DARTS. The differ-ence lies in that we select top-n edges with respect to \u03b1 i,j k . Here n is a hyper-parameter that controls the density of the network. E.g., n = 1 means a sparse net, and n = \u221e means a very dense net involving all those edges.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 11,
                        "end": 12,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Improved DARTS",
                "sec_num": "2.2"
            },
            {
                "text": "We test our method on language modeling and named entity recognition tasks. Our experiments consist of two parts: recurrent neural architecture search and architecture evaluation. In architecture search, we search for good RNN cell architectures. Then, we train and evaluate the learned architecture.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiments",
                "sec_num": "3"
            },
            {
                "text": "For language modeling, we run neural search on the PTB corpus. We use the standard preprocessed version of the dataset (Pham et al., 2018) . To make it comparable with previous work, we copy the setup used in (Pham et al., 2018; Liu et al., 2019) . The recurrent cell consist of 8 nodes. The candidate operation set of every edge contain 5 activation functions, including zeroize, tanh, relu, sigmoid, and identity. To learn architectures, we run the search system for 40 training epochs with a batch size of 256. We optimize models parameters {W i } using SGD with a learning rate of 20 and a weight decay rate of 5e-7, and optimized softmax relaxation parameters {w i,j k } by Adam with a learning rate of 3e-3 and a weight decay rate of 1e-3. For RNN models, we use a singlelayer recurrent network with embedding and hidden sizes = 300. It takes us 4 hours to learn the architecture on a single GPU of NVIDIA 1080Ti.",
                "cite_spans": [
                    {
                        "start": 119,
                        "end": 138,
                        "text": "(Pham et al., 2018)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 209,
                        "end": 228,
                        "text": "(Pham et al., 2018;",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 229,
                        "end": 246,
                        "text": "Liu et al., 2019)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Architecture Search",
                "sec_num": "3.1"
            },
            {
                "text": "For named entity recognition, we choose the CONLL-2003 English dataset. We follow the same setup as in language modeling but with a different learning rate (0.1) and a different hidden layer size (256). It takes us 4 hours to learn the architecture on the same GPU.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Architecture Search",
                "sec_num": "3.1"
            },
            {
                "text": "Firstly, the discovered architecture is evaluated on the language modeling task. Before that, we train it on the same data used in architecture search. The size of hidden layers is set to 850. We use averaged SGD to train the model for 3,000 epochs, with a learning rate of 20 and a weight decay rate of 8e-7. For a fair comparison, we do not fine-tune the model at the end of the training. Table 1 shows the perplexities of different RN-N models on PTB. We also report the results of previous systems. The model discovered by I-DARTS achieves a validation perplexity of 58.0 and a test perplexity of 56.0 when n = 1. It is on par with the state-of-the-art models that are designed either manually or automatically. However, we find that the model failed to optimize when n = 2. It might result from the complex interaction between operations. We leave this issue for future study.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 397,
                        "end": 398,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Architecture Evaluation",
                "sec_num": "3.2"
            },
            {
                "text": "Since architecture search is initializationsensitive (Pham et al., 2018; Liu et al., 2019) , we search the architectures for 4 times with different random seeds. We evaluate the architecture every 10 search epochs by retraining it on PTB for 500 epochs. We compare DARTS with our I-DARTS method with the same random seed. See Figure 2 (b) for averaged validation perplexities over 4 different runs at different search epochs. We see that I-DARTS is easier to converge than DARTS (4 hours). It is 1.4X faster than that of DARTS. Another interesting finding is that I-DARTS achieves a lower validation perplexity than DARTS during architecture search. This may indicate better architectures found by I-DARTS because the search model is optimized with respect to validation perplexity.",
                "cite_spans": [
                    {
                        "start": 53,
                        "end": 72,
                        "text": "(Pham et al., 2018;",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 73,
                        "end": 90,
                        "text": "Liu et al., 2019)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 333,
                        "end": 334,
                        "text": "2",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Architecture Evaluation",
                "sec_num": "3.2"
            },
            {
                "text": "Then, we test the learned architecture in a named entity recognition system on the English data from CoNLL-2003 shared task (Sang and Meulder, 2003) . Following previous work (Akbik et al., 2018; Peters et al., 2017) , we report the averaged F1 score over 5 runs on the test set. For modeling, we choose the single-layer RNN-CRF model because it achieved state-of-the-art results on several sequence labeling tasks (Lample et al., 2016; Ma and Hovy, 2016) . We use GloVe 100dimensional word embeddings (Pennington et al., 2014) and pooled contextual embeddings (Akbik et al., 2019) as pre-trained word embeddings. We replace the standard bidirectional LSTMs with the discovered recurrent neural cells. Also, we set the hidden layer size to 512 and apply variational dropout to the input and output of the RNN layer. We train the network using SGD with a learning rate of 0.1 and a gradient clipping threshold of 5.0. We reduce the learning rate by a factor of 0.25 if the test error does not decrease for 2 epochs.",
                "cite_spans": [
                    {
                        "start": 101,
                        "end": 133,
                        "text": "CoNLL-2003 shared task (Sang and",
                        "ref_id": null
                    },
                    {
                        "start": 134,
                        "end": 148,
                        "text": "Meulder, 2003)",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 175,
                        "end": 195,
                        "text": "(Akbik et al., 2018;",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 196,
                        "end": 216,
                        "text": "Peters et al., 2017)",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 415,
                        "end": 436,
                        "text": "(Lample et al., 2016;",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 437,
                        "end": 455,
                        "text": "Ma and Hovy, 2016)",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 502,
                        "end": 527,
                        "text": "(Pennington et al., 2014)",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 561,
                        "end": 581,
                        "text": "(Akbik et al., 2019)",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Architecture Evaluation",
                "sec_num": "3.2"
            },
            {
                "text": "Table 2 shows a comparison of different methods. Our baseline uses RNN cells generated from random initialized whose F1-score varies greatly and is lower than that of the standard LSTMs. I-DARTS significantly outperforms Random RNNs and DARTS. The best score is achieved when n = 1. It indicates that the task prefers a sparse network. Also, we see that our model works with the advanced pre-trained language models in that we replace the LSTM cell to our cell. The I-DARTS architecture yields a new RNN-based state-of-theart on this task (93.47 F1-score). In Table 2 , We find it interesting that Random RNNs are good for NER task. This may result from the design of search space that fit for such tasks substantially. Search space is also a key factor in neural architecture search that new efforts should focus on (Xie et al., 2019a) .",
                "cite_spans": [
                    {
                        "start": 817,
                        "end": 836,
                        "text": "(Xie et al., 2019a)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "2",
                        "ref_id": "TABREF1"
                    },
                    {
                        "start": 566,
                        "end": 567,
                        "text": "2",
                        "ref_id": "TABREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Architecture Evaluation",
                "sec_num": "3.2"
            },
            {
                "text": "We visualize the discovered cells in Figure 3 . Each cell is a directed acyclic graph consisting of an ordered sequence of 8 nodes with an activation function applied on each edge. These automatically discovered cells are complex and hard to be designed manually. An interesting phenomenon comes up that the best architecture on language modeling is different from that on name entity recognition. This might result from the fact that different tasks have different inductive bias. Also, this suggests the possibility of architecture selection from the top-k search results on the target task.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 44,
                        "end": 45,
                        "text": "3",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Architecture Evaluation",
                "sec_num": "3.2"
            },
            {
                "text": "Neural architecture search has been proposed to automatically search for better architectures, showing competitive results on several tasks, e.g., image recognition and language modeling. A s- Model F1 best published BiLSTM-CRF (Lample et al., 2016) 90.94 BiLSTM-CRF+ELMo (Peters et al., 2018) 92.22 BERT Base (Devlin et al., 2018) 92.40 BERT Large (Devlin et al., 2018) 92.80 BiLSTM-CRF+PCE (Akbik et al., 2019) 93 trand of NAS research focuses on reinforcement learning (Zoph and Le, 2016) and evolutionary algorithm-based (Xie and Yuille, 2017) methods. They are powerful but inefficient. Recent approaches speed up the search process by weight sharing (Pham et al., 2018) and differentiable architecture search (Liu et al., 2019) . But there is no discussion on the softmax-local problem in previous work. Moreover, previous methods are often tested on language modeling. It is rare to see studies on these methods for other NLP tasks. ",
                "cite_spans": [
                    {
                        "start": 228,
                        "end": 249,
                        "text": "(Lample et al., 2016)",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 272,
                        "end": 293,
                        "text": "(Peters et al., 2018)",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 310,
                        "end": 331,
                        "text": "(Devlin et al., 2018)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 349,
                        "end": 370,
                        "text": "(Devlin et al., 2018)",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 392,
                        "end": 412,
                        "text": "(Akbik et al., 2019)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 472,
                        "end": 491,
                        "text": "(Zoph and Le, 2016)",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 656,
                        "end": 675,
                        "text": "(Pham et al., 2018)",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 715,
                        "end": 733,
                        "text": "(Liu et al., 2019)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related Work",
                "sec_num": "4"
            },
            {
                "text": "We improved the DARTS to address the bias problem by removing the softmax-local constraint. Our method is search efficient and discovers several better architectures for PTB language modeling and CoNLL named entity recognition (NER) tasks. We plan to consider the network density problem in search and apply I-DARTS to more tasks in our future study.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusions",
                "sec_num": "5"
            }
        ],
        "back_matter": [
            {
                "text": "This work was supported in part by the National Science Foundation of China (Nos. 61876035, 61732005 and 61432013), the National Key R&D Program of China (No. 2019QY1801) and the Opening Project of Beijing Key Laboratory of Internet Culture and Digital Dissemination Research. We also thank the reviewers for their insightful comments.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Acknowledgments",
                "sec_num": "6"
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Pooled contextualized embeddings for named entity recognition",
                "authors": [
                    {
                        "first": "Alan",
                        "middle": [],
                        "last": "Akbik",
                        "suffix": ""
                    },
                    {
                        "first": "Tanja",
                        "middle": [],
                        "last": "Bergmann",
                        "suffix": ""
                    },
                    {
                        "first": "Roland",
                        "middle": [],
                        "last": "Vollgraf",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "NAACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alan Akbik, Tanja Bergmann, and Roland Vollgraf. 2019. Pooled contextualized embeddings for named entity recognition. In NAACL.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Contextual string embeddings for sequence labeling",
                "authors": [
                    {
                        "first": "Alan",
                        "middle": [],
                        "last": "Akbik",
                        "suffix": ""
                    },
                    {
                        "first": "Duncan",
                        "middle": [],
                        "last": "Blythe",
                        "suffix": ""
                    },
                    {
                        "first": "Roland",
                        "middle": [],
                        "last": "Vollgraf",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "COLING",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alan Akbik, Duncan Blythe, and Roland Vollgraf. 2018. Contextual string embeddings for sequence labeling. In COLING.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Designing neural network architectures using reinforcement learning",
                "authors": [
                    {
                        "first": "Bowen",
                        "middle": [],
                        "last": "Baker",
                        "suffix": ""
                    },
                    {
                        "first": "Otkrist",
                        "middle": [],
                        "last": "Gupta",
                        "suffix": ""
                    },
                    {
                        "first": "Nikhil",
                        "middle": [],
                        "last": "Naik",
                        "suffix": ""
                    },
                    {
                        "first": "Ramesh",
                        "middle": [],
                        "last": "Raskar",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1611.02167"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Bowen Baker, Otkrist Gupta, Nikhil Naik, and Ramesh Raskar. 2016. Designing neural network architec- tures using reinforcement learning. arXiv preprint arXiv:1611.02167.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Probabilistic Graphical Models -Principles and Techniques",
                "authors": [
                    {
                        "first": "Daphne",
                        "middle": [],
                        "last": "Koller",
                        "suffix": ""
                    },
                    {
                        "first": "Nir",
                        "middle": [],
                        "last": "Friedman",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Daphne Koller and Nir Friedman. 2009. Probabilis- tic Graphical Models -Principles and Techniques. MIT press.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
                "authors": [
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Devlin",
                        "suffix": ""
                    },
                    {
                        "first": "Ming-Wei",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Kenton",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Kristina",
                        "middle": [],
                        "last": "Toutanova",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1810.04805"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing. arXiv preprint arXiv:1810.04805.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Wenet: Weighted networks for recurrent network architecture search",
                "authors": [
                    {
                        "first": "Zhiheng",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Bing",
                        "middle": [],
                        "last": "Xiang",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1904.03819"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Zhiheng Huang and Bing Xiang. 2019. Wenet: Weighted networks for recurrent network architec- ture search. arXiv preprint arXiv:1904.03819.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
                "authors": [
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Lafferty",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Mccallum",
                        "suffix": ""
                    },
                    {
                        "first": "Fernando Cn",
                        "middle": [],
                        "last": "Pereira",
                        "suffix": ""
                    }
                ],
                "year": 2001,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "John Lafferty, Andrew McCallum, and Fernando CN Pereira. 2001. Conditional random fields: Prob- abilistic models for segmenting and labeling se- quence data.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Neural architectures for named entity recognition",
                "authors": [
                    {
                        "first": "Guillaume",
                        "middle": [],
                        "last": "Lample",
                        "suffix": ""
                    },
                    {
                        "first": "Miguel",
                        "middle": [],
                        "last": "Ballesteros",
                        "suffix": ""
                    },
                    {
                        "first": "Sandeep",
                        "middle": [],
                        "last": "Subramanian",
                        "suffix": ""
                    },
                    {
                        "first": "Kazuya",
                        "middle": [],
                        "last": "Kawakami",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Dyer",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "",
                "issue": "",
                "pages": "260--270",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/N16-1030"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Guillaume Lample, Miguel Ballesteros, Sandeep Sub- ramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural architectures for named entity recognition. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, pages 260-270, San Diego, California. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "DARTS: Differentiable architecture search",
                "authors": [
                    {
                        "first": "Hanxiao",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Karen",
                        "middle": [],
                        "last": "Simonyan",
                        "suffix": ""
                    },
                    {
                        "first": "Yiming",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "International Conference on Learning Representations",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hanxiao Liu, Karen Simonyan, and Yiming Yang. 2019. DARTS: Differentiable architecture search. In International Conference on Learning Represen- tations.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF",
                "authors": [
                    {
                        "first": "Xuezhe",
                        "middle": [],
                        "last": "Ma",
                        "suffix": ""
                    },
                    {
                        "first": "Eduard",
                        "middle": [],
                        "last": "Hovy",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "1064--1074",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/P16-1101"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Xuezhe Ma and Eduard Hovy. 2016. End-to-end sequence labeling via bi-directional LSTM-CNNs- CRF. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers), pages 1064-1074, Berlin, Germany. Association for Computational Linguis- tics.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Regularizing and optimizing LSTM language models",
                "authors": [
                    {
                        "first": "Stephen",
                        "middle": [],
                        "last": "Merity",
                        "suffix": ""
                    },
                    {
                        "first": "Nitish",
                        "middle": [],
                        "last": "Shirish Keskar",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "International Conference on Learning Representations",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Stephen Merity, Nitish Shirish Keskar, and Richard Socher. 2018. Regularizing and optimizing LSTM language models. In International Conference on Learning Representations.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Glove: Global vectors for word representation",
                "authors": [
                    {
                        "first": "Jeffrey",
                        "middle": [],
                        "last": "Pennington",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jeffrey Pennington, Richard Socher, and Christo- pher D. Manning. 2014. Glove: Global vectors for word representation. In EMNLP.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Semi-supervised sequence tagging with bidirectional language models",
                "authors": [
                    {
                        "first": "Matthew",
                        "middle": [],
                        "last": "Peters",
                        "suffix": ""
                    },
                    {
                        "first": "Waleed",
                        "middle": [],
                        "last": "Ammar",
                        "suffix": ""
                    },
                    {
                        "first": "Chandra",
                        "middle": [],
                        "last": "Bhagavatula",
                        "suffix": ""
                    },
                    {
                        "first": "Russell",
                        "middle": [],
                        "last": "Power",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
                "volume": "1",
                "issue": "",
                "pages": "1756--1765",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/P17-1161"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Matthew Peters, Waleed Ammar, Chandra Bhagavatu- la, and Russell Power. 2017. Semi-supervised se- quence tagging with bidirectional language models. In Proceedings of the 55th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1: Long Papers), pages 1756-1765, Vancouver, Cana- da. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Deep contextualized word representations",
                "authors": [
                    {
                        "first": "Matthew",
                        "middle": [],
                        "last": "Peters",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Neumann",
                        "suffix": ""
                    },
                    {
                        "first": "Mohit",
                        "middle": [],
                        "last": "Iyyer",
                        "suffix": ""
                    },
                    {
                        "first": "Matt",
                        "middle": [],
                        "last": "Gardner",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    },
                    {
                        "first": "Kenton",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Luke",
                        "middle": [],
                        "last": "Zettlemoyer",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "volume": "1",
                "issue": "",
                "pages": "2227--2237",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/N18-1202"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Matthew Peters, Mark Neumann, Mohit Iyyer, Mat- t Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word rep- resentations. In Proceedings of the 2018 Confer- ence of the North American Chapter of the Associ- ation for Computational Linguistics: Human Lan- guage Technologies, Volume 1 (Long Papers), pages 2227-2237, New Orleans, Louisiana. Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Efficient neural architecture search via parameter sharing",
                "authors": [
                    {
                        "first": "Hieu",
                        "middle": [],
                        "last": "Pham",
                        "suffix": ""
                    },
                    {
                        "first": "Melody",
                        "middle": [
                            "Y"
                        ],
                        "last": "Guan",
                        "suffix": ""
                    },
                    {
                        "first": "Barret",
                        "middle": [],
                        "last": "Zoph",
                        "suffix": ""
                    },
                    {
                        "first": "Jeff",
                        "middle": [],
                        "last": "Quoc V Le",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Dean",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hieu Pham, Melody Y Guan, Barret Zoph, Quoc V Le, and Jeff Dean. 2018. Efficient neural architecture search via parameter sharing. arXiv preprint arX- iv:1802.03268.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Introduction to the conll-2003 shared task: Languageindependent named entity recognition",
                "authors": [
                    {
                        "first": "Erik",
                        "middle": [],
                        "last": "Tjong",
                        "suffix": ""
                    },
                    {
                        "first": "Kim",
                        "middle": [],
                        "last": "Sang",
                        "suffix": ""
                    },
                    {
                        "first": "Fien",
                        "middle": [],
                        "last": "De",
                        "suffix": ""
                    },
                    {
                        "first": "Meulder",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "CoNLL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Erik Tjong Kim Sang and Fien De Meulder. 2003. In- troduction to the conll-2003 shared task: Language- independent named entity recognition. In CoNLL.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Genetic cnn",
                "authors": [
                    {
                        "first": "Lingxi",
                        "middle": [],
                        "last": "Xie",
                        "suffix": ""
                    },
                    {
                        "first": "Alan",
                        "middle": [],
                        "last": "Yuille",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the IEEE International Conference on Computer Vision",
                "volume": "",
                "issue": "",
                "pages": "1379--1388",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Lingxi Xie and Alan Yuille. 2017. Genetic cnn. In Proceedings of the IEEE International Conference on Computer Vision, pages 1379-1388.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Exploring randomly wired neural networks for image recognition",
                "authors": [
                    {
                        "first": "Saining",
                        "middle": [],
                        "last": "Xie",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [],
                        "last": "Kirillov",
                        "suffix": ""
                    },
                    {
                        "first": "Ross",
                        "middle": [],
                        "last": "Girshick",
                        "suffix": ""
                    },
                    {
                        "first": "Kaiming",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1904.01569"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Saining Xie, Alexander Kirillov, Ross Girshick, and Kaiming He. 2019a. Exploring randomly wired neural networks for image recognition. arXiv preprint arXiv:1904.01569.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "SNAS: stochastic neural architecture search",
                "authors": [
                    {
                        "first": "Sirui",
                        "middle": [],
                        "last": "Xie",
                        "suffix": ""
                    },
                    {
                        "first": "Hehui",
                        "middle": [],
                        "last": "Zheng",
                        "suffix": ""
                    },
                    {
                        "first": "Chunxiao",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Liang",
                        "middle": [],
                        "last": "Lin",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "International Conference on Learning Representations",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sirui Xie, Hehui Zheng, Chunxiao Liu, and Liang Lin. 2019b. SNAS: stochastic neural architecture search. In International Conference on Learning Represen- tations.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Breaking the softmax bottleneck: A high-rank RNN language model",
                "authors": [
                    {
                        "first": "Zhilin",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Zihang",
                        "middle": [],
                        "last": "Dai",
                        "suffix": ""
                    },
                    {
                        "first": "Ruslan",
                        "middle": [],
                        "last": "Salakhutdinov",
                        "suffix": ""
                    },
                    {
                        "first": "William",
                        "middle": [
                            "W"
                        ],
                        "last": "Cohen",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W. Cohen. 2018. Breaking the softmax bot- tleneck: A high-rank RNN language model. In In- ternational Conference on Learning Representation- s.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Recurrent highway networks",
                "authors": [
                    {
                        "first": "Julian",
                        "middle": [
                            "G"
                        ],
                        "last": "Zilly",
                        "suffix": ""
                    },
                    {
                        "first": "Rupesh",
                        "middle": [
                            "Kumar"
                        ],
                        "last": "Srivastava",
                        "suffix": ""
                    },
                    {
                        "first": "Jan",
                        "middle": [],
                        "last": "Koutn\u00edk",
                        "suffix": ""
                    },
                    {
                        "first": "J\u00fcrgen",
                        "middle": [],
                        "last": "Schmidhuber",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "ICML",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Julian G. Zilly, Rupesh Kumar Srivastava, Jan Koutn\u00edk, and J\u00fcrgen Schmidhuber. 2016. Recurrent highway networks. In ICML.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Neural architecture search with reinforcement learning",
                "authors": [
                    {
                        "first": "Barret",
                        "middle": [],
                        "last": "Zoph",
                        "suffix": ""
                    },
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Quoc",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Le",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1611.01578"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Barret Zoph and Quoc V Le. 2016. Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "text": "Figure 2: Perplexity vs. search epoch number.",
                "uris": null,
                "fig_num": "2",
                "type_str": "figure"
            },
            "FIGREF1": {
                "num": null,
                "text": "Figure 3: Cells discovered by I-DARTS for language modeling (top) and NER (bottom).",
                "uris": null,
                "fig_num": "3",
                "type_str": "figure"
            },
            "TABREF1": {
                "content": "<table><tr><td>.18</td></tr></table>",
                "type_str": "table",
                "text": "F1 scores on the CoNLL-2003 English NER test set.",
                "html": null,
                "num": null
            }
        }
    }
}